{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061a4b03",
   "metadata": {},
   "source": [
    "# Simple Sensor Data Visualization v2\n",
    "\n",
    "**Purpose**: Load sensor data around sync events with individual time axes for manual sync event identification.\n",
    "\n",
    "**Features**:\n",
    "- Load 4 hours around sync start time (configurable)\n",
    "- Each sensor has its own independent time axis\n",
    "- Time shifts controlled by Sync_Parameters.yaml\n",
    "- Preprocessing done before plotting\n",
    "- Simple and focused approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e3011",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4c6fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration:\n",
      "  Subject: OutSense-036\n",
      "  Time window: ±2 hours around sync start\n",
      "  Target frequency: 25 Hz\n",
      "  Project root: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src\n",
      "  Labels file: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/Final_Labels_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "SUBJECT_ID = \"OutSense-036\"  # Change this to your subject\n",
    "HOURS_AROUND_SYNC = 2  # Hours to load around sync start time (2 hours before, 2 hours after)\n",
    "TARGET_FREQUENCY = 25  # Hz for resampling\n",
    "\n",
    "# Paths\n",
    "script_dir = '/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/debug_labels_v2.ipynb'\n",
    "project_root = '/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src'\n",
    "sync_params_path = os.path.join(project_root, 'Sync_Parameters_andre.yaml')\n",
    "sync_events_path = os.path.join(project_root, 'Sync_Events_Times.csv')\n",
    "config_path = os.path.join(project_root, 'config.yaml')\n",
    "labels_path = os.path.join(project_root, 'Final_Labels_corrected.csv')\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"  Subject: {SUBJECT_ID}\")\n",
    "print(f\"  Time window: ±{HOURS_AROUND_SYNC} hours around sync start\")\n",
    "print(f\"  Target frequency: {TARGET_FREQUENCY} Hz\")\n",
    "print(f\"  Project root: {project_root}\")\n",
    "print(f\"  Labels file: {labels_path}\")\n",
    "\n",
    "# Generate a consistent set of colors for labels\n",
    "def generate_label_colors(labels_list):\n",
    "    \"\"\"Generate consistent random colors for each unique label\"\"\"\n",
    "    unique_labels = list(set(labels_list))\n",
    "    random.seed(42)  # For consistent colors across runs\n",
    "    colors = []\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # Use HSV color space for better color distribution\n",
    "        hue = (i * 137.5) % 360  # Golden angle for good distribution\n",
    "        saturation = 0.7 + (i % 3) * 0.1  # Vary saturation\n",
    "        value = 0.8 + (i % 2) * 0.15  # Vary brightness\n",
    "        \n",
    "        # Convert HSV to RGB\n",
    "        rgb = mcolors.hsv_to_rgb([hue/360, saturation, value])\n",
    "        colors.append(rgb)\n",
    "    \n",
    "    return dict(zip(unique_labels, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7535c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Label Drift Correction Configuration:\n",
      "  Manual offset: 0.0s\n",
      "  Linear drift: disabled\n",
      "  Reference events: disabled\n"
     ]
    }
   ],
   "source": [
    "# ========== LABEL DRIFT CORRECTION CONFIGURATION ==========\n",
    "# Label time drift correction parameters (separate from sensor drift)\n",
    "LABEL_DRIFT_PARAMS = {\n",
    "    \"enabled\": True,\n",
    "    \"manual_offset_seconds\": 0.0,  # Manual offset in seconds (+ = shift labels forward, - = shift backward)\n",
    "    \"linear_drift_correction\": {\n",
    "        \"enabled\": False,\n",
    "        \"start_time\": None,  # Will be set to sync_start_time by default\n",
    "        \"end_time\": None,    # Will be set to sync_end_time by default\n",
    "        \"drift_seconds\": 0.0  # Total drift over the time period\n",
    "    },\n",
    "    \"reference_events\": {\n",
    "        # Define reference points for label alignment\n",
    "        \"enabled\": False,\n",
    "        \"events\": [\n",
    "            # {\"label_time\": \"2024-01-01 10:00:00\", \"true_time\": \"2024-01-01 10:00:05\", \"description\": \"Manual sync point\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"📋 Label Drift Correction Configuration:\")\n",
    "print(f\"  Manual offset: {LABEL_DRIFT_PARAMS['manual_offset_seconds']}s\")\n",
    "print(f\"  Linear drift: {'enabled' if LABEL_DRIFT_PARAMS['linear_drift_correction']['enabled'] else 'disabled'}\")\n",
    "print(f\"  Reference events: {'enabled' if LABEL_DRIFT_PARAMS['reference_events']['enabled'] else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ec783",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Sync Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a7aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Final_Labels.csv with 7214 entries\n",
      "📊 Found 258 labels for subject OutSense-036\n",
      "📅 258 labels have valid timestamps\n",
      "🎨 Generated colors for 29 unique labels\n",
      "\n",
      "📋 Label distribution:\n",
      "  conversation: 54 instances\n",
      "  self_propulsion: 50 instances\n",
      "  dark: 48 instances\n",
      "  cycling: 22 instances\n",
      "  sitting_wheelchair: 13 instances\n",
      "  drinking: 8 instances\n",
      "  eating: 6 instances\n",
      "  bending: 6 instances\n",
      "  reading_newspaper: 5 instances\n",
      "  pressure_relief: 5 instances\n",
      "  ... and 19 more labels\n",
      "✅ Loaded configurations:\n",
      "  Main config: 64 sections\n",
      "  Sync parameters: 16 subjects\n",
      "  Sync events: 16 entries\n",
      "  Labels: 258 for OutSense-036\n",
      "\n",
      "🎯 Sync times for OutSense-036:\n",
      "  Sync Start: 2024-02-06 09:51:10\n",
      "  Sync End: 2024-02-08 10:23:35\n",
      "  Duration: 2 days 00:32:25\n",
      "\n",
      "📊 Data window (includes both sync events + 1.0h buffer each side):\n",
      "  Window Start: 2024-02-06 08:51:10 (sync start - 1.0h)\n",
      "  Window End: 2024-02-08 11:23:35 (sync end + 1.0h)\n",
      "  Total Window Duration: 2 days 02:32:25\n",
      "  Sync Event Duration: 2 days 00:32:25\n",
      "  Buffer Coverage: 0 days 01:00:00 before sync start, 0 days 01:00:00 after sync end\n",
      "\n",
      "🏷️ Labels in data window: 258\n",
      "  conversation: 54 instances\n",
      "  self_propulsion: 50 instances\n",
      "  dark: 48 instances\n",
      "  cycling: 22 instances\n",
      "  sitting_wheelchair: 13 instances\n"
     ]
    }
   ],
   "source": [
    "# Load main configuration\n",
    "with open('/scai_data3/scratch/stirnimann_r/config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Load sync parameters\n",
    "with open('/scai_data3/scratch/stirnimann_r/Sync_Parameters_andre.yaml', 'r') as f:\n",
    "    sync_params = yaml.safe_load(f)\n",
    "\n",
    "# Load sync events\n",
    "sync_events_df = pd.read_csv('/scai_data3/scratch/stirnimann_r/Sync_Events_Times.csv')\n",
    "\n",
    "# Load Final_Labels.csv\n",
    "try:\n",
    "    labels_df = pd.read_csv('/scai_data3/scratch/stirnimann_r/Final_Labels_corrected.csv')\n",
    "    print(f\"✅ Loaded Final_Labels.csv with {len(labels_df)} entries\")\n",
    "    \n",
    "    # Filter labels for the current subject\n",
    "    subject_labels = labels_df[labels_df['Video_File'].str.contains(SUBJECT_ID, na=False)]\n",
    "    print(f\"📊 Found {len(subject_labels)} labels for subject {SUBJECT_ID}\")\n",
    "    \n",
    "    if len(subject_labels) > 0:\n",
    "        # Parse the Real_Start_Time and Real_End_Time columns\n",
    "        subject_labels = subject_labels.copy()\n",
    "        subject_labels['Real_Start_Time'] = pd.to_datetime(subject_labels['Real_Start_Time'], errors='coerce')\n",
    "        subject_labels['Real_End_Time'] = pd.to_datetime(subject_labels['Real_End_Time'], errors='coerce')\n",
    "        \n",
    "        # Remove any rows with invalid timestamps\n",
    "        valid_labels = subject_labels.dropna(subset=['Real_Start_Time', 'Real_End_Time'])\n",
    "        print(f\"📅 {len(valid_labels)} labels have valid timestamps\")\n",
    "        \n",
    "        # Generate colors for labels\n",
    "        label_colors = generate_label_colors(valid_labels['Label'].tolist())\n",
    "        print(f\"🎨 Generated colors for {len(label_colors)} unique labels\")\n",
    "        \n",
    "        # Show label summary\n",
    "        label_summary = valid_labels['Label'].value_counts()\n",
    "        print(f\"\\n📋 Label distribution:\")\n",
    "        for label, count in label_summary.head(10).items():\n",
    "            print(f\"  {label}: {count} instances\")\n",
    "        if len(label_summary) > 10:\n",
    "            print(f\"  ... and {len(label_summary) - 10} more labels\")\n",
    "    else:\n",
    "        valid_labels = pd.DataFrame()\n",
    "        label_colors = {}\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Final_Labels.csv not found - plots will show without labels\")\n",
    "    valid_labels = pd.DataFrame()\n",
    "    label_colors = {}\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error loading Final_Labels.csv: {e}\")\n",
    "    valid_labels = pd.DataFrame()\n",
    "    label_colors = {}\n",
    "\n",
    "print(f\"✅ Loaded configurations:\")\n",
    "print(f\"  Main config: {len(cfg)} sections\")\n",
    "print(f\"  Sync parameters: {len(sync_params)} subjects\")\n",
    "print(f\"  Sync events: {len(sync_events_df)} entries\")\n",
    "print(f\"  Labels: {len(valid_labels)} for {SUBJECT_ID}\")\n",
    "\n",
    "\n",
    "if SUBJECT_ID == \"OutSense-425\":\n",
    "    SUBJECT_ID_FIXED = \"OutSense-425_48h\"\n",
    "else:\n",
    "    SUBJECT_ID_FIXED = SUBJECT_ID\n",
    "\n",
    "# Get sync start time for the subject\n",
    "subject_sync = sync_events_df[sync_events_df['Subject'] == SUBJECT_ID_FIXED]\n",
    "if subject_sync.empty:\n",
    "    raise ValueError(f\"No sync events found for subject {SUBJECT_ID_FIXED}\")\n",
    "\n",
    "sync_start_str = subject_sync.iloc[0]['Sync Start']\n",
    "sync_end_str = subject_sync.iloc[0]['Sync End']\n",
    "\n",
    "# Parse sync times\n",
    "sync_start_time = pd.to_datetime(sync_start_str, format='%d.%m.%Y.%H.%M.%S')\n",
    "sync_end_time = pd.to_datetime(sync_end_str, format='%d.%m.%Y.%H.%M.%S')\n",
    "\n",
    "print(f\"\\n🎯 Sync times for {SUBJECT_ID}:\")\n",
    "print(f\"  Sync Start: {sync_start_time}\")\n",
    "print(f\"  Sync End: {sync_end_time}\")\n",
    "print(f\"  Duration: {sync_end_time - sync_start_time}\")\n",
    "\n",
    "# Calculate data window to include both sync start and sync end events\n",
    "# Add buffer time around both events\n",
    "buffer_time = pd.Timedelta(hours=HOURS_AROUND_SYNC/2)\n",
    "data_window_start = sync_start_time - buffer_time\n",
    "data_window_end = sync_end_time + buffer_time\n",
    "\n",
    "# Ensure we capture the full sync event duration plus buffer\n",
    "sync_duration = sync_end_time - sync_start_time\n",
    "total_window_duration = data_window_end - data_window_start\n",
    "\n",
    "print(f\"\\n📊 Data window (includes both sync events + {HOURS_AROUND_SYNC/2}h buffer each side):\")\n",
    "print(f\"  Window Start: {data_window_start} (sync start - {HOURS_AROUND_SYNC/2}h)\")\n",
    "print(f\"  Window End: {data_window_end} (sync end + {HOURS_AROUND_SYNC/2}h)\")\n",
    "print(f\"  Total Window Duration: {total_window_duration}\")\n",
    "print(f\"  Sync Event Duration: {sync_duration}\")\n",
    "print(f\"  Buffer Coverage: {buffer_time} before sync start, {buffer_time} after sync end\")\n",
    "\n",
    "# Show labels in the data window\n",
    "if len(valid_labels) > 0:\n",
    "    window_labels = valid_labels[\n",
    "        (valid_labels['Real_Start_Time'] <= data_window_end) & \n",
    "        (valid_labels['Real_End_Time'] >= data_window_start)\n",
    "    ]\n",
    "    print(f\"\\n🏷️ Labels in data window: {len(window_labels)}\")\n",
    "    if len(window_labels) > 0:\n",
    "        window_label_summary = window_labels['Label'].value_counts()\n",
    "        for label, count in window_label_summary.head(5).items():\n",
    "            print(f\"  {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c45dff",
   "metadata": {},
   "source": [
    "## 3. Load and Import Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8adfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imported functions from raw_data_processor\n",
      "\n",
      "📂 Data paths:\n",
      "  Raw data dir: /scai_data2/scai_datasets/interim/scai-outsense/\n",
      "  Subject dir: /scai_data2/scai_datasets/interim/scai-outsense/OutSense-036\n",
      "  Available sensors: ['corsano_wrist_acc', 'cosinuss_ear_acc_x_acc_y_acc_z', 'mbient_imu_wc_accelerometer', 'mbient_imu_wc_gyroscope', 'vivalnk_vv330_acceleration', 'sensomative_bottom_logger', 'sensomative_back_logger', 'corsano_bioz_acc']\n"
     ]
    }
   ],
   "source": [
    "# Import data loading functions from the original notebook/scripts\n",
    "import sys\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import necessary functions (you may need to adjust these based on your actual module structure)\n",
    "try:\n",
    "    from raw_data_processor import (\n",
    "        select_data_loader,\n",
    "        modify_modality_names,\n",
    "        process_modality_duplicates,\n",
    "        handle_missing_data_interpolation,\n",
    "        correct_timestamp_drift\n",
    "    )\n",
    "    print(\"✅ Imported functions from raw_data_processor\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Could not import from raw_data_processor: {e}\")\n",
    "    print(\"You may need to adjust the import paths or copy the required functions\")\n",
    "    \n",
    "    # Define minimal data loader selection function\n",
    "    def select_data_loader(sensor_name):\n",
    "        \"\"\"Simple data loader selector - you may need to implement based on your data structure\"\"\"\n",
    "        def simple_csv_loader(subject_dir, sensor_name, sensor_settings):\n",
    "            # This is a placeholder - implement based on your actual data structure\n",
    "            csv_path = os.path.join(subject_dir, f\"{sensor_name}.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                return pd.read_csv(csv_path)\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "        return simple_csv_loader\n",
    "    \n",
    "    def modify_modality_names(data, sensor_name):\n",
    "        \"\"\"Simple modality name modifier\"\"\"\n",
    "        return sensor_name, data\n",
    "    \n",
    "    def process_modality_duplicates(data, sample_rate):\n",
    "        \"\"\"Simple duplicate processor\"\"\"\n",
    "        return data.drop_duplicates()\n",
    "    \n",
    "    def handle_missing_data_interpolation(data, max_interp_gap_s=2, target_freq=50):\n",
    "        \"\"\"Simple interpolation\"\"\"\n",
    "        return data.interpolate(method='linear', limit=int(max_interp_gap_s * target_freq))\n",
    "    \n",
    "    def correct_timestamp_drift(timestamp, t0, t1, drift_secs):\n",
    "        \"\"\"Simple drift correction\"\"\"\n",
    "        if t0 <= timestamp <= t1:\n",
    "            progress = (timestamp - t0) / (t1 - t0)\n",
    "            return timestamp + (drift_secs * progress)\n",
    "        return timestamp\n",
    "    \n",
    "    print(\"📝 Using simplified placeholder functions\")\n",
    "\n",
    "# Get raw data configuration\n",
    "raw_data_parsing_config = cfg.get('raw_data_parsing_config', {})\n",
    "raw_data_base_dir = os.path.join(project_root, cfg.get('raw_data_input_dir', 'data'))\n",
    "subject_dir = os.path.join(raw_data_base_dir, SUBJECT_ID)\n",
    "\n",
    "print(f\"\\n📂 Data paths:\")\n",
    "print(f\"  Raw data dir: {raw_data_base_dir}\")\n",
    "print(f\"  Subject dir: {subject_dir}\")\n",
    "print(f\"  Available sensors: {list(raw_data_parsing_config.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae41fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Label drift correction functions defined\n"
     ]
    }
   ],
   "source": [
    "def apply_label_time_corrections(labels_df, drift_params, sync_start_time, sync_end_time):\n",
    "    \"\"\"\n",
    "    Apply time drift corrections to label timestamps\n",
    "    \n",
    "    Parameters:\n",
    "    - labels_df: DataFrame with Real_Start_Time and Real_End_Time columns\n",
    "    - drift_params: Label drift correction parameters\n",
    "    - sync_start_time: Reference sync start time\n",
    "    - sync_end_time: Reference sync end time\n",
    "    \n",
    "    Returns:\n",
    "    - corrected_labels_df: DataFrame with corrected timestamps\n",
    "    - correction_log: Dictionary with applied corrections\n",
    "    \"\"\"\n",
    "    if labels_df.empty:\n",
    "        return labels_df, {}\n",
    "    \n",
    "    corrected_labels = labels_df.copy()\n",
    "    correction_log = {\n",
    "        \"manual_offset_applied\": 0.0,\n",
    "        \"linear_drift_applied\": False,\n",
    "        \"reference_events_applied\": 0,\n",
    "        \"total_labels_corrected\": len(labels_df)\n",
    "    }\n",
    "    \n",
    "    print(f\"🔧 Applying label time corrections to {len(labels_df)} labels...\")\n",
    "    \n",
    "    # 1. Apply manual offset\n",
    "    if drift_params.get(\"enabled\", False):\n",
    "        manual_offset = drift_params.get(\"manual_offset_seconds\", 0.0)\n",
    "        if manual_offset != 0.0:\n",
    "            offset_timedelta = pd.Timedelta(seconds=manual_offset)\n",
    "            corrected_labels['Real_Start_Time'] += offset_timedelta\n",
    "            corrected_labels['Real_End_Time'] += offset_timedelta\n",
    "            correction_log[\"manual_offset_applied\"] = manual_offset\n",
    "            print(f\"  ⏱️ Applied manual offset: {manual_offset}s\")\n",
    "    \n",
    "    # 2. Apply linear drift correction\n",
    "    linear_drift = drift_params.get(\"linear_drift_correction\", {})\n",
    "    if linear_drift.get(\"enabled\", False):\n",
    "        start_time = pd.to_datetime(linear_drift.get(\"start_time\")) if linear_drift.get(\"start_time\") else sync_start_time\n",
    "        end_time = pd.to_datetime(linear_drift.get(\"end_time\")) if linear_drift.get(\"end_time\") else sync_end_time\n",
    "        drift_seconds = linear_drift.get(\"drift_seconds\", 0.0)\n",
    "        \n",
    "        if drift_seconds != 0.0 and start_time < end_time:\n",
    "            duration_seconds = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            def apply_linear_drift(timestamp):\n",
    "                if pd.isna(timestamp):\n",
    "                    return timestamp\n",
    "                \n",
    "                if start_time <= timestamp <= end_time:\n",
    "                    # Calculate progress through the drift period (0 to 1)\n",
    "                    progress = (timestamp - start_time).total_seconds() / duration_seconds\n",
    "                    # Apply proportional drift correction\n",
    "                    drift_correction = pd.Timedelta(seconds=drift_seconds * progress)\n",
    "                    return timestamp + drift_correction\n",
    "                return timestamp\n",
    "            \n",
    "            corrected_labels['Real_Start_Time'] = corrected_labels['Real_Start_Time'].apply(apply_linear_drift)\n",
    "            corrected_labels['Real_End_Time'] = corrected_labels['Real_End_Time'].apply(apply_linear_drift)\n",
    "            correction_log[\"linear_drift_applied\"] = True\n",
    "            print(f\"  📐 Applied linear drift correction: {drift_seconds}s over {duration_seconds/60:.1f} minutes\")\n",
    "    \n",
    "    # 3. Apply reference event corrections\n",
    "    ref_events = drift_params.get(\"reference_events\", {})\n",
    "    if ref_events.get(\"enabled\", False) and ref_events.get(\"events\"):\n",
    "        events_applied = 0\n",
    "        for event in ref_events[\"events\"]:\n",
    "            label_time = pd.to_datetime(event[\"label_time\"])\n",
    "            true_time = pd.to_datetime(event[\"true_time\"])\n",
    "            correction = true_time - label_time\n",
    "            \n",
    "            # Apply correction to labels near this reference point\n",
    "            # (You could implement more sophisticated interpolation between reference points)\n",
    "            tolerance = pd.Timedelta(minutes=30)  # Apply to labels within 30 minutes\n",
    "            mask = (\n",
    "                (corrected_labels['Real_Start_Time'] >= label_time - tolerance) &\n",
    "                (corrected_labels['Real_Start_Time'] <= label_time + tolerance)\n",
    "            )\n",
    "            \n",
    "            if mask.any():\n",
    "                corrected_labels.loc[mask, 'Real_Start_Time'] += correction\n",
    "                corrected_labels.loc[mask, 'Real_End_Time'] += correction\n",
    "                events_applied += mask.sum()\n",
    "                print(f\"  🎯 Applied reference event correction: {correction} to {mask.sum()} labels\")\n",
    "        \n",
    "        correction_log[\"reference_events_applied\"] = events_applied\n",
    "    \n",
    "    return corrected_labels, correction_log\n",
    "\n",
    "def create_label_drift_controls():\n",
    "    \"\"\"Create interactive controls for label drift correction\"\"\"\n",
    "    \n",
    "    # Manual offset control\n",
    "    manual_offset_slider = widgets.FloatSlider(\n",
    "        value=LABEL_DRIFT_PARAMS[\"manual_offset_seconds\"],\n",
    "        min=-300.0,  # -5 minutes\n",
    "        max=300.0,   # +5 minutes\n",
    "        step=0.1,\n",
    "        description='Manual Offset (s):',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Linear drift controls\n",
    "    enable_linear_drift = widgets.Checkbox(\n",
    "        value=LABEL_DRIFT_PARAMS[\"linear_drift_correction\"][\"enabled\"],\n",
    "        description='Enable Linear Drift Correction',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    linear_drift_slider = widgets.FloatSlider(\n",
    "        value=LABEL_DRIFT_PARAMS[\"linear_drift_correction\"][\"drift_seconds\"],\n",
    "        min=-60.0,   # -1 minute total drift\n",
    "        max=60.0,    # +1 minute total drift\n",
    "        step=0.1,\n",
    "        description='Linear Drift (s):',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Quick preset buttons\n",
    "    reset_corrections = widgets.Button(\n",
    "        description='🔄 Reset All Corrections',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    apply_corrections = widgets.Button(\n",
    "        description='✅ Apply & Replot',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    # Drift analysis display\n",
    "    drift_analysis_output = widgets.Output()\n",
    "    \n",
    "    return {\n",
    "        'manual_offset_slider': manual_offset_slider,\n",
    "        'enable_linear_drift': enable_linear_drift,\n",
    "        'linear_drift_slider': linear_drift_slider,\n",
    "        'reset_corrections': reset_corrections,\n",
    "        'apply_corrections': apply_corrections,\n",
    "        'drift_analysis_output': drift_analysis_output\n",
    "    }\n",
    "\n",
    "print(\"✅ Label drift correction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59895fa",
   "metadata": {},
   "source": [
    "## 4. Load and Process Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ef9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING SENSOR DATA ===\n",
      "Processing sensors for subject: OutSense-036\n",
      "Time window: 2024-02-06 08:51:10 to 2024-02-08 11:23:35\n",
      "\n",
      "--- Processing sensor: corsano_wrist_acc ---\n",
      "📊 Loaded 5344000 raw samples\n",
      "⏱️ Applied time shift: 3598s\n",
      "📐 Applied drift correction: 6s over 174745.0s interval\n",
      "🔍 Filtered from 5344000 to 4366237 samples (81.7% retained)\n",
      "✅ Final shape: (4366237, 3)\n",
      "✅ Time range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "\n",
      "--- Processing sensor: cosinuss_ear_acc_x_acc_y_acc_z ---\n",
      "📊 Loaded 8422770 raw samples\n",
      "⏱️ Applied time shift: 3600s\n",
      "🔍 Filtered from 8422770 to 5526048 samples (65.6% retained)\n",
      "✅ Final shape: (5526048, 3)\n",
      "✅ Time range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "\n",
      "--- Processing sensor: mbient_imu_wc_accelerometer ---\n",
      "📊 Loaded 9387669 raw samples\n",
      "⏱️ Applied time shift: 3621s\n",
      "📐 Applied drift correction: -15s over 174745.0s interval\n",
      "🔍 Filtered from 9387669 to 9187973 samples (97.9% retained)\n",
      "✅ Final shape: (9187973, 3)\n",
      "✅ Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "\n",
      "--- Processing sensor: mbient_imu_wc_gyroscope ---\n",
      "📊 Loaded 9387670 raw samples\n",
      "⏱️ Applied time shift: 3621s\n",
      "📐 Applied drift correction: -15s over 174745.0s interval\n",
      "🔍 Filtered from 9387670 to 9187973 samples (97.9% retained)\n",
      "✅ Final shape: (9187973, 3)\n",
      "✅ Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "\n",
      "--- Processing sensor: vivalnk_vv330_acceleration ---\n",
      "📊 Loaded 2099295 raw samples\n",
      "⏱️ Applied time shift: 3603s\n",
      "📐 Applied drift correction: -9s over 174745.0s interval\n",
      "🔍 Filtered from 2099295 to 483997 samples (23.1% retained)\n",
      "✅ Final shape: (483997, 3)\n",
      "✅ Time range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "\n",
      "--- Processing sensor: sensomative_bottom_logger ---\n",
      "📊 Loaded 4600049 raw samples\n",
      "⏱️ Applied time shift: 3599s\n",
      "🔍 Filtered from 4600049 to 2781544 samples (60.5% retained)\n",
      "✅ Final shape: (2774494, 11)\n",
      "✅ Time range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "\n",
      "--- Processing sensor: sensomative_back_logger ---\n",
      "❌ No data loaded for sensomative_back_logger\n",
      "\n",
      "--- Processing sensor: corsano_bioz_acc ---\n",
      "📊 Loaded 4438176 raw samples\n",
      "⏱️ Applied time shift: 3599s\n",
      "🔍 Filtered from 4438176 to 4406913 samples (99.3% retained)\n",
      "✅ Final shape: (4406913, 3)\n",
      "✅ Time range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "\n",
      "📈 Successfully processed 7 sensors:\n",
      "  📊 corsano_wrist: 4366237 samples, duration 2 days 01:34:03.001763105\n",
      "  📊 cosinuss_ear: 5526048 samples, duration 1 days 11:05:42.597000122\n",
      "  📊 mbient_acc: 9187973 samples, duration 2 days 02:32:24.981617451\n",
      "  📊 mbient_gyro: 9187973 samples, duration 2 days 02:32:24.981617451\n",
      "  📊 vivalnk_acc: 483997 samples, duration 2 days 01:33:36.792416096\n",
      "  📊 sensomative_bottom: 2774494 samples, duration 2 days 02:32:24.995999814\n",
      "  📊 corsano_bioz: 4406913 samples, duration 2 days 01:34:05\n"
     ]
    }
   ],
   "source": [
    "# Load and process each sensor with time shifts from Sync_Parameters.yaml\n",
    "print(f\"\\n=== LOADING SENSOR DATA ===\")\n",
    "print(f\"Processing sensors for subject: {SUBJECT_ID}\")\n",
    "print(f\"Time window: {data_window_start} to {data_window_end}\")\n",
    "\n",
    "processed_sensors = {}\n",
    "subject_correction_params = sync_params.get(SUBJECT_ID, {})\n",
    "\n",
    "if SUBJECT_ID == \"OutSense-425\":\n",
    "    subject_dir = os.path.join(raw_data_base_dir, \"OutSense-425_48h\")\n",
    "\n",
    "for sensor_name, sensor_settings in raw_data_parsing_config.items():\n",
    "    print(f\"\\n--- Processing sensor: {sensor_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load raw sensor data\n",
    "        loader = select_data_loader(sensor_name)\n",
    "        sensor_data_raw = loader(subject_dir, sensor_name, sensor_settings)\n",
    "        \n",
    "        if sensor_data_raw.empty or 'time' not in sensor_data_raw.columns:\n",
    "            print(f\"❌ No data loaded for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"📊 Loaded {len(sensor_data_raw)} raw samples\")\n",
    "        \n",
    "        # Get time correction parameters for this sensor\n",
    "        sensor_corr_params = subject_correction_params.get(sensor_name, {'unit': 's'})\n",
    "        time_unit = sensor_corr_params.get('unit', 's')\n",
    "        shift_val = sensor_corr_params.get('shift', 0)\n",
    "        \n",
    "        # Apply time corrections\n",
    "        time_col_num = sensor_data_raw['time'].astype(float)\n",
    "        \n",
    "        # Convert to seconds if needed\n",
    "        if time_unit == 'ms':\n",
    "            time_col_num = time_col_num / 1000.0\n",
    "        \n",
    "        # Apply shift correction\n",
    "        if shift_val != 0:\n",
    "            time_col_num = time_col_num + shift_val\n",
    "            print(f\"⏱️ Applied time shift: {shift_val}s\")\n",
    "        \n",
    "        # Apply drift correction if available\n",
    "        drift_params = sensor_corr_params.get('drift')\n",
    "        if drift_params and all(k in drift_params for k in ['t0', 't1', 'drift_secs']):\n",
    "            t0_ts = pd.Timestamp(drift_params['t0'])\n",
    "            t1_ts = pd.Timestamp(drift_params['t1'])\n",
    "            if not pd.isna(t0_ts) and not pd.isna(t1_ts):\n",
    "                t0, t1 = t0_ts.timestamp(), t1_ts.timestamp()\n",
    "                drift = drift_params['drift_secs']\n",
    "                time_col_num = time_col_num.apply(correct_timestamp_drift, args=(t0, t1, drift))\n",
    "                print(f\"📐 Applied drift correction: {drift}s over {t1-t0:.1f}s interval\")\n",
    "        \n",
    "        # Convert to datetime\n",
    "        corrected_timestamps = pd.to_datetime(time_col_num, unit='s', errors='coerce')\n",
    "        sensor_data_corrected = sensor_data_raw.drop(columns=['time']).copy()\n",
    "        sensor_data_corrected['time'] = corrected_timestamps\n",
    "        sensor_data_corrected.dropna(subset=['time'], inplace=True)\n",
    "        \n",
    "        if sensor_data_corrected.empty:\n",
    "            print(f\"❌ No valid data after time correction for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to data window\n",
    "        original_count = len(sensor_data_corrected)\n",
    "        time_mask = (sensor_data_corrected['time'] >= data_window_start) & (sensor_data_corrected['time'] <= data_window_end)\n",
    "        sensor_data_filtered = sensor_data_corrected[time_mask].copy()\n",
    "        \n",
    "        filtered_count = len(sensor_data_filtered)\n",
    "        retention_pct = (filtered_count / original_count * 100) if original_count > 0 else 0\n",
    "        print(f\"🔍 Filtered from {original_count} to {filtered_count} samples ({retention_pct:.1f}% retained)\")\n",
    "        \n",
    "        if sensor_data_filtered.empty:\n",
    "            print(f\"❌ No data in time window for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Set time as index\n",
    "        sensor_data_filtered.set_index('time', inplace=True)\n",
    "        sensor_data_filtered.sort_index(inplace=True)\n",
    "        \n",
    "        # Apply basic preprocessing\n",
    "        sample_rate = sensor_settings.get('sample_rate', TARGET_FREQUENCY)\n",
    "        processed_data = process_modality_duplicates(sensor_data_filtered, sample_rate)\n",
    "        processed_data = handle_missing_data_interpolation(processed_data, max_interp_gap_s=2, target_freq=TARGET_FREQUENCY)\n",
    "        \n",
    "        # Apply column renaming\n",
    "        new_name, processed_data = modify_modality_names(processed_data, sensor_name)\n",
    "        \n",
    "        if processed_data.empty:\n",
    "            print(f\"❌ No data after preprocessing for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"✅ Final shape: {processed_data.shape}\")\n",
    "        print(f\"✅ Time range: {processed_data.index.min()} to {processed_data.index.max()}\")\n",
    "        \n",
    "        processed_sensors[new_name] = processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing sensor {sensor_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n📈 Successfully processed {len(processed_sensors)} sensors:\")\n",
    "for sensor_name, data in processed_sensors.items():\n",
    "    duration = data.index.max() - data.index.min()\n",
    "    print(f\"  📊 {sensor_name}: {len(data)} samples, duration {duration}\")\n",
    "\n",
    "if not processed_sensors:\n",
    "    raise ValueError(\"No sensor data was successfully processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d7615",
   "metadata": {},
   "source": [
    "## 5. Interactive Plotting with Independent Time Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246a8d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INTERACTIVE SENSOR VISUALIZATION WITH LABEL DRIFT CORRECTION ===\n",
      "🎯 Each sensor has its own independent time axis\n",
      "🔍 Perfect for manual sync event identification\n",
      "🏷️ Labels from Final_Labels.csv with real-time drift correction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52290927780b4fe1a576174d6c0333e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>🎛️ Controls</h3>'), SelectMultiple(description='Select Sensors:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applying label time corrections to 258 labels...\n",
      "\n",
      "🚀 Interactive visualization with label drift correction ready!\n",
      "\n",
      "📝 Instructions:\n",
      "  1. Select sensors to visualize\n",
      "  2. Adjust label drift corrections:\n",
      "     • Manual Offset: Move all labels forward/backward in time\n",
      "     • Linear Drift: Apply gradual time correction over sync period\n",
      "  3. Use 'Apply & Replot' to see corrections\n",
      "  4. Navigate through time to verify label alignment\n",
      "  5. Corrected labels show with ✓ marker and green background\n",
      "\n",
      "💡 Label Drift Correction Features:\n",
      "  ✅ Real-time manual offset adjustment (-5 to +5 minutes)\n",
      "  ✅ Linear drift correction over sync period\n",
      "  ✅ Visual feedback of corrections applied\n",
      "  ✅ Reset button to clear all corrections\n",
      "  ✅ Live preview during adjustment\n",
      "  ✅ Corrected labels marked with ✓ symbol\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9525e2d6598d46c9b236ac64620e1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>🎛️ Controls</h3>'), SelectMultiple(description='Select Sensors:'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applying label time corrections to 258 labels...\n",
      "\n",
      "🚀 Interactive visualization with label drift correction ready!\n",
      "\n",
      "📝 Instructions:\n",
      "  1. Select sensors to visualize\n",
      "  2. Adjust label drift corrections:\n",
      "     • Manual Offset: Move all labels forward/backward in time\n",
      "     • Linear Drift: Apply gradual time correction over sync period\n",
      "  3. Use 'Apply & Replot' to see corrections\n",
      "  4. Navigate through time to verify label alignment\n",
      "  5. Corrected labels show with ✓ marker and green background\n",
      "\n",
      "💡 Label Drift Correction Features:\n",
      "  ✅ Real-time manual offset adjustment (-5 to +5 minutes)\n",
      "  ✅ Linear drift correction over sync period\n",
      "  ✅ Visual feedback of corrections applied\n",
      "  ✅ Reset button to clear all corrections\n",
      "  ✅ Live preview during adjustment\n",
      "  ✅ Corrected labels marked with ✓ symbol\n"
     ]
    }
   ],
   "source": [
    "# Create interactive plotting tool with independent time axes AND label drift correction\n",
    "print(\"=== INTERACTIVE SENSOR VISUALIZATION WITH LABEL DRIFT CORRECTION ===\")\n",
    "print(\"🎯 Each sensor has its own independent time axis\")\n",
    "print(\"🔍 Perfect for manual sync event identification\")\n",
    "print(\"🏷️ Labels from Final_Labels.csv with real-time drift correction\")\n",
    "\n",
    "# Create controls\n",
    "sensor_names = list(processed_sensors.keys())\n",
    "\n",
    "# Sensor selection (existing)\n",
    "sensor_selection = widgets.SelectMultiple(\n",
    "    options=sensor_names,\n",
    "    value=sensor_names[:3] if len(sensor_names) >= 3 else sensor_names,\n",
    "    description='Select Sensors:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='200px', width='300px')\n",
    ")\n",
    "\n",
    "# Label display controls (existing)\n",
    "show_labels = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show Labels',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "label_alpha = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.1,\n",
    "    max=0.8,\n",
    "    step=0.1,\n",
    "    description='Label Alpha:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create label drift correction controls\n",
    "label_drift_controls = create_label_drift_controls()\n",
    "\n",
    "# Store corrected labels globally for reuse\n",
    "corrected_labels_global = valid_labels.copy() if len(valid_labels) > 0 else pd.DataFrame()\n",
    "current_correction_log = {}\n",
    "\n",
    "def update_label_corrections():\n",
    "    \"\"\"Update label corrections based on current control values\"\"\"\n",
    "    global corrected_labels_global, current_correction_log\n",
    "    \n",
    "    if len(valid_labels) == 0:\n",
    "        return\n",
    "    \n",
    "    # Get current drift parameters from controls\n",
    "    current_drift_params = {\n",
    "        \"enabled\": True,\n",
    "        \"manual_offset_seconds\": label_drift_controls['manual_offset_slider'].value,\n",
    "        \"linear_drift_correction\": {\n",
    "            \"enabled\": label_drift_controls['enable_linear_drift'].value,\n",
    "            \"start_time\": sync_start_time,\n",
    "            \"end_time\": sync_end_time,\n",
    "            \"drift_seconds\": label_drift_controls['linear_drift_slider'].value\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Apply corrections\n",
    "    corrected_labels_global, current_correction_log = apply_label_time_corrections(\n",
    "        valid_labels, current_drift_params, sync_start_time, sync_end_time\n",
    "    )\n",
    "    \n",
    "    # Update drift analysis display\n",
    "    with label_drift_controls['drift_analysis_output']:\n",
    "        clear_output(wait=True)\n",
    "        print(\"📊 Current Label Drift Corrections:\")\n",
    "        print(f\"  Manual Offset: {current_correction_log.get('manual_offset_applied', 0)}s\")\n",
    "        if current_correction_log.get('linear_drift_applied', False):\n",
    "            print(f\"  Linear Drift: {current_drift_params['linear_drift_correction']['drift_seconds']}s\")\n",
    "        print(f\"  Labels Affected: {current_correction_log.get('total_labels_corrected', 0)}\")\n",
    "        \n",
    "        if len(corrected_labels_global) > 0:\n",
    "            original_time_range = f\"{valid_labels['Real_Start_Time'].min()} to {valid_labels['Real_End_Time'].max()}\"\n",
    "            corrected_time_range = f\"{corrected_labels_global['Real_Start_Time'].min()} to {corrected_labels_global['Real_End_Time'].max()}\"\n",
    "            print(f\"\\n⏱️ Time Range Comparison:\")\n",
    "            print(f\"  Original:  {original_time_range}\")\n",
    "            print(f\"  Corrected: {corrected_time_range}\")\n",
    "\n",
    "# Label filter (updated to use corrected labels)\n",
    "if len(valid_labels) > 0:\n",
    "    unique_labels_in_window = sorted(valid_labels['Label'].unique())\n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=unique_labels_in_window,\n",
    "        value=unique_labels_in_window[:10] if len(unique_labels_in_window) > 10 else unique_labels_in_window,\n",
    "        description='Show Labels:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='150px', width='300px')\n",
    "    )\n",
    "else:\n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        value=[],\n",
    "        description='Show Labels:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='150px', width='300px')\n",
    "    )\n",
    "\n",
    "# Time window controls\n",
    "center_time_text = widgets.Text(\n",
    "    value=sync_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    description='Center Time:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "window_minutes = widgets.IntSlider(\n",
    "    value=60,  # 1 hour window\n",
    "    min=1,\n",
    "    max=240,  # 4 hours max\n",
    "    step=1,\n",
    "    description='Window (min):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Time navigation buttons (10 minutes forward/backward)\n",
    "nav_backward_10min = widgets.Button(description='⏪ -10min', button_style='', \n",
    "                                   layout=widgets.Layout(width='100px'))\n",
    "nav_forward_10min = widgets.Button(description='⏩ +10min', button_style='', \n",
    "                                  layout=widgets.Layout(width='100px'))\n",
    "\n",
    "# Quick jump buttons\n",
    "jump_sync_start = widgets.Button(description='🎯 Jump to Sync Start', button_style='success')\n",
    "jump_sync_end = widgets.Button(description='🎯 Jump to Sync End', button_style='warning')\n",
    "jump_data_start = widgets.Button(description='📊 Jump to Data Start', button_style='info')\n",
    "jump_data_end = widgets.Button(description='📊 Jump to Data End', button_style='info')\n",
    "\n",
    "# Plot button\n",
    "plot_button = widgets.Button(description='📈 Plot Sensors', button_style='primary', layout=widgets.Layout(width='150px'))\n",
    "\n",
    "# Auto-plot checkbox\n",
    "auto_plot = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Auto-plot on navigation',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Output area\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "def get_center_time():\n",
    "    \"\"\"Get center time from text widget\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(center_time_text.value)\n",
    "    except:\n",
    "        return sync_start_time\n",
    "\n",
    "def update_center_time(new_time):\n",
    "    \"\"\"Update center time text widget\"\"\"\n",
    "    center_time_text.value = new_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def plot_sensors_with_drift_correction(btn):\n",
    "    \"\"\"Plot selected sensors with corrected label overlays\"\"\"\n",
    "    \n",
    "    # Update label corrections first\n",
    "    update_label_corrections()\n",
    "    \n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        try:\n",
    "            selected_sensors = list(sensor_selection.value)\n",
    "            if not selected_sensors:\n",
    "                print(\"❌ Please select at least one sensor\")\n",
    "                return\n",
    "            \n",
    "            center_time = get_center_time()\n",
    "            window_mins = window_minutes.value\n",
    "            \n",
    "            # Calculate time window\n",
    "            half_window = pd.Timedelta(minutes=window_mins/2)\n",
    "            plot_start = center_time - half_window\n",
    "            plot_end = center_time + half_window\n",
    "            \n",
    "            print(f\"📊 Plotting {len(selected_sensors)} sensors with corrected labels\")\n",
    "            print(f\"⏱️ Time window: {plot_start} to {plot_end} ({window_mins} minutes)\")\n",
    "            print(f\"🎯 Center time: {center_time}\")\n",
    "            \n",
    "            # Use corrected labels for plotting\n",
    "            if show_labels.value and len(corrected_labels_global) > 0:\n",
    "                selected_label_types = list(label_filter.value)\n",
    "                plot_labels = corrected_labels_global[\n",
    "                    (corrected_labels_global['Real_Start_Time'] <= plot_end) & \n",
    "                    (corrected_labels_global['Real_End_Time'] >= plot_start) &\n",
    "                    (corrected_labels_global['Label'].isin(selected_label_types))\n",
    "                ]\n",
    "                print(f\"🏷️ Showing {len(plot_labels)} corrected labels in window\")\n",
    "                \n",
    "                # Show correction info\n",
    "                if current_correction_log.get('manual_offset_applied', 0) != 0:\n",
    "                    print(f\"🔧 Manual offset applied: {current_correction_log['manual_offset_applied']}s\")\n",
    "                if current_correction_log.get('linear_drift_applied', False):\n",
    "                    print(f\"🔧 Linear drift correction applied\")\n",
    "            else:\n",
    "                plot_labels = pd.DataFrame()\n",
    "            \n",
    "            # Create plot\n",
    "            fig, axes = plt.subplots(len(selected_sensors), 1, \n",
    "                                   figsize=(16, 3*len(selected_sensors)), \n",
    "                                   sharex=False)\n",
    "            if len(selected_sensors) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, sensor_name in enumerate(selected_sensors):\n",
    "                ax = axes[i]\n",
    "                \n",
    "                if sensor_name not in processed_sensors:\n",
    "                    ax.text(0.5, 0.5, f'No data for {sensor_name}', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title(f'{sensor_name} - No Data')\n",
    "                    continue\n",
    "                \n",
    "                sensor_data = processed_sensors[sensor_name]\n",
    "                mask = (sensor_data.index >= plot_start) & (sensor_data.index <= plot_end)\n",
    "                plot_data = sensor_data[mask]\n",
    "                \n",
    "                if plot_data.empty:\n",
    "                    ax.text(0.5, 0.5, f'No data in time window for {sensor_name}', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title(f'{sensor_name} - No Data in Window')\n",
    "                    continue\n",
    "                \n",
    "                # Plot sensor data\n",
    "                numeric_cols = plot_data.select_dtypes(include=[np.number]).columns\n",
    "                for col in numeric_cols:\n",
    "                    ax.plot(plot_data.index, plot_data[col], \n",
    "                           label=col, alpha=0.7, linewidth=1)\n",
    "                \n",
    "                # Add CORRECTED label shading\n",
    "                if show_labels.value and len(plot_labels) > 0:\n",
    "                    y_min, y_max = ax.get_ylim() if len(numeric_cols) > 0 else (0, 1)\n",
    "                    \n",
    "                    label_count = {}\n",
    "                    for _, label_row in plot_labels.iterrows():\n",
    "                        label_name = label_row['Label']\n",
    "                        start_time = max(label_row['Real_Start_Time'], plot_start)\n",
    "                        end_time = min(label_row['Real_End_Time'], plot_end)\n",
    "                        \n",
    "                        if start_time < end_time:\n",
    "                            color = label_colors.get(label_name, 'gray')\n",
    "                            \n",
    "                            if label_name not in label_count:\n",
    "                                label_count[label_name] = 0\n",
    "                            label_count[label_name] += 1\n",
    "                            \n",
    "                            # Add shaded region with corrected times\n",
    "                            ax.axvspan(start_time, end_time, \n",
    "                                     alpha=label_alpha.value, \n",
    "                                     color=color,\n",
    "                                     label=f'{label_name} (corrected)' if label_count[label_name] == 1 else \"\")\n",
    "                            \n",
    "                            # Add label text for longer labels\n",
    "                            duration = end_time - start_time\n",
    "                            if duration > pd.Timedelta(minutes=2):\n",
    "                                mid_time = start_time + (end_time - start_time) / 2\n",
    "                                if len(numeric_cols) > 0:\n",
    "                                    y_pos = y_max - (y_max - y_min) * 0.05\n",
    "                                else:\n",
    "                                    y_pos = 0.5\n",
    "                                \n",
    "                                ax.text(mid_time, y_pos, f'{label_name}✓', \n",
    "                                       ha='center', va='top', rotation=0,\n",
    "                                       fontsize=8, alpha=0.8,\n",
    "                                       bbox=dict(boxstyle='round,pad=0.2', \n",
    "                                               facecolor='lightgreen', alpha=0.7))\n",
    "                \n",
    "                # Add sync events and other markers\n",
    "                if plot_start <= sync_start_time <= plot_end:\n",
    "                    ax.axvline(sync_start_time, color='red', linestyle='--', \n",
    "                             linewidth=2, alpha=0.8, label='🎯 Sync Start')\n",
    "                \n",
    "                if plot_start <= sync_end_time <= plot_end:\n",
    "                    ax.axvline(sync_end_time, color='darkred', linestyle='--', \n",
    "                             linewidth=2, alpha=0.8, label='🎯 Sync End')\n",
    "                \n",
    "                ax.axvline(center_time, color='green', linestyle=':', \n",
    "                         linewidth=1, alpha=0.6, label='Center')\n",
    "                \n",
    "                # Formatting\n",
    "                title_text = f'{sensor_name} ({len(numeric_cols)} channels)'\n",
    "                if show_labels.value and len(plot_labels) > 0:\n",
    "                    title_text += f' | {len(plot_labels)} corrected labels'\n",
    "                ax.set_title(title_text)\n",
    "                ax.set_ylabel('Value')\n",
    "                ax.set_xlabel('Time')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, window_mins//10)))\n",
    "                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "                \n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                if len(handles) <= 15:\n",
    "                    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "            \n",
    "            # Title with correction info\n",
    "            title_text = f'Sensor Data with Corrected Labels - Independent Time Axes\\n'\n",
    "            title_text += f'Window: {plot_start} to {plot_end}'\n",
    "            if current_correction_log.get('manual_offset_applied', 0) != 0:\n",
    "                title_text += f' | Manual Offset: {current_correction_log[\"manual_offset_applied\"]}s'\n",
    "            if current_correction_log.get('linear_drift_applied', False):\n",
    "                title_text += f' | Linear Drift Applied'\n",
    "            \n",
    "            plt.suptitle(title_text, fontsize=14, y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(right=0.85, top=0.92)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating plot: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "# Button event handlers\n",
    "def reset_label_corrections(btn):\n",
    "    \"\"\"Reset all label corrections to zero\"\"\"\n",
    "    label_drift_controls['manual_offset_slider'].value = 0.0\n",
    "    label_drift_controls['enable_linear_drift'].value = False\n",
    "    label_drift_controls['linear_drift_slider'].value = 0.0\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def apply_and_replot(btn):\n",
    "    \"\"\"Apply current corrections and replot\"\"\"\n",
    "    plot_sensors_with_drift_correction(None)\n",
    "\n",
    "# Connect button events\n",
    "plot_button.on_click(plot_sensors_with_drift_correction)\n",
    "label_drift_controls['reset_corrections'].on_click(reset_label_corrections)\n",
    "label_drift_controls['apply_corrections'].on_click(apply_and_replot)\n",
    "\n",
    "# Update existing navigation functions to use corrected plotting\n",
    "def navigate_backward_10min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time - pd.Timedelta(minutes=10)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def navigate_forward_10min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time + pd.Timedelta(minutes=10)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_sync_start(btn):\n",
    "    update_center_time(sync_start_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_sync_end(btn):\n",
    "    update_center_time(sync_end_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_data_start(btn):\n",
    "    all_starts = [data.index.min() for data in processed_sensors.values()]\n",
    "    earliest = min(all_starts)\n",
    "    update_center_time(earliest + pd.Timedelta(minutes=window_minutes.value/2))\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_data_end(btn):\n",
    "    all_ends = [data.index.max() for data in processed_sensors.values()]\n",
    "    latest = max(all_ends)\n",
    "    update_center_time(latest - pd.Timedelta(minutes=window_minutes.value/2))\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "# Connect navigation buttons\n",
    "nav_backward_10min.on_click(navigate_backward_10min)\n",
    "nav_forward_10min.on_click(navigate_forward_10min)\n",
    "jump_sync_start.on_click(jump_to_sync_start)\n",
    "jump_sync_end.on_click(jump_to_sync_end)\n",
    "jump_data_start.on_click(jump_to_data_start)\n",
    "jump_data_end.on_click(jump_to_data_end)\n",
    "\n",
    "# Layout with label drift correction controls\n",
    "label_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>🏷️ Label Controls & Drift Correction</h4>\"),\n",
    "    show_labels,\n",
    "    label_alpha,\n",
    "    label_filter,\n",
    "    widgets.HTML(\"<hr><h5>⚙️ Label Time Drift Correction</h5>\"),\n",
    "    label_drift_controls['manual_offset_slider'],\n",
    "    label_drift_controls['enable_linear_drift'],\n",
    "    label_drift_controls['linear_drift_slider'],\n",
    "    widgets.HBox([\n",
    "        label_drift_controls['reset_corrections'], \n",
    "        label_drift_controls['apply_corrections']\n",
    "    ]),\n",
    "    label_drift_controls['drift_analysis_output']\n",
    "]) if len(valid_labels) > 0 else widgets.HTML(\"<p>No labels available</p>\")\n",
    "\n",
    "time_navigation = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>⏱️ Time Navigation</h4>\"),\n",
    "    center_time_text,\n",
    "    window_minutes,\n",
    "    widgets.HBox([nav_backward_10min, nav_forward_10min]),\n",
    "    auto_plot,\n",
    "    widgets.HBox([jump_sync_start, jump_sync_end]),\n",
    "    widgets.HBox([jump_data_start, jump_data_end]),\n",
    "])\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🎛️ Controls</h3>\"),\n",
    "    sensor_selection,\n",
    "    label_controls,\n",
    "    time_navigation,\n",
    "    plot_button\n",
    "])\n",
    "\n",
    "display(widgets.VBox([controls, plot_output]))\n",
    "\n",
    "# Initialize with default corrections\n",
    "update_label_corrections()\n",
    "\n",
    "print(\"\\n🚀 Interactive visualization with label drift correction ready!\")\n",
    "print(\"\\n📝 Instructions:\")\n",
    "print(\"  1. Select sensors to visualize\")\n",
    "print(\"  2. Adjust label drift corrections:\")\n",
    "print(\"     • Manual Offset: Move all labels forward/backward in time\")\n",
    "print(\"     • Linear Drift: Apply gradual time correction over sync period\")\n",
    "print(\"  3. Use 'Apply & Replot' to see corrections\")\n",
    "print(\"  4. Navigate through time to verify label alignment\")\n",
    "print(\"  5. Corrected labels show with ✓ marker and green background\")\n",
    "print(\"\\n💡 Label Drift Correction Features:\")\n",
    "print(\"  ✅ Real-time manual offset adjustment (-5 to +5 minutes)\")\n",
    "print(\"  ✅ Linear drift correction over sync period\")\n",
    "print(\"  ✅ Visual feedback of corrections applied\")\n",
    "print(\"  ✅ Reset button to clear all corrections\")\n",
    "print(\"  ✅ Live preview during adjustment\")\n",
    "print(\"  ✅ Corrected labels marked with ✓ symbol\")\n",
    "\n",
    "# Layout with label drift correction controls\n",
    "label_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>🏷️ Label Controls & Drift Correction</h4>\"),\n",
    "    show_labels,\n",
    "    label_alpha,\n",
    "    label_filter,\n",
    "    widgets.HTML(\"<hr><h5>⚙️ Label Time Drift Correction</h5>\"),\n",
    "    label_drift_controls['manual_offset_slider'],\n",
    "    label_drift_controls['enable_linear_drift'],\n",
    "    label_drift_controls['linear_drift_slider'],\n",
    "    widgets.HBox([\n",
    "        label_drift_controls['reset_corrections'], \n",
    "        label_drift_controls['apply_corrections']\n",
    "    ]),\n",
    "    label_drift_controls['drift_analysis_output']\n",
    "]) if len(valid_labels) > 0 else widgets.HTML(\"<p>No labels available</p>\")\n",
    "\n",
    "time_navigation = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>⏱️ Time Navigation</h4>\"),\n",
    "    center_time_text,\n",
    "    window_minutes,\n",
    "    widgets.HBox([nav_backward_10min, nav_forward_10min]),\n",
    "    auto_plot,\n",
    "    widgets.HBox([jump_sync_start, jump_sync_end]),\n",
    "    widgets.HBox([jump_data_start, jump_data_end]),\n",
    "])\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🎛️ Controls</h3>\"),\n",
    "    sensor_selection,\n",
    "    label_controls,\n",
    "    time_navigation,\n",
    "    plot_button\n",
    "])\n",
    "\n",
    "display(widgets.VBox([controls, plot_output]))\n",
    "\n",
    "# Initialize with default corrections\n",
    "update_label_corrections()\n",
    "\n",
    "print(\"\\n🚀 Interactive visualization with label drift correction ready!\")\n",
    "print(\"\\n📝 Instructions:\")\n",
    "print(\"  1. Select sensors to visualize\")\n",
    "print(\"  2. Adjust label drift corrections:\")\n",
    "print(\"     • Manual Offset: Move all labels forward/backward in time\")\n",
    "print(\"     • Linear Drift: Apply gradual time correction over sync period\")\n",
    "print(\"  3. Use 'Apply & Replot' to see corrections\")\n",
    "print(\"  4. Navigate through time to verify label alignment\")\n",
    "print(\"  5. Corrected labels show with ✓ marker and green background\")\n",
    "print(\"\\n💡 Label Drift Correction Features:\")\n",
    "print(\"  ✅ Real-time manual offset adjustment (-5 to +5 minutes)\")\n",
    "print(\"  ✅ Linear drift correction over sync period\")\n",
    "print(\"  ✅ Visual feedback of corrections applied\")\n",
    "print(\"  ✅ Reset button to clear all corrections\")\n",
    "print(\"  ✅ Live preview during adjustment\")\n",
    "print(\"  ✅ Corrected labels marked with ✓ symbol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c6899",
   "metadata": {},
   "source": [
    "## 6. Summary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56472a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUMMARY ===\n",
      "Subject: OutSense-036\n",
      "Data window: 2h around sync start\n",
      "Sync start: 2024-02-06 09:51:10\n",
      "Sync end: 2024-02-08 10:23:35\n",
      "Processed sensors: 7\n",
      "Available labels: 258\n",
      "\n",
      "📊 Sensor Details:\n",
      "  📈 corsano_wrist:\n",
      "    Samples: 4366237\n",
      "    Time range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "    Duration: 2 days 01:34:03.001763105\n",
      "    Columns: ['wrist_acc_x', 'wrist_acc_y', 'wrist_acc_z']\n",
      "    Time shift applied: 0s\n",
      "  📈 cosinuss_ear:\n",
      "    Samples: 5526048\n",
      "    Time range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "    Duration: 1 days 11:05:42.597000122\n",
      "    Columns: ['ear_acc_x', 'ear_acc_y', 'ear_acc_z']\n",
      "    Time shift applied: 0s\n",
      "  📈 mbient_acc:\n",
      "    Samples: 9187973\n",
      "    Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    Duration: 2 days 02:32:24.981617451\n",
      "    Columns: ['x_axis_g', 'y_axis_g', 'z_axis_g']\n",
      "    Time shift applied: 0s\n",
      "  📈 mbient_gyro:\n",
      "    Samples: 9187973\n",
      "    Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    Duration: 2 days 02:32:24.981617451\n",
      "    Columns: ['x_axis_dps', 'y_axis_dps', 'z_axis_dps']\n",
      "    Time shift applied: 0s\n",
      "  📈 vivalnk_acc:\n",
      "    Samples: 483997\n",
      "    Time range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "    Duration: 2 days 01:33:36.792416096\n",
      "    Columns: ['vivalnk_acc_x', 'vivalnk_acc_y', 'vivalnk_acc_z']\n",
      "    Time shift applied: 0s\n",
      "  📈 sensomative_bottom:\n",
      "    Samples: 2774494\n",
      "    Time range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "    Duration: 2 days 02:32:24.995999814\n",
      "    Columns: ['bottom_value_1', 'bottom_value_2', 'bottom_value_3', 'bottom_value_4', 'bottom_value_5', 'bottom_value_6', 'bottom_value_7', 'bottom_value_8', 'bottom_value_9', 'bottom_value_10', 'bottom_value_11']\n",
      "    Time shift applied: 0s\n",
      "  📈 corsano_bioz:\n",
      "    Samples: 4406913\n",
      "    Time range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "    Duration: 2 days 01:34:05\n",
      "    Columns: ['bioz_acc_x', 'bioz_acc_y', 'bioz_acc_z']\n",
      "    Time shift applied: 0s\n",
      "\n",
      "🏷️ Label Details:\n",
      "  Total labels for OutSense-036: 258\n",
      "  Label time range: 2024-02-06 10:52:27 to 2024-02-08 10:18:02\n",
      "  Label duration span: 1 days 23:25:35\n",
      "  Top 5 most common labels:\n",
      "    📋 conversation: 54 instances (avg: 149.1s)\n",
      "    📋 self_propulsion: 50 instances (avg: 27.4s)\n",
      "    📋 dark: 48 instances (avg: 1494.9s)\n",
      "    📋 cycling: 22 instances (avg: 239.6s)\n",
      "    📋 sitting_wheelchair: 13 instances (avg: 47.2s)\n",
      "  Labels in current data window: 258\n",
      "\n",
      "🎨 Label Colors (29 unique labels):\n",
      "  🟦 arm_raises\n",
      "  🟦 arm_sleeve\n",
      "  🟦 assisted_propulsion\n",
      "  🟦 bending\n",
      "  🟦 chair_to_wheelchair\n",
      "  🟦 conversation\n",
      "  🟦 cycling\n",
      "  🟦 dark\n",
      "  🟦 drinking\n",
      "  🟦 eating\n",
      "  ... and 19 more labels\n",
      "\n",
      "🎯 Ready for manual sync event identification with label overlay!\n",
      "Use the interactive plot above to examine each sensor independently.\n",
      "Labels from Final_Labels.csv will be displayed as colored shaded areas.\n"
     ]
    }
   ],
   "source": [
    "# Display summary information\n",
    "print(\"=== SUMMARY ===\")\n",
    "print(f\"Subject: {SUBJECT_ID}\")\n",
    "print(f\"Data window: {HOURS_AROUND_SYNC}h around sync start\")\n",
    "print(f\"Sync start: {sync_start_time}\")\n",
    "print(f\"Sync end: {sync_end_time}\")\n",
    "print(f\"Processed sensors: {len(processed_sensors)}\")\n",
    "print(f\"Available labels: {len(valid_labels)}\")\n",
    "\n",
    "print(\"\\n📊 Sensor Details:\")\n",
    "for sensor_name, data in processed_sensors.items():\n",
    "    # Get time shift applied\n",
    "    original_sensor_name = sensor_name  # May be modified by modify_modality_names\n",
    "    for orig_name in raw_data_parsing_config.keys():\n",
    "        if orig_name in sensor_name:\n",
    "            original_sensor_name = orig_name\n",
    "            break\n",
    "    \n",
    "    sensor_corr_params = subject_correction_params.get(original_sensor_name, {})\n",
    "    shift_applied = sensor_corr_params.get('shift', 0)\n",
    "    \n",
    "    print(f\"  📈 {sensor_name}:\")\n",
    "    print(f\"    Samples: {len(data)}\")\n",
    "    print(f\"    Time range: {data.index.min()} to {data.index.max()}\")\n",
    "    print(f\"    Duration: {data.index.max() - data.index.min()}\")\n",
    "    print(f\"    Columns: {list(data.columns)}\")\n",
    "    print(f\"    Time shift applied: {shift_applied}s\")\n",
    "\n",
    "if len(valid_labels) > 0:\n",
    "    print(f\"\\n🏷️ Label Details:\")\n",
    "    print(f\"  Total labels for {SUBJECT_ID}: {len(valid_labels)}\")\n",
    "    \n",
    "    # Time range of labels\n",
    "    label_start = valid_labels['Real_Start_Time'].min()\n",
    "    label_end = valid_labels['Real_End_Time'].max()\n",
    "    print(f\"  Label time range: {label_start} to {label_end}\")\n",
    "    print(f\"  Label duration span: {label_end - label_start}\")\n",
    "    \n",
    "    # Most common labels\n",
    "    print(f\"  Top 5 most common labels:\")\n",
    "    for label, count in valid_labels['Label'].value_counts().head(5).items():\n",
    "        total_duration = 0\n",
    "        label_instances = valid_labels[valid_labels['Label'] == label]\n",
    "        for _, row in label_instances.iterrows():\n",
    "            duration = row['Real_End_Time'] - row['Real_Start_Time']\n",
    "            total_duration += duration.total_seconds()\n",
    "        avg_duration = total_duration / count if count > 0 else 0\n",
    "        print(f\"    📋 {label}: {count} instances (avg: {avg_duration:.1f}s)\")\n",
    "    \n",
    "    # Labels in the data window\n",
    "    window_labels = valid_labels[\n",
    "        (valid_labels['Real_Start_Time'] <= data_window_end) & \n",
    "        (valid_labels['Real_End_Time'] >= data_window_start)\n",
    "    ]\n",
    "    print(f\"  Labels in current data window: {len(window_labels)}\")\n",
    "    \n",
    "    # Color legend\n",
    "    print(f\"\\n🎨 Label Colors ({len(label_colors)} unique labels):\")\n",
    "    for i, (label, color) in enumerate(sorted(label_colors.items())[:10]):\n",
    "        print(f\"  🟦 {label}\")\n",
    "    if len(label_colors) > 10:\n",
    "        print(f\"  ... and {len(label_colors) - 10} more labels\")\n",
    "\n",
    "print(\"\\n🎯 Ready for manual sync event identification with label overlay!\")\n",
    "print(\"Use the interactive plot above to examine each sensor independently.\")\n",
    "print(\"Labels from Final_Labels.csv will be displayed as colored shaded areas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522d736",
   "metadata": {},
   "source": [
    "## 7. Export Corrected Labels\n",
    "\n",
    "Export the label drift corrections for reuse and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42955768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Label Export Tool:\n",
      "  Use this to save your corrected labels for reuse in other workflows\n",
      "  Exports both the corrected CSV and correction parameters for reproducibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e82cd421664409b41fd71b370e9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='💾 Export Corrected Labels', layout=Layout(width='200px'), style=Bu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export corrected labels for reuse\n",
    "def export_corrected_labels():\n",
    "    \"\"\"Export the currently corrected labels to CSV and show drift parameters\"\"\"\n",
    "    \n",
    "    if len(corrected_labels_global) == 0:\n",
    "        print(\"❌ No corrected labels to export\")\n",
    "        return\n",
    "    \n",
    "    # Create export filename\n",
    "    export_filename = f\"{SUBJECT_ID}_corrected_labels.csv\"\n",
    "    export_path = os.path.join(project_root, export_filename)\n",
    "    \n",
    "    # Export corrected labels\n",
    "    try:\n",
    "        corrected_labels_global.to_csv(export_path, index=False)\n",
    "        print(f\"✅ Corrected labels exported to: {export_path}\")\n",
    "        print(f\"📊 Exported {len(corrected_labels_global)} corrected labels\")\n",
    "        \n",
    "        # Show correction summary\n",
    "        print(f\"\\n🔧 Applied Corrections Summary:\")\n",
    "        print(f\"  Manual Offset: {current_correction_log.get('manual_offset_applied', 0)}s\")\n",
    "        if current_correction_log.get('linear_drift_applied', False):\n",
    "            print(f\"  Linear Drift: {label_drift_controls['linear_drift_slider'].value}s over sync period\")\n",
    "        print(f\"  Labels Corrected: {current_correction_log.get('total_labels_corrected', 0)}\")\n",
    "        \n",
    "        # Export correction parameters for reproducibility\n",
    "        correction_params = {\n",
    "            \"subject_id\": SUBJECT_ID,\n",
    "            \"export_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"sync_start_time\": sync_start_time.isoformat(),\n",
    "            \"sync_end_time\": sync_end_time.isoformat(),\n",
    "            \"corrections_applied\": {\n",
    "                \"manual_offset_seconds\": current_correction_log.get('manual_offset_applied', 0),\n",
    "                \"linear_drift_enabled\": current_correction_log.get('linear_drift_applied', False),\n",
    "                \"linear_drift_seconds\": label_drift_controls['linear_drift_slider'].value if current_correction_log.get('linear_drift_applied', False) else 0,\n",
    "                \"total_labels_corrected\": current_correction_log.get('total_labels_corrected', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params_filename = f\"{SUBJECT_ID}_label_correction_params.yaml\"\n",
    "        params_path = os.path.join(project_root, params_filename)\n",
    "        \n",
    "        with open(params_path, 'w') as f:\n",
    "            yaml.dump(correction_params, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"✅ Correction parameters saved to: {params_path}\")\n",
    "        \n",
    "        # Show time shift comparison\n",
    "        if len(valid_labels) > 0:\n",
    "            original_time_range = f\"{valid_labels['Real_Start_Time'].min()} to {valid_labels['Real_End_Time'].max()}\"\n",
    "            corrected_time_range = f\"{corrected_labels_global['Real_Start_Time'].min()} to {corrected_labels_global['Real_End_Time'].max()}\"\n",
    "            \n",
    "            print(f\"\\n⏱️ Time Range Comparison:\")\n",
    "            print(f\"  Original:  {original_time_range}\")\n",
    "            print(f\"  Corrected: {corrected_time_range}\")\n",
    "            \n",
    "            # Calculate total shift\n",
    "            original_start = valid_labels['Real_Start_Time'].min()\n",
    "            corrected_start = corrected_labels_global['Real_Start_Time'].min()\n",
    "            total_shift = (corrected_start - original_start).total_seconds()\n",
    "            print(f\"  Net Shift: {total_shift}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting corrected labels: {e}\")\n",
    "\n",
    "# Create export button\n",
    "export_button = widgets.Button(\n",
    "    description='💾 Export Corrected Labels',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "export_button.on_click(lambda btn: export_corrected_labels())\n",
    "\n",
    "print(\"📤 Label Export Tool:\")\n",
    "print(\"  Use this to save your corrected labels for reuse in other workflows\")\n",
    "print(\"  Exports both the corrected CSV and correction parameters for reproducibility\")\n",
    "\n",
    "display(export_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0978a",
   "metadata": {},
   "source": [
    "## 7. Final Data Visualization Export\n",
    "\n",
    "**Purpose**: Generate comprehensive PDF visualizations of all processed sensor data with labels for final quality check before AI model preprocessing.\n",
    "\n",
    "**Features**:\n",
    "- Create subject-specific folder structure in results directory\n",
    "- One PDF per sensor with all data plotted\n",
    "- 10-minute segments per page in landscape format\n",
    "- Synchronized time axis across all sensors and labels\n",
    "- Labels shown as shaded areas with consistent colors\n",
    "- Page-by-page navigation through entire dataset\n",
    "- Final validation before AI preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66489818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"=== FINAL DATA VISUALIZATION EXPORT ===\")\\nprint(\"🎯 Creating comprehensive PDF plots for quality check before AI preprocessing\")\\n\\n# Create results directory structure\\nresults_base_dir = os.path.join(project_root, \\'stirnimann_r/results\\')\\nsubject_results_dir = os.path.join(results_base_dir, SUBJECT_ID)\\nos.makedirs(subject_results_dir, exist_ok=True)\\n\\nprint(f\"📂 Results directory: {subject_results_dir}\")\\n\\n# Configuration for plotting\\nMINUTES_PER_PAGE = 10  # 10 minutes per page\\nPAGE_WIDTH = 16       # Landscape format\\nPAGE_HEIGHT = 12      # Landscape format\\nDPI = 150            # Good quality for PDFs\\n\\n# Determine overall time range across all sensors\\nprint(\"\\n📊 Determining overall time range...\")\\nall_start_times = []\\nall_end_times = []\\n\\nfor sensor_name, sensor_data in processed_sensors.items():\\n    all_start_times.append(sensor_data.index.min())\\n    all_end_times.append(sensor_data.index.max())\\n\\nif all_start_times and all_end_times:\\n    overall_start = min(all_start_times)\\n    overall_end = max(all_end_times)\\n    total_duration = overall_end - overall_start\\n    \\n    print(f\"⏱️ Overall time range: {overall_start} to {overall_end}\")\\n    print(f\"⏱️ Total duration: {total_duration}\")\\n    \\n    # Calculate number of pages needed\\n    total_minutes = total_duration.total_seconds() / 60\\n    num_pages = math.ceil(total_minutes / MINUTES_PER_PAGE)\\n    \\n    print(f\"📄 Will create {num_pages} pages ({MINUTES_PER_PAGE} minutes each)\")\\n    \\n    # Prepare labels for the entire time range\\n    if len(valid_labels) > 0:\\n        # Filter labels to the overall time range\\n        full_range_labels = valid_labels[\\n            (valid_labels[\\'Real_Start_Time\\'] <= overall_end) & \\n            (valid_labels[\\'Real_End_Time\\'] >= overall_start)\\n        ]\\n        print(f\"🏷️ {len(full_range_labels)} labels in full time range\")\\n    else:\\n        full_range_labels = pd.DataFrame()\\n    \\n    # Process each sensor\\n    for sensor_name, sensor_data in processed_sensors.items():\\n        print(f\"\\n--- Processing sensor: {sensor_name} ---\")\\n        \\n        # Create sensor-specific directory\\n        sensor_dir = os.path.join(subject_results_dir, sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\'))\\n        os.makedirs(sensor_dir, exist_ok=True)\\n        \\n        # Define PDF filename\\n        pdf_filename = f\"{SUBJECT_ID}_{sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\')}_complete_data.pdf\"\\n        pdf_path = os.path.join(sensor_dir, pdf_filename)\\n        \\n        print(f\"📄 Creating PDF: {pdf_path}\")\\n        \\n        # Get numeric columns for this sensor\\n        numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\\n        print(f\"📈 Plotting {len(numeric_cols)} channels: {list(numeric_cols)}\")\\n        \\n        # Create PDF with multiple pages\\n        with PdfPages(pdf_path) as pdf:\\n            for page_num in range(num_pages):\\n                print(f\"  📄 Creating page {page_num + 1}/{num_pages}...\")\\n                \\n                # Calculate time window for this page\\n                page_start = overall_start + pd.Timedelta(minutes=page_num * MINUTES_PER_PAGE)\\n                page_end = overall_start + pd.Timedelta(minutes=(page_num + 1) * MINUTES_PER_PAGE)\\n                \\n                # Don\\'t go beyond the actual data range\\n                page_end = min(page_end, overall_end)\\n                \\n                # Filter sensor data for this page\\n                page_mask = (sensor_data.index >= page_start) & (sensor_data.index <= page_end)\\n                page_data = sensor_data[page_mask]\\n                \\n                # Filter labels for this page\\n                if len(full_range_labels) > 0:\\n                    page_labels = full_range_labels[\\n                        (full_range_labels[\\'Real_Start_Time\\'] <= page_end) & \\n                        (full_range_labels[\\'Real_End_Time\\'] >= page_start)\\n                    ]\\n                else:\\n                    page_labels = pd.DataFrame()\\n                \\n                # Create figure\\n                fig, ax = plt.subplots(1, 1, figsize=(PAGE_WIDTH, PAGE_HEIGHT), dpi=DPI)\\n                \\n                # Plot sensor data\\n                if not page_data.empty and len(numeric_cols) > 0:\\n                    for col in numeric_cols:\\n                        ax.plot(page_data.index, page_data[col], \\n                               label=col, alpha=0.8, linewidth=1.5)\\n                    \\n                    # Get y-axis limits for label positioning\\n                    y_min, y_max = ax.get_ylim()\\n                    y_range = y_max - y_min\\n                    \\n                    # Add label shading\\n                    if len(page_labels) > 0:\\n                        label_count = {}\\n                        for _, label_row in page_labels.iterrows():\\n                            label_name = label_row[\\'Label\\']\\n                            start_time = max(label_row[\\'Real_Start_Time\\'], page_start)\\n                            end_time = min(label_row[\\'Real_End_Time\\'], page_end)\\n                            \\n                            if start_time < end_time:  # Valid time range\\n                                color = label_colors.get(label_name, \\'gray\\')\\n                                \\n                                # Count occurrences for legend\\n                                if label_name not in label_count:\\n                                    label_count[label_name] = 0\\n                                label_count[label_name] += 1\\n                                \\n                                # Add shaded region\\n                                ax.axvspan(start_time, end_time, \\n                                         alpha=0.3, \\n                                         color=color,\\n                                         label=f\\'{label_name}\\' if label_count[label_name] == 1 else \"\",\\n                                         zorder=0)  # Behind the data\\n                                \\n                                # Add label text for longer labels\\n                                duration = end_time - start_time\\n                                if duration > pd.Timedelta(minutes=1):  # Only show text for labels > 1 minute\\n                                    mid_time = start_time + (end_time - start_time) / 2\\n                                    y_pos = y_max - y_range * 0.05\\n                                    \\n                                    ax.text(mid_time, y_pos, label_name, \\n                                           ha=\\'center\\', va=\\'top\\', rotation=0,\\n                                           fontsize=10, alpha=0.9,\\n                                           bbox=dict(boxstyle=\\'round,pad=0.3\\', \\n                                                   facecolor=\\'white\\', alpha=0.8))\\n                    \\n                    # Add sync event markers if in range\\n                    if page_start <= sync_start_time <= page_end:\\n                        ax.axvline(sync_start_time, color=\\'red\\', linestyle=\\'--\\', \\n                                 linewidth=3, alpha=0.9, label=\\'🎯 Sync Start\\')\\n                    \\n                    if page_start <= sync_end_time <= page_end:\\n                        ax.axvline(sync_end_time, color=\\'darkred\\', linestyle=\\'--\\', \\n                                 linewidth=3, alpha=0.9, label=\\'🎯 Sync End\\')\\n                    \\n                else:\\n                    # No data on this page\\n                    ax.text(0.5, 0.5, f\\'No {sensor_name} data in this time window\\', \\n                           ha=\\'center\\', va=\\'center\\', transform=ax.transAxes,\\n                           fontsize=16, alpha=0.7)\\n                \\n                # Set time axis limits to exactly match the page window\\n                ax.set_xlim(page_start, page_end)\\n                \\n                # Formatting\\n                page_title = f\\'{sensor_name} - Page {page_num + 1}/{num_pages}\\n\\'\\n                page_title += f\\'Time: {page_start.strftime(\"%Y-%m-%d %H:%M:%S\")} to {page_end.strftime(\"%H:%M:%S\")}\\'\\n                if len(page_labels) > 0:\\n                    page_title += f\\' | {len(page_labels)} labels\\'\\n                \\n                ax.set_title(page_title, fontsize=14, fontweight=\\'bold\\')\\n                ax.set_xlabel(\\'Time\\', fontsize=12)\\n                ax.set_ylabel(\\'Value\\', fontsize=12)\\n                ax.grid(True, alpha=0.3)\\n                \\n                # Format time axis\\n                ax.xaxis.set_major_formatter(mdates.DateFormatter(\\'%H:%M\\'))\\n                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, MINUTES_PER_PAGE//10)))\\n                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\\n                \\n                # Legend (only if not too many items)\\n                handles, labels = ax.get_legend_handles_labels()\\n                if len(handles) <= 20:  # Reasonable number for legend\\n                    ax.legend(bbox_to_anchor=(1.02, 1), loc=\\'upper left\\', fontsize=10)\\n                \\n                # Add metadata text\\n                metadata_text = f\\'Subject: {SUBJECT_ID} | Sensor: {sensor_name}\\n\\'\\n                metadata_text += f\\'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\'\\n                metadata_text += f\\'Total Duration: {total_duration} | Target Freq: {TARGET_FREQUENCY}Hz\\'\\n                \\n                fig.text(0.02, 0.02, metadata_text, fontsize=8, alpha=0.7,\\n                        verticalalignment=\\'bottom\\')\\n                \\n                plt.tight_layout()\\n                plt.subplots_adjust(right=0.85, bottom=0.1)\\n                \\n                # Save page to PDF\\n                pdf.savefig(fig, dpi=DPI, bbox_inches=\\'tight\\')\\n                plt.close(fig)\\n        \\n        print(f\"✅ PDF saved: {pdf_path}\")\\n        print(f\"   📄 {num_pages} pages, {MINUTES_PER_PAGE} minutes each\")\\n        if len(numeric_cols) > 0:\\n            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\\n            print(f\"   ⏱️ Sensor duration: {sensor_duration}\")\\n            print(f\"   📊 {len(sensor_data)} samples, {len(numeric_cols)} channels\")\\n    \\n    # Create summary report\\n    print(f\"\\n📋 Creating summary report...\")\\n    summary_path = os.path.join(subject_results_dir, f\"{SUBJECT_ID}_data_summary.txt\")\\n    \\n    with open(summary_path, \\'w\\') as f:\\n        f.write(f\"DATA VISUALIZATION SUMMARY REPORT\\n\")\\n        f.write(f\"=================================\\n\\n\")\\n        f.write(f\"Subject: {SUBJECT_ID}\\n\")\\n        f.write(f\"Generated: {datetime.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')}\\n\")\\n        f.write(f\"Project Root: {project_root}\\n\\n\")\\n        \\n        f.write(f\"TIME RANGE:\\n\")\\n        f.write(f\"  Overall Start: {overall_start}\\n\")\\n        f.write(f\"  Overall End: {overall_end}\\n\")\\n        f.write(f\"  Total Duration: {total_duration}\\n\")\\n        f.write(f\"  Pages Generated: {num_pages} ({MINUTES_PER_PAGE} minutes each)\\n\\n\")\\n        \\n        f.write(f\"SYNC EVENTS:\\n\")\\n        f.write(f\"  Sync Start: {sync_start_time}\\n\")\\n        f.write(f\"  Sync End: {sync_end_time}\\n\")\\n        f.write(f\"  Sync Duration: {sync_end_time - sync_start_time}\\n\\n\")\\n        \\n        f.write(f\"SENSORS PROCESSED ({len(processed_sensors)}):\\n\")\\n        for sensor_name, sensor_data in processed_sensors.items():\\n            numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\\n            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\\n            f.write(f\"  📈 {sensor_name}:\\n\")\\n            f.write(f\"    Samples: {len(sensor_data)}\\n\")\\n            f.write(f\"    Duration: {sensor_duration}\\n\")\\n            f.write(f\"    Channels: {len(numeric_cols)} {list(numeric_cols)}\\n\")\\n            f.write(f\"    Time Range: {sensor_data.index.min()} to {sensor_data.index.max()}\\n\")\\n            f.write(f\"    PDF: {sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\')}_complete_data.pdf\\n\\n\")\\n        \\n        if len(full_range_labels) > 0:\\n            f.write(f\"LABELS ({len(full_range_labels)}):\\n\")\\n            label_summary = full_range_labels[\\'Label\\'].value_counts()\\n            for label, count in label_summary.items():\\n                f.write(f\"  🏷️ {label}: {count} instances\\n\")\\n            f.write(f\"\\n  Label Time Range: {full_range_labels[\\'Real_Start_Time\\'].min()} to {full_range_labels[\\'Real_End_Time\\'].max()}\\n\")\\n        else:\\n            f.write(f\"LABELS: No labels available\\n\")\\n        \\n        f.write(f\"\\nCONFIGURATION:\\n\")\\n        f.write(f\"  Target Frequency: {TARGET_FREQUENCY}Hz\\n\")\\n        f.write(f\"  Minutes per Page: {MINUTES_PER_PAGE}\\n\")\\n        f.write(f\"  Page Format: {PAGE_WIDTH}x{PAGE_HEIGHT} inches (landscape)\\n\")\\n        f.write(f\"  DPI: {DPI}\\n\")\\n        \\n        # Time corrections applied\\n        f.write(f\"\\nTIME CORRECTIONS APPLIED:\\n\")\\n        subject_correction_params = sync_params.get(SUBJECT_ID, {})\\n        for sensor_name in processed_sensors.keys():\\n            # Find original sensor name\\n            original_sensor_name = sensor_name\\n            for orig_name in raw_data_parsing_config.keys():\\n                if orig_name in sensor_name:\\n                    original_sensor_name = orig_name\\n                    break\\n            \\n            sensor_corr_params = subject_correction_params.get(original_sensor_name, {})\\n            shift_applied = sensor_corr_params.get(\\'shift\\', 0)\\n            unit = sensor_corr_params.get(\\'unit\\', \\'s\\')\\n            \\n            f.write(f\"  {sensor_name}: {shift_applied}{unit} shift\")\\n            \\n            # Check for drift correction\\n            drift_params = sensor_corr_params.get(\\'drift\\')\\n            if drift_params and all(k in drift_params for k in [\\'t0\\', \\'t1\\', \\'drift_secs\\']):\\n                f.write(f\", {drift_params[\\'drift_secs\\']}s drift correction\")\\n            f.write(f\"\\n\")\\n    \\n    print(f\"✅ Summary report saved: {summary_path}\")\\n    \\n    print(f\"\\n🎯 FINAL DATA VISUALIZATION COMPLETE!\")\\n    print(f\"📂 Results directory: {subject_results_dir}\")\\n    print(f\"📄 PDFs created: {len(processed_sensors)} (one per sensor)\")\\n    print(f\"📋 Pages per PDF: {num_pages} ({MINUTES_PER_PAGE} minutes each)\")\\n    print(f\"⏱️ Total time span: {total_duration}\")\\n    print(f\"🏷️ Labels included: {len(full_range_labels)}\")\\n    print(f\"\\n✅ Ready for AI model preprocessing!\")\\n    print(f\"\\n💡 Use these PDFs to:\")\\n    print(f\"  • Verify data quality across all sensors\")\\n    print(f\"  • Check time synchronization accuracy\")\\n    print(f\"  • Validate label alignment with sensor data\")\\n    print(f\"  • Identify any remaining artifacts or issues\")\\n    print(f\"  • Confirm preprocessing parameters\")\\n\\nelse:\\n    print(\"❌ No sensor data available for visualization\")\\n    \\nprint(f\"\\n📁 Results structure:\")\\nprint(f\"  {subject_results_dir}/\")\\nfor sensor_name in processed_sensors.keys():\\n    clean_name = sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\')\\n    print(f\"  ├── {clean_name}/\")\\n    print(f\"  │   └── {SUBJECT_ID}_{clean_name}_complete_data.pdf\")\\nprint(f\"  └── {SUBJECT_ID}_data_summary.txt\")\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comprehensive visualization export for final data quality check\n",
    "import matplotlib.backends.backend_pdf as backend_pdf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import math\n",
    "\"\"\"\n",
    "print(\"=== FINAL DATA VISUALIZATION EXPORT ===\")\n",
    "print(\"🎯 Creating comprehensive PDF plots for quality check before AI preprocessing\")\n",
    "\n",
    "# Create results directory structure\n",
    "results_base_dir = os.path.join(project_root, 'stirnimann_r/results')\n",
    "subject_results_dir = os.path.join(results_base_dir, SUBJECT_ID)\n",
    "os.makedirs(subject_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Results directory: {subject_results_dir}\")\n",
    "\n",
    "# Configuration for plotting\n",
    "MINUTES_PER_PAGE = 10  # 10 minutes per page\n",
    "PAGE_WIDTH = 16       # Landscape format\n",
    "PAGE_HEIGHT = 12      # Landscape format\n",
    "DPI = 150            # Good quality for PDFs\n",
    "\n",
    "# Determine overall time range across all sensors\n",
    "print(\"\\n📊 Determining overall time range...\")\n",
    "all_start_times = []\n",
    "all_end_times = []\n",
    "\n",
    "for sensor_name, sensor_data in processed_sensors.items():\n",
    "    all_start_times.append(sensor_data.index.min())\n",
    "    all_end_times.append(sensor_data.index.max())\n",
    "\n",
    "if all_start_times and all_end_times:\n",
    "    overall_start = min(all_start_times)\n",
    "    overall_end = max(all_end_times)\n",
    "    total_duration = overall_end - overall_start\n",
    "    \n",
    "    print(f\"⏱️ Overall time range: {overall_start} to {overall_end}\")\n",
    "    print(f\"⏱️ Total duration: {total_duration}\")\n",
    "    \n",
    "    # Calculate number of pages needed\n",
    "    total_minutes = total_duration.total_seconds() / 60\n",
    "    num_pages = math.ceil(total_minutes / MINUTES_PER_PAGE)\n",
    "    \n",
    "    print(f\"📄 Will create {num_pages} pages ({MINUTES_PER_PAGE} minutes each)\")\n",
    "    \n",
    "    # Prepare labels for the entire time range\n",
    "    if len(valid_labels) > 0:\n",
    "        # Filter labels to the overall time range\n",
    "        full_range_labels = valid_labels[\n",
    "            (valid_labels['Real_Start_Time'] <= overall_end) & \n",
    "            (valid_labels['Real_End_Time'] >= overall_start)\n",
    "        ]\n",
    "        print(f\"🏷️ {len(full_range_labels)} labels in full time range\")\n",
    "    else:\n",
    "        full_range_labels = pd.DataFrame()\n",
    "    \n",
    "    # Process each sensor\n",
    "    for sensor_name, sensor_data in processed_sensors.items():\n",
    "        print(f\"\\n--- Processing sensor: {sensor_name} ---\")\n",
    "        \n",
    "        # Create sensor-specific directory\n",
    "        sensor_dir = os.path.join(subject_results_dir, sensor_name.replace('/', '_').replace(' ', '_'))\n",
    "        os.makedirs(sensor_dir, exist_ok=True)\n",
    "        \n",
    "        # Define PDF filename\n",
    "        pdf_filename = f\"{SUBJECT_ID}_{sensor_name.replace('/', '_').replace(' ', '_')}_complete_data.pdf\"\n",
    "        pdf_path = os.path.join(sensor_dir, pdf_filename)\n",
    "        \n",
    "        print(f\"📄 Creating PDF: {pdf_path}\")\n",
    "        \n",
    "        # Get numeric columns for this sensor\n",
    "        numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\n",
    "        print(f\"📈 Plotting {len(numeric_cols)} channels: {list(numeric_cols)}\")\n",
    "        \n",
    "        # Create PDF with multiple pages\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            for page_num in range(num_pages):\n",
    "                print(f\"  📄 Creating page {page_num + 1}/{num_pages}...\")\n",
    "                \n",
    "                # Calculate time window for this page\n",
    "                page_start = overall_start + pd.Timedelta(minutes=page_num * MINUTES_PER_PAGE)\n",
    "                page_end = overall_start + pd.Timedelta(minutes=(page_num + 1) * MINUTES_PER_PAGE)\n",
    "                \n",
    "                # Don't go beyond the actual data range\n",
    "                page_end = min(page_end, overall_end)\n",
    "                \n",
    "                # Filter sensor data for this page\n",
    "                page_mask = (sensor_data.index >= page_start) & (sensor_data.index <= page_end)\n",
    "                page_data = sensor_data[page_mask]\n",
    "                \n",
    "                # Filter labels for this page\n",
    "                if len(full_range_labels) > 0:\n",
    "                    page_labels = full_range_labels[\n",
    "                        (full_range_labels['Real_Start_Time'] <= page_end) & \n",
    "                        (full_range_labels['Real_End_Time'] >= page_start)\n",
    "                    ]\n",
    "                else:\n",
    "                    page_labels = pd.DataFrame()\n",
    "                \n",
    "                # Create figure\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(PAGE_WIDTH, PAGE_HEIGHT), dpi=DPI)\n",
    "                \n",
    "                # Plot sensor data\n",
    "                if not page_data.empty and len(numeric_cols) > 0:\n",
    "                    for col in numeric_cols:\n",
    "                        ax.plot(page_data.index, page_data[col], \n",
    "                               label=col, alpha=0.8, linewidth=1.5)\n",
    "                    \n",
    "                    # Get y-axis limits for label positioning\n",
    "                    y_min, y_max = ax.get_ylim()\n",
    "                    y_range = y_max - y_min\n",
    "                    \n",
    "                    # Add label shading\n",
    "                    if len(page_labels) > 0:\n",
    "                        label_count = {}\n",
    "                        for _, label_row in page_labels.iterrows():\n",
    "                            label_name = label_row['Label']\n",
    "                            start_time = max(label_row['Real_Start_Time'], page_start)\n",
    "                            end_time = min(label_row['Real_End_Time'], page_end)\n",
    "                            \n",
    "                            if start_time < end_time:  # Valid time range\n",
    "                                color = label_colors.get(label_name, 'gray')\n",
    "                                \n",
    "                                # Count occurrences for legend\n",
    "                                if label_name not in label_count:\n",
    "                                    label_count[label_name] = 0\n",
    "                                label_count[label_name] += 1\n",
    "                                \n",
    "                                # Add shaded region\n",
    "                                ax.axvspan(start_time, end_time, \n",
    "                                         alpha=0.3, \n",
    "                                         color=color,\n",
    "                                         label=f'{label_name}' if label_count[label_name] == 1 else \"\",\n",
    "                                         zorder=0)  # Behind the data\n",
    "                                \n",
    "                                # Add label text for longer labels\n",
    "                                duration = end_time - start_time\n",
    "                                if duration > pd.Timedelta(minutes=1):  # Only show text for labels > 1 minute\n",
    "                                    mid_time = start_time + (end_time - start_time) / 2\n",
    "                                    y_pos = y_max - y_range * 0.05\n",
    "                                    \n",
    "                                    ax.text(mid_time, y_pos, label_name, \n",
    "                                           ha='center', va='top', rotation=0,\n",
    "                                           fontsize=10, alpha=0.9,\n",
    "                                           bbox=dict(boxstyle='round,pad=0.3', \n",
    "                                                   facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    # Add sync event markers if in range\n",
    "                    if page_start <= sync_start_time <= page_end:\n",
    "                        ax.axvline(sync_start_time, color='red', linestyle='--', \n",
    "                                 linewidth=3, alpha=0.9, label='🎯 Sync Start')\n",
    "                    \n",
    "                    if page_start <= sync_end_time <= page_end:\n",
    "                        ax.axvline(sync_end_time, color='darkred', linestyle='--', \n",
    "                                 linewidth=3, alpha=0.9, label='🎯 Sync End')\n",
    "                    \n",
    "                else:\n",
    "                    # No data on this page\n",
    "                    ax.text(0.5, 0.5, f'No {sensor_name} data in this time window', \n",
    "                           ha='center', va='center', transform=ax.transAxes,\n",
    "                           fontsize=16, alpha=0.7)\n",
    "                \n",
    "                # Set time axis limits to exactly match the page window\n",
    "                ax.set_xlim(page_start, page_end)\n",
    "                \n",
    "                # Formatting\n",
    "                page_title = f'{sensor_name} - Page {page_num + 1}/{num_pages}\\n'\n",
    "                page_title += f'Time: {page_start.strftime(\"%Y-%m-%d %H:%M:%S\")} to {page_end.strftime(\"%H:%M:%S\")}'\n",
    "                if len(page_labels) > 0:\n",
    "                    page_title += f' | {len(page_labels)} labels'\n",
    "                \n",
    "                ax.set_title(page_title, fontsize=14, fontweight='bold')\n",
    "                ax.set_xlabel('Time', fontsize=12)\n",
    "                ax.set_ylabel('Value', fontsize=12)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Format time axis\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, MINUTES_PER_PAGE//10)))\n",
    "                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "                \n",
    "                # Legend (only if not too many items)\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                if len(handles) <= 20:  # Reasonable number for legend\n",
    "                    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)\n",
    "                \n",
    "                # Add metadata text\n",
    "                metadata_text = f'Subject: {SUBJECT_ID} | Sensor: {sensor_name}\\n'\n",
    "                metadata_text += f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n'\n",
    "                metadata_text += f'Total Duration: {total_duration} | Target Freq: {TARGET_FREQUENCY}Hz'\n",
    "                \n",
    "                fig.text(0.02, 0.02, metadata_text, fontsize=8, alpha=0.7,\n",
    "                        verticalalignment='bottom')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(right=0.85, bottom=0.1)\n",
    "                \n",
    "                # Save page to PDF\n",
    "                pdf.savefig(fig, dpi=DPI, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "        \n",
    "        print(f\"✅ PDF saved: {pdf_path}\")\n",
    "        print(f\"   📄 {num_pages} pages, {MINUTES_PER_PAGE} minutes each\")\n",
    "        if len(numeric_cols) > 0:\n",
    "            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\n",
    "            print(f\"   ⏱️ Sensor duration: {sensor_duration}\")\n",
    "            print(f\"   📊 {len(sensor_data)} samples, {len(numeric_cols)} channels\")\n",
    "    \n",
    "    # Create summary report\n",
    "    print(f\"\\n📋 Creating summary report...\")\n",
    "    summary_path = os.path.join(subject_results_dir, f\"{SUBJECT_ID}_data_summary.txt\")\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(f\"DATA VISUALIZATION SUMMARY REPORT\\n\")\n",
    "        f.write(f\"=================================\\n\\n\")\n",
    "        f.write(f\"Subject: {SUBJECT_ID}\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Project Root: {project_root}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"TIME RANGE:\\n\")\n",
    "        f.write(f\"  Overall Start: {overall_start}\\n\")\n",
    "        f.write(f\"  Overall End: {overall_end}\\n\")\n",
    "        f.write(f\"  Total Duration: {total_duration}\\n\")\n",
    "        f.write(f\"  Pages Generated: {num_pages} ({MINUTES_PER_PAGE} minutes each)\\n\\n\")\n",
    "        \n",
    "        f.write(f\"SYNC EVENTS:\\n\")\n",
    "        f.write(f\"  Sync Start: {sync_start_time}\\n\")\n",
    "        f.write(f\"  Sync End: {sync_end_time}\\n\")\n",
    "        f.write(f\"  Sync Duration: {sync_end_time - sync_start_time}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"SENSORS PROCESSED ({len(processed_sensors)}):\\n\")\n",
    "        for sensor_name, sensor_data in processed_sensors.items():\n",
    "            numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\n",
    "            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\n",
    "            f.write(f\"  📈 {sensor_name}:\\n\")\n",
    "            f.write(f\"    Samples: {len(sensor_data)}\\n\")\n",
    "            f.write(f\"    Duration: {sensor_duration}\\n\")\n",
    "            f.write(f\"    Channels: {len(numeric_cols)} {list(numeric_cols)}\\n\")\n",
    "            f.write(f\"    Time Range: {sensor_data.index.min()} to {sensor_data.index.max()}\\n\")\n",
    "            f.write(f\"    PDF: {sensor_name.replace('/', '_').replace(' ', '_')}_complete_data.pdf\\n\\n\")\n",
    "        \n",
    "        if len(full_range_labels) > 0:\n",
    "            f.write(f\"LABELS ({len(full_range_labels)}):\\n\")\n",
    "            label_summary = full_range_labels['Label'].value_counts()\n",
    "            for label, count in label_summary.items():\n",
    "                f.write(f\"  🏷️ {label}: {count} instances\\n\")\n",
    "            f.write(f\"\\n  Label Time Range: {full_range_labels['Real_Start_Time'].min()} to {full_range_labels['Real_End_Time'].max()}\\n\")\n",
    "        else:\n",
    "            f.write(f\"LABELS: No labels available\\n\")\n",
    "        \n",
    "        f.write(f\"\\nCONFIGURATION:\\n\")\n",
    "        f.write(f\"  Target Frequency: {TARGET_FREQUENCY}Hz\\n\")\n",
    "        f.write(f\"  Minutes per Page: {MINUTES_PER_PAGE}\\n\")\n",
    "        f.write(f\"  Page Format: {PAGE_WIDTH}x{PAGE_HEIGHT} inches (landscape)\\n\")\n",
    "        f.write(f\"  DPI: {DPI}\\n\")\n",
    "        \n",
    "        # Time corrections applied\n",
    "        f.write(f\"\\nTIME CORRECTIONS APPLIED:\\n\")\n",
    "        subject_correction_params = sync_params.get(SUBJECT_ID, {})\n",
    "        for sensor_name in processed_sensors.keys():\n",
    "            # Find original sensor name\n",
    "            original_sensor_name = sensor_name\n",
    "            for orig_name in raw_data_parsing_config.keys():\n",
    "                if orig_name in sensor_name:\n",
    "                    original_sensor_name = orig_name\n",
    "                    break\n",
    "            \n",
    "            sensor_corr_params = subject_correction_params.get(original_sensor_name, {})\n",
    "            shift_applied = sensor_corr_params.get('shift', 0)\n",
    "            unit = sensor_corr_params.get('unit', 's')\n",
    "            \n",
    "            f.write(f\"  {sensor_name}: {shift_applied}{unit} shift\")\n",
    "            \n",
    "            # Check for drift correction\n",
    "            drift_params = sensor_corr_params.get('drift')\n",
    "            if drift_params and all(k in drift_params for k in ['t0', 't1', 'drift_secs']):\n",
    "                f.write(f\", {drift_params['drift_secs']}s drift correction\")\n",
    "            f.write(f\"\\n\")\n",
    "    \n",
    "    print(f\"✅ Summary report saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\n🎯 FINAL DATA VISUALIZATION COMPLETE!\")\n",
    "    print(f\"📂 Results directory: {subject_results_dir}\")\n",
    "    print(f\"📄 PDFs created: {len(processed_sensors)} (one per sensor)\")\n",
    "    print(f\"📋 Pages per PDF: {num_pages} ({MINUTES_PER_PAGE} minutes each)\")\n",
    "    print(f\"⏱️ Total time span: {total_duration}\")\n",
    "    print(f\"🏷️ Labels included: {len(full_range_labels)}\")\n",
    "    print(f\"\\n✅ Ready for AI model preprocessing!\")\n",
    "    print(f\"\\n💡 Use these PDFs to:\")\n",
    "    print(f\"  • Verify data quality across all sensors\")\n",
    "    print(f\"  • Check time synchronization accuracy\")\n",
    "    print(f\"  • Validate label alignment with sensor data\")\n",
    "    print(f\"  • Identify any remaining artifacts or issues\")\n",
    "    print(f\"  • Confirm preprocessing parameters\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No sensor data available for visualization\")\n",
    "    \n",
    "print(f\"\\n📁 Results structure:\")\n",
    "print(f\"  {subject_results_dir}/\")\n",
    "for sensor_name in processed_sensors.keys():\n",
    "    clean_name = sensor_name.replace('/', '_').replace(' ', '_')\n",
    "    print(f\"  ├── {clean_name}/\")\n",
    "    print(f\"  │   └── {SUBJECT_ID}_{clean_name}_complete_data.pdf\")\n",
    "print(f\"  └── {SUBJECT_ID}_data_summary.txt\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c695ac",
   "metadata": {},
   "source": [
    "## 8. Combined Data Table Creation\n",
    "\n",
    "**Purpose**: Create a single synchronized table combining all sensor data with labels using a shared timestamp axis.\n",
    "\n",
    "**Features**:\n",
    "- Unified timestamp axis across all sensors\n",
    "- All sensor channels as separate columns\n",
    "- Label column indicating activity at each timestamp\n",
    "- Empty label cells when no activity is present\n",
    "- Saved as PKL file for efficient loading in AI preprocessing\n",
    "- Perfect synchronization for machine learning workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Results directory: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036\n",
      "=== COMBINED DATA TABLE CREATION ===\n",
      "🔄 Creating unified table with synchronized timestamps, all sensors, and labels\n",
      "📊 Processing 7 sensors...\n",
      "\n",
      "📅 Step 1: Determining longest time range...\n",
      "  📈 corsano_wrist: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "  📈 cosinuss_ear: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "  📈 mbient_acc: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "  📈 mbient_gyro: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "  📈 vivalnk_acc: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "  📈 sensomative_bottom: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "  📈 corsano_bioz: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "\n",
      "⏱️ Longest time range (union of all sensors):\n",
      "  Start: 2024-02-06 08:51:10.002000093\n",
      "  End: 2024-02-08 11:23:35\n",
      "  Duration: 2 days 02:32:24.997999907\n",
      "  ℹ️ Sensors not covering full range will have NaN values in gaps\n",
      "\n",
      "⚙️ Step 2: Creating unified timestamp index at 25Hz...\n",
      "  📊 Created 4548624 timestamps\n",
      "  🕐 Frequency: 25Hz\n",
      "  📏 Interval: 0 days 00:00:00.040000017\n",
      "\n",
      "🔄 Step 3: Resampling and aligning sensor data...\n",
      "  📈 Processing corsano_wrist...\n",
      "    📊 3 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "    🎯 Coverage in full range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "    ✅ 3 channels added\n",
      "    📊 Data coverage: 75.0% (10238844/13645872 samples)\n",
      "    ⏰ Time coverage: 98.1% of full range\n",
      "    🕳️ 3407028 samples filled with NaN (outside sensor range)\n",
      "  📈 Processing cosinuss_ear...\n",
      "    📊 3 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "    🎯 Coverage in full range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "    ✅ 3 channels added\n",
      "    📊 Data coverage: 30.5% (4164582/13645872 samples)\n",
      "    ⏰ Time coverage: 69.4% of full range\n",
      "    🕳️ 9481290 samples filled with NaN (outside sensor range)\n",
      "  📈 Processing mbient_acc...\n",
      "    📊 3 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    🎯 Coverage in full range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    ✅ 3 channels added\n",
      "    📊 Data coverage: 100.0% (13645872/13645872 samples)\n",
      "    ⏰ Time coverage: 100.0% of full range\n",
      "  📈 Processing mbient_gyro...\n",
      "    📊 3 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    🎯 Coverage in full range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    ✅ 3 channels added\n",
      "    📊 Data coverage: 100.0% (13645872/13645872 samples)\n",
      "    ⏰ Time coverage: 100.0% of full range\n",
      "  📈 Processing vivalnk_acc...\n",
      "    📊 3 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "    🎯 Coverage in full range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "    ✅ 3 channels added\n",
      "    📊 Data coverage: 53.2% (7260561/13645872 samples)\n",
      "    ⏰ Time coverage: 98.1% of full range\n",
      "    🕳️ 6385311 samples filled with NaN (outside sensor range)\n",
      "  📈 Processing sensomative_bottom...\n",
      "    📊 11 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "    🎯 Coverage in full range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "    ✅ 11 channels added\n",
      "    📊 Data coverage: 77.4% (38707042/50034864 samples)\n",
      "    ⏰ Time coverage: 100.0% of full range\n",
      "    🕳️ 11327822 samples filled with NaN (outside sensor range)\n",
      "  📈 Processing corsano_bioz...\n",
      "    📊 3 numeric columns\n",
      "    📅 Sensor range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "    🎯 Coverage in full range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "    ✅ 3 channels added\n",
      "    📊 Data coverage: 75.7% (10333056/13645872 samples)\n",
      "    ⏰ Time coverage: 98.1% of full range\n",
      "    🕳️ 3312816 samples filled with NaN (outside sensor range)\n",
      "\n",
      "🏷️ Step 4: Adding corrected label information...\n",
      "  📋 Processing 258 corrected labels...\n",
      "  🔧 Label corrections applied:\n",
      "  🎯 258 corrected labels in longest time range\n",
      "  ✅ 2328938 timestamps labeled\n",
      "  ⚠️ 20499 overlapping label instances handled\n",
      "  📊 Label coverage: 50.8% (2308439/4548624 timestamps)\n",
      "  📋 Label distribution (top 10):\n",
      "    🏷️ dark: 1793777 samples (71751.1s)\n",
      "    🏷️ conversation: 184311 samples (7372.4s)\n",
      "    🏷️ cycling: 131739 samples (5269.6s)\n",
      "    🏷️ reading_newspaper: 46272 samples (1850.9s)\n",
      "    🏷️ eating: 42397 samples (1695.9s)\n",
      "    🏷️ self_propulsion: 34199 samples (1368.0s)\n",
      "    🏷️ eating+conversation: 16874 samples (675.0s)\n",
      "    🏷️ sitting_wheelchair: 15350 samples (614.0s)\n",
      "    🏷️ toilet_routine: 13474 samples (539.0s)\n",
      "    🏷️ assisted_propulsion+using_phone: 3175 samples (127.0s)\n",
      "    ... and 21 more labels\n",
      "\n",
      "📊 Step 5: Data quality summary...\n",
      "  📋 Final combined table:\n",
      "    Timestamps: 4548624\n",
      "    Sensor columns: 29\n",
      "    Label column: 1\n",
      "    Total columns: 30\n",
      "    Time range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:35\n",
      "    Duration: 2 days 02:32:24.997999907\n",
      "    Frequency: 25Hz\n",
      "  🕳️ Missing data summary (NaN values where sensors don't cover full range):\n",
      "    corsano_wrist_wrist_acc_x: 1135676 samples (25.0%)\n",
      "    corsano_wrist_wrist_acc_y: 1135676 samples (25.0%)\n",
      "    corsano_wrist_wrist_acc_z: 1135676 samples (25.0%)\n",
      "    cosinuss_ear_ear_acc_x: 3160430 samples (69.5%)\n",
      "    cosinuss_ear_ear_acc_y: 3160430 samples (69.5%)\n",
      "    cosinuss_ear_ear_acc_z: 3160430 samples (69.5%)\n",
      "    vivalnk_acc_vivalnk_acc_x: 2128437 samples (46.8%)\n",
      "    vivalnk_acc_vivalnk_acc_y: 2128437 samples (46.8%)\n",
      "    vivalnk_acc_vivalnk_acc_z: 2128437 samples (46.8%)\n",
      "    sensomative_bottom_bottom_value_1: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_2: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_3: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_4: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_5: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_6: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_7: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_8: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_9: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_10: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_11: 1029802 samples (22.6%)\n",
      "    corsano_bioz_bioz_acc_x: 1104272 samples (24.3%)\n",
      "    corsano_bioz_bioz_acc_y: 1104272 samples (24.3%)\n",
      "    corsano_bioz_bioz_acc_z: 1104272 samples (24.3%)\n",
      "\n",
      "💾 Step 6: Saving combined data...\n",
      "✅ Combined data saved: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data.pkl\n",
      "  📁 File size: 1063.2 MB\n",
      "✅ Metadata saved: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data_metadata.json\n",
      "\n",
      "✅ COMBINED DATA TABLE COMPLETE!\n",
      "📂 Saved to: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data.pkl\n",
      "📊 Shape: (4548624, 30)\n",
      "⏱️ Time span: 2 days 02:32:24.997999907\n",
      "🎯 Frequency: 25Hz\n",
      "📈 Sensors: 7\n",
      "🏷️ Labels: Yes\n",
      "🕳️ NaN handling: Gaps filled for sensors not covering full time range\n",
      "\n",
      "💡 Usage for AI preprocessing:\n",
      "  import pickle\n",
      "  with open('/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data.pkl', 'rb') as f:\n",
      "      data = pickle.load(f)\n",
      "  # data is a pandas DataFrame with:\n",
      "  # - Index: synchronized timestamps (longest range)\n",
      "  # - Columns: all sensor channels + 'Label'\n",
      "  # - NaN values where sensors don't cover full range\n",
      "  # - Ready for feature extraction and ML\n",
      "\n",
      "📋 Column structure:\n",
      "  Sensor columns (29):\n",
      "    📈 corsano_wrist: 3 channels (covers 98.1% of time range)\n",
      "    📈 cosinuss_ear: 3 channels (covers 69.4% of time range)\n",
      "    📈 mbient_acc: 3 channels (covers 100.0% of time range)\n",
      "    📈 mbient_gyro: 3 channels (covers 100.0% of time range)\n",
      "    📈 vivalnk_acc: 3 channels (covers 98.1% of time range)\n",
      "    📈 sensomative_bottom: 11 channels (covers 100.0% of time range)\n",
      "    📈 corsano_bioz: 3 channels (covers 98.1% of time range)\n",
      "  🏷️ Label column: 'Label' (activity annotations)\n",
      "\n",
      "👀 Sample data (first 5 rows):\n",
      "                               corsano_wrist_wrist_acc_x  \\\n",
      "2024-02-06 08:51:10.002000093                        NaN   \n",
      "2024-02-06 08:51:10.042000110                        NaN   \n",
      "2024-02-06 08:51:10.082000127                        NaN   \n",
      "2024-02-06 08:51:10.122000144                        NaN   \n",
      "2024-02-06 08:51:10.162000161                        NaN   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_y  \\\n",
      "2024-02-06 08:51:10.002000093                        NaN   \n",
      "2024-02-06 08:51:10.042000110                        NaN   \n",
      "2024-02-06 08:51:10.082000127                        NaN   \n",
      "2024-02-06 08:51:10.122000144                        NaN   \n",
      "2024-02-06 08:51:10.162000161                        NaN   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_z  \\\n",
      "2024-02-06 08:51:10.002000093                        NaN   \n",
      "2024-02-06 08:51:10.042000110                        NaN   \n",
      "2024-02-06 08:51:10.082000127                        NaN   \n",
      "2024-02-06 08:51:10.122000144                        NaN   \n",
      "2024-02-06 08:51:10.162000161                        NaN   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_x  cosinuss_ear_ear_acc_y  \\\n",
      "2024-02-06 08:51:10.002000093                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.042000110                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.082000127                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.122000144                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.162000161                     NaN                     NaN   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_z  mbient_acc_x_axis_g  \\\n",
      "2024-02-06 08:51:10.002000093                     NaN              -0.0350   \n",
      "2024-02-06 08:51:10.042000110                     NaN              -0.0350   \n",
      "2024-02-06 08:51:10.082000127                     NaN              -0.0360   \n",
      "2024-02-06 08:51:10.122000144                     NaN              -0.0355   \n",
      "2024-02-06 08:51:10.162000161                     NaN              -0.0360   \n",
      "\n",
      "                               mbient_acc_y_axis_g  mbient_acc_z_axis_g  \\\n",
      "2024-02-06 08:51:10.002000093               0.0875               1.0265   \n",
      "2024-02-06 08:51:10.042000110               0.0870               1.0245   \n",
      "2024-02-06 08:51:10.082000127               0.0860               1.0250   \n",
      "2024-02-06 08:51:10.122000144               0.0865               1.0260   \n",
      "2024-02-06 08:51:10.162000161               0.0875               1.0255   \n",
      "\n",
      "                               mbient_gyro_x_axis_dps  ...  \\\n",
      "2024-02-06 08:51:10.002000093                  0.0610  ...   \n",
      "2024-02-06 08:51:10.042000110                 -0.0305  ...   \n",
      "2024-02-06 08:51:10.082000127                 -0.1525  ...   \n",
      "2024-02-06 08:51:10.122000144                 -0.0305  ...   \n",
      "2024-02-06 08:51:10.162000161                 -0.0305  ...   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_6  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_7  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_8  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_9  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_10  \\\n",
      "2024-02-06 08:51:10.002000093                                 0.0   \n",
      "2024-02-06 08:51:10.042000110                                 0.0   \n",
      "2024-02-06 08:51:10.082000127                                 0.0   \n",
      "2024-02-06 08:51:10.122000144                                 0.0   \n",
      "2024-02-06 08:51:10.162000161                                 0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_11  \\\n",
      "2024-02-06 08:51:10.002000093                                 0.0   \n",
      "2024-02-06 08:51:10.042000110                                 0.0   \n",
      "2024-02-06 08:51:10.082000127                                 0.0   \n",
      "2024-02-06 08:51:10.122000144                                 0.0   \n",
      "2024-02-06 08:51:10.162000161                                 0.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_x  \\\n",
      "2024-02-06 08:51:10.002000093                      NaN   \n",
      "2024-02-06 08:51:10.042000110                      NaN   \n",
      "2024-02-06 08:51:10.082000127                      NaN   \n",
      "2024-02-06 08:51:10.122000144                      NaN   \n",
      "2024-02-06 08:51:10.162000161                      NaN   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_y  \\\n",
      "2024-02-06 08:51:10.002000093                      NaN   \n",
      "2024-02-06 08:51:10.042000110                      NaN   \n",
      "2024-02-06 08:51:10.082000127                      NaN   \n",
      "2024-02-06 08:51:10.122000144                      NaN   \n",
      "2024-02-06 08:51:10.162000161                      NaN   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_z  Label  \n",
      "2024-02-06 08:51:10.002000093                      NaN         \n",
      "2024-02-06 08:51:10.042000110                      NaN         \n",
      "2024-02-06 08:51:10.082000127                      NaN         \n",
      "2024-02-06 08:51:10.122000144                      NaN         \n",
      "2024-02-06 08:51:10.162000161                      NaN         \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "🏷️ Sample labeled data:\n",
      "                               corsano_wrist_wrist_acc_x  \\\n",
      "2024-02-06 10:52:27.005119747                       28.5   \n",
      "2024-02-06 10:52:27.045119764                       24.0   \n",
      "2024-02-06 10:52:27.085119781                       26.0   \n",
      "2024-02-06 10:52:27.125119799                       22.5   \n",
      "2024-02-06 10:52:27.165119816                       23.0   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_y  \\\n",
      "2024-02-06 10:52:27.005119747                     -509.0   \n",
      "2024-02-06 10:52:27.045119764                     -510.0   \n",
      "2024-02-06 10:52:27.085119781                     -510.0   \n",
      "2024-02-06 10:52:27.125119799                     -513.0   \n",
      "2024-02-06 10:52:27.165119816                     -509.0   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_z  \\\n",
      "2024-02-06 10:52:27.005119747                       22.5   \n",
      "2024-02-06 10:52:27.045119764                       18.0   \n",
      "2024-02-06 10:52:27.085119781                       15.0   \n",
      "2024-02-06 10:52:27.125119799                       19.0   \n",
      "2024-02-06 10:52:27.165119816                       22.0   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_x  cosinuss_ear_ear_acc_y  \\\n",
      "2024-02-06 10:52:27.005119747                  0.6660                 -0.7495   \n",
      "2024-02-06 10:52:27.045119764                  0.6670                 -0.7500   \n",
      "2024-02-06 10:52:27.085119781                  0.6665                 -0.7550   \n",
      "2024-02-06 10:52:27.125119799                  0.6635                 -0.7500   \n",
      "2024-02-06 10:52:27.165119816                  0.6675                 -0.7495   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_z  mbient_acc_x_axis_g  \\\n",
      "2024-02-06 10:52:27.005119747                -0.05850               0.2060   \n",
      "2024-02-06 10:52:27.045119764                -0.05950               0.1725   \n",
      "2024-02-06 10:52:27.085119781                -0.06225               0.1665   \n",
      "2024-02-06 10:52:27.125119799                -0.06500              -0.0210   \n",
      "2024-02-06 10:52:27.165119816                -0.06500               0.1995   \n",
      "\n",
      "                               mbient_acc_y_axis_g  mbient_acc_z_axis_g  \\\n",
      "2024-02-06 10:52:27.005119747               0.9430              -0.0885   \n",
      "2024-02-06 10:52:27.045119764               0.9350              -0.1095   \n",
      "2024-02-06 10:52:27.085119781               0.9555              -0.0855   \n",
      "2024-02-06 10:52:27.125119799               1.0560              -0.1350   \n",
      "2024-02-06 10:52:27.165119816               1.2450              -0.0165   \n",
      "\n",
      "                               mbient_gyro_x_axis_dps  ...  \\\n",
      "2024-02-06 10:52:27.005119747                  4.4815  ...   \n",
      "2024-02-06 10:52:27.045119764                  5.0305  ...   \n",
      "2024-02-06 10:52:27.085119781                  4.6340  ...   \n",
      "2024-02-06 10:52:27.125119799                  6.3110  ...   \n",
      "2024-02-06 10:52:27.165119816                  5.2135  ...   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_6  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_7  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_8  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_9  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_10  \\\n",
      "2024-02-06 10:52:27.005119747                                 0.0   \n",
      "2024-02-06 10:52:27.045119764                                 0.0   \n",
      "2024-02-06 10:52:27.085119781                                 0.0   \n",
      "2024-02-06 10:52:27.125119799                                 0.0   \n",
      "2024-02-06 10:52:27.165119816                                 0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_11  \\\n",
      "2024-02-06 10:52:27.005119747                                 0.0   \n",
      "2024-02-06 10:52:27.045119764                                 0.0   \n",
      "2024-02-06 10:52:27.085119781                                 0.0   \n",
      "2024-02-06 10:52:27.125119799                                 0.0   \n",
      "2024-02-06 10:52:27.165119816                                 0.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_x  \\\n",
      "2024-02-06 10:52:27.005119747                    -37.5   \n",
      "2024-02-06 10:52:27.045119764                   -114.0   \n",
      "2024-02-06 10:52:27.085119781                   -126.0   \n",
      "2024-02-06 10:52:27.125119799                   -177.5   \n",
      "2024-02-06 10:52:27.165119816                   -221.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_y  \\\n",
      "2024-02-06 10:52:27.005119747                   -569.0   \n",
      "2024-02-06 10:52:27.045119764                   -629.0   \n",
      "2024-02-06 10:52:27.085119781                   -599.0   \n",
      "2024-02-06 10:52:27.125119799                   -563.0   \n",
      "2024-02-06 10:52:27.165119816                   -493.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_z                  Label  \n",
      "2024-02-06 10:52:27.005119747                    101.0  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.045119764                    263.0  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.085119781                    311.0  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.125119799                    202.5  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.165119816                    211.0  sit_bed_to_wheelchair  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "📁 Final results structure:\n",
      "  /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/\n",
      "  ├── corsano_wrist/\n",
      "  │   └── OutSense-036_corsano_wrist_complete_data.pdf\n",
      "  ├── cosinuss_ear/\n",
      "  │   └── OutSense-036_cosinuss_ear_complete_data.pdf\n",
      "  ├── mbient_acc/\n",
      "  │   └── OutSense-036_mbient_acc_complete_data.pdf\n",
      "  ├── mbient_gyro/\n",
      "  │   └── OutSense-036_mbient_gyro_complete_data.pdf\n",
      "  ├── vivalnk_acc/\n",
      "  │   └── OutSense-036_vivalnk_acc_complete_data.pdf\n",
      "  ├── sensomative_bottom/\n",
      "  │   └── OutSense-036_sensomative_bottom_complete_data.pdf\n",
      "  ├── corsano_bioz/\n",
      "  │   └── OutSense-036_corsano_bioz_complete_data.pdf\n",
      "  ├── OutSense-036_combined_data.pkl\n",
      "  ├── OutSense-036_combined_data_metadata.json\n",
      "  └── OutSense-036_data_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Create combined synchronized data table with all sensors and labels\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Create results directory structure\n",
    "results_base_dir = '/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/results/pipeline/models'\n",
    "subject_results_dir = os.path.join(results_base_dir, SUBJECT_ID)\n",
    "os.makedirs(subject_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Results directory: {subject_results_dir}\")\n",
    "print(\"=== COMBINED DATA TABLE CREATION ===\")\n",
    "print(\"🔄 Creating unified table with synchronized timestamps, all sensors, and labels\")\n",
    "\n",
    "if not processed_sensors:\n",
    "    print(\"❌ No processed sensor data available\")\n",
    "else:\n",
    "    print(f\"📊 Processing {len(processed_sensors)} sensors...\")\n",
    "    \n",
    "    # Step 1: Determine the longest time range across all sensors\n",
    "    print(\"\\n📅 Step 1: Determining longest time range...\")\n",
    "    \n",
    "    all_start_times = []\n",
    "    all_end_times = []\n",
    "    \n",
    "    for sensor_name, sensor_data in processed_sensors.items():\n",
    "        all_start_times.append(sensor_data.index.min())\n",
    "        all_end_times.append(sensor_data.index.max())\n",
    "        print(f\"  📈 {sensor_name}: {sensor_data.index.min()} to {sensor_data.index.max()}\")\n",
    "    \n",
    "    # Use the union of all sensor time ranges (longest possible range)\n",
    "    common_start = min(all_start_times)  # Earliest start time\n",
    "    common_end = max(all_end_times)      # Latest end time\n",
    "    \n",
    "    print(f\"\\n⏱️ Longest time range (union of all sensors):\")\n",
    "    print(f\"  Start: {common_start}\")\n",
    "    print(f\"  End: {common_end}\")\n",
    "    print(f\"  Duration: {common_end - common_start}\")\n",
    "    print(f\"  ℹ️ Sensors not covering full range will have NaN values in gaps\")\n",
    "    \n",
    "    if common_start >= common_end:\n",
    "        print(\"❌ Invalid time range found!\")\n",
    "    else:\n",
    "        # Step 2: Create unified timestamp index\n",
    "        print(f\"\\n⚙️ Step 2: Creating unified timestamp index at {TARGET_FREQUENCY}Hz...\")\n",
    "        \n",
    "        # Create regular timestamp index at target frequency\n",
    "        total_seconds = (common_end - common_start).total_seconds()\n",
    "        num_samples = int(total_seconds * TARGET_FREQUENCY)\n",
    "        \n",
    "        # Create timestamp index\n",
    "        timestamp_index = pd.date_range(\n",
    "            start=common_start,\n",
    "            end=common_end,\n",
    "            periods=num_samples\n",
    "        )\n",
    "        \n",
    "        print(f\"  📊 Created {len(timestamp_index)} timestamps\")\n",
    "        print(f\"  🕐 Frequency: {TARGET_FREQUENCY}Hz\")\n",
    "        print(f\"  📏 Interval: {timestamp_index[1] - timestamp_index[0]}\")\n",
    "        \n",
    "        # Step 3: Resample and align all sensor data\n",
    "        print(f\"\\n🔄 Step 3: Resampling and aligning sensor data...\")\n",
    "        \n",
    "        combined_data = pd.DataFrame(index=timestamp_index)\n",
    "        sensor_stats = {}\n",
    "        \n",
    "        for sensor_name, sensor_data in processed_sensors.items():\n",
    "            print(f\"  📈 Processing {sensor_name}...\")\n",
    "            \n",
    "            # Get numeric columns\n",
    "            numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\n",
    "            print(f\"    📊 {len(numeric_cols)} numeric columns\")\n",
    "            \n",
    "            # Check coverage within the longest time range\n",
    "            sensor_start = sensor_data.index.min()\n",
    "            sensor_end = sensor_data.index.max()\n",
    "            coverage_start = max(sensor_start, common_start)\n",
    "            coverage_end = min(sensor_end, common_end)\n",
    "            \n",
    "            print(f\"    📅 Sensor range: {sensor_start} to {sensor_end}\")\n",
    "            print(f\"    🎯 Coverage in full range: {coverage_start} to {coverage_end}\")\n",
    "            \n",
    "            # Process the sensor data (no filtering to common range)\n",
    "            sensor_data_clean = sensor_data.copy()\n",
    "            \n",
    "            # First ensure monotonic index (remove any duplicates)\n",
    "            sensor_data_clean = sensor_data_clean[~sensor_data_clean.index.duplicated(keep='first')]\n",
    "            sensor_data_clean = sensor_data_clean.sort_index()\n",
    "            \n",
    "            # Resample using linear interpolation\n",
    "            resampled_data = sensor_data_clean[numeric_cols].resample(f'{1000//TARGET_FREQUENCY}ms').mean()\n",
    "            \n",
    "            # Interpolate to fill gaps within sensor's own time range\n",
    "            resampled_data = resampled_data.interpolate(method='linear', limit=TARGET_FREQUENCY*2)\n",
    "            \n",
    "            # Align to our full timestamp index (this will create NaN for times outside sensor range)\n",
    "            aligned_data = resampled_data.reindex(timestamp_index, method='nearest', tolerance=pd.Timedelta(f'{2000//TARGET_FREQUENCY}ms'))\n",
    "            \n",
    "            # Add sensor prefix to column names to avoid conflicts\n",
    "            sensor_prefix = sensor_name.replace(' ', '_').replace('/', '_')\n",
    "            aligned_data.columns = [f\"{sensor_prefix}_{col}\" for col in aligned_data.columns]\n",
    "            \n",
    "            # Add to combined dataframe\n",
    "            for col in aligned_data.columns:\n",
    "                combined_data[col] = aligned_data[col]\n",
    "            \n",
    "            # Statistics\n",
    "            valid_samples = aligned_data.notna().sum().sum()\n",
    "            total_samples = len(aligned_data) * len(aligned_data.columns)\n",
    "            coverage = valid_samples / total_samples * 100 if total_samples > 0 else 0\n",
    "            \n",
    "            # Calculate time coverage\n",
    "            time_coverage_start = max(sensor_start, common_start)\n",
    "            time_coverage_end = min(sensor_end, common_end)\n",
    "            time_coverage_duration = time_coverage_end - time_coverage_start\n",
    "            full_duration = common_end - common_start\n",
    "            time_coverage_percent = (time_coverage_duration.total_seconds() / full_duration.total_seconds()) * 100\n",
    "            \n",
    "            sensor_stats[sensor_name] = {\n",
    "                'original_samples': len(sensor_data_clean),\n",
    "                'resampled_samples': len(aligned_data),\n",
    "                'channels': len(aligned_data.columns),\n",
    "                'coverage_percent': coverage,\n",
    "                'missing_samples': total_samples - valid_samples,\n",
    "                'time_coverage_percent': time_coverage_percent,\n",
    "                'sensor_start': sensor_start.isoformat(),\n",
    "                'sensor_end': sensor_end.isoformat(),\n",
    "                'gaps_filled_with_nan': total_samples - valid_samples\n",
    "            }\n",
    "            \n",
    "            print(f\"    ✅ {len(aligned_data.columns)} channels added\")\n",
    "            print(f\"    📊 Data coverage: {coverage:.1f}% ({valid_samples}/{total_samples} samples)\")\n",
    "            print(f\"    ⏰ Time coverage: {time_coverage_percent:.1f}% of full range\")\n",
    "            if total_samples - valid_samples > 0:\n",
    "                print(f\"    🕳️ {total_samples - valid_samples} samples filled with NaN (outside sensor range)\")\n",
    "        \n",
    "        # Step 4: Add corrected label information\n",
    "        print(f\"\\n🏷️ Step 4: Adding corrected label information...\")\n",
    "        \n",
    "        # Initialize label column\n",
    "        combined_data['Label'] = ''\n",
    "        \n",
    "        # Use the globally corrected labels if available, otherwise use original\n",
    "        labels_to_use = corrected_labels_global if len(corrected_labels_global) > 0 else valid_labels\n",
    "        \n",
    "        if len(labels_to_use) > 0:\n",
    "            print(f\"  📋 Processing {len(labels_to_use)} corrected labels...\")\n",
    "            \n",
    "            # Show correction summary\n",
    "            if len(corrected_labels_global) > 0 and current_correction_log:\n",
    "                print(f\"  🔧 Label corrections applied:\")\n",
    "                if current_correction_log.get('manual_offset_applied', 0) != 0:\n",
    "                    print(f\"    Manual offset: {current_correction_log['manual_offset_applied']}s\")\n",
    "                if current_correction_log.get('linear_drift_applied', False):\n",
    "                    print(f\"    Linear drift correction: enabled\")\n",
    "            \n",
    "            # Filter labels to longest time range\n",
    "            common_labels = labels_to_use[\n",
    "                (labels_to_use['Real_Start_Time'] <= common_end) & \n",
    "                (labels_to_use['Real_End_Time'] >= common_start)\n",
    "            ].copy()\n",
    "            \n",
    "            print(f\"  🎯 {len(common_labels)} corrected labels in longest time range\")\n",
    "            \n",
    "            # For each label, mark the corresponding timestamps\n",
    "            label_count = 0\n",
    "            overlap_count = 0\n",
    "            \n",
    "            for _, label_row in common_labels.iterrows():\n",
    "                start_time = max(label_row['Real_Start_Time'], common_start)\n",
    "                end_time = min(label_row['Real_End_Time'], common_end)\n",
    "                label_name = label_row['Label']\n",
    "                \n",
    "                if start_time < end_time:\n",
    "                    # Find timestamps within this label period\n",
    "                    mask = (combined_data.index >= start_time) & (combined_data.index <= end_time)\n",
    "                    matching_timestamps = combined_data.index[mask]\n",
    "                    \n",
    "                    if len(matching_timestamps) > 0:\n",
    "                        # Check for overlapping labels\n",
    "                        existing_labels = combined_data.loc[mask, 'Label']\n",
    "                        overlaps = existing_labels[existing_labels != '']\n",
    "                        \n",
    "                        if len(overlaps) > 0:\n",
    "                            overlap_count += len(overlaps)\n",
    "                            # For overlapping labels, create combined label\n",
    "                            for idx in matching_timestamps:\n",
    "                                existing = combined_data.loc[idx, 'Label']\n",
    "                                if existing == '':\n",
    "                                    combined_data.loc[idx, 'Label'] = label_name\n",
    "                                elif label_name not in existing:\n",
    "                                    combined_data.loc[idx, 'Label'] = f\"{existing}+{label_name}\"\n",
    "                        else:\n",
    "                            # No overlap, assign label directly\n",
    "                            combined_data.loc[mask, 'Label'] = label_name\n",
    "                        \n",
    "                        label_count += len(matching_timestamps)\n",
    "            \n",
    "            print(f\"  ✅ {label_count} timestamps labeled\")\n",
    "            print(f\"  ⚠️ {overlap_count} overlapping label instances handled\")\n",
    "            \n",
    "            # Label statistics\n",
    "            labeled_count = (combined_data['Label'] != '').sum()\n",
    "            total_count = len(combined_data)\n",
    "            label_coverage = labeled_count / total_count * 100\n",
    "            \n",
    "            print(f\"  📊 Label coverage: {label_coverage:.1f}% ({labeled_count}/{total_count} timestamps)\")\n",
    "            \n",
    "            # Show label distribution\n",
    "            label_dist = combined_data['Label'].value_counts()\n",
    "            non_empty_labels = label_dist[label_dist.index != '']\n",
    "            \n",
    "            print(f\"  📋 Label distribution (top 10):\")\n",
    "            for label, count in non_empty_labels.head(10).items():\n",
    "                duration_seconds = count / TARGET_FREQUENCY\n",
    "                print(f\"    🏷️ {label}: {count} samples ({duration_seconds:.1f}s)\")\n",
    "            \n",
    "            if len(non_empty_labels) > 10:\n",
    "                print(f\"    ... and {len(non_empty_labels) - 10} more labels\")\n",
    "        \n",
    "        else:\n",
    "            print(\"  ⚠️ No labels available - Label column will remain empty\")\n",
    "        \n",
    "        # Step 5: Data quality summary\n",
    "        print(f\"\\n📊 Step 5: Data quality summary...\")\n",
    "        \n",
    "        total_sensor_cols = len([col for col in combined_data.columns if col != 'Label'])\n",
    "        total_samples = len(combined_data)\n",
    "        \n",
    "        print(f\"  📋 Final combined table:\")\n",
    "        print(f\"    Timestamps: {total_samples}\")\n",
    "        print(f\"    Sensor columns: {total_sensor_cols}\")\n",
    "        print(f\"    Label column: 1\")\n",
    "        print(f\"    Total columns: {len(combined_data.columns)}\")\n",
    "        print(f\"    Time range: {combined_data.index.min()} to {combined_data.index.max()}\")\n",
    "        print(f\"    Duration: {combined_data.index.max() - combined_data.index.min()}\")\n",
    "        print(f\"    Frequency: {TARGET_FREQUENCY}Hz\")\n",
    "        \n",
    "        # Missing data analysis\n",
    "        missing_data = combined_data.select_dtypes(include=[np.number]).isnull().sum()\n",
    "        if missing_data.sum() > 0:\n",
    "            print(f\"  🕳️ Missing data summary (NaN values where sensors don't cover full range):\")\n",
    "            for col, missing_count in missing_data[missing_data > 0].items():\n",
    "                missing_pct = missing_count / total_samples * 100\n",
    "                print(f\"    {col}: {missing_count} samples ({missing_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  ✅ No missing data in sensor columns\")\n",
    "        \n",
    "        # Step 6: Save combined data\n",
    "        print(f\"\\n💾 Step 6: Saving combined data...\")\n",
    "        \n",
    "        # Define save path\n",
    "        pkl_filename = f\"{SUBJECT_ID}_combined_data.pkl\"\n",
    "        pkl_path = os.path.join(subject_results_dir, pkl_filename)\n",
    "        \n",
    "        # Save as pickle file\n",
    "        try:\n",
    "            with open(pkl_path, 'wb') as f:\n",
    "                pickle.dump(combined_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            print(f\"✅ Combined data saved: {pkl_path}\")\n",
    "            \n",
    "            # File size info\n",
    "            file_size = os.path.getsize(pkl_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            print(f\"  📁 File size: {file_size_mb:.1f} MB\")\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'subject_id': SUBJECT_ID,\n",
    "                'creation_time': datetime.now().isoformat(),\n",
    "                'time_range': {\n",
    "                    'start': combined_data.index.min().isoformat(),\n",
    "                    'end': combined_data.index.max().isoformat(),\n",
    "                    'duration_seconds': (combined_data.index.max() - combined_data.index.min()).total_seconds(),\n",
    "                    'range_type': 'longest_union'  # Indicates this uses the longest range\n",
    "                },\n",
    "                'sampling': {\n",
    "                    'target_frequency_hz': TARGET_FREQUENCY,\n",
    "                    'total_samples': total_samples,\n",
    "                    'actual_frequency_hz': total_samples / (combined_data.index.max() - combined_data.index.min()).total_seconds()\n",
    "                },\n",
    "                'sensors': sensor_stats,\n",
    "                'columns': {\n",
    "                    'total': len(combined_data.columns),\n",
    "                    'sensor_channels': total_sensor_cols,\n",
    "                    'label_column': 1,\n",
    "                    'column_names': list(combined_data.columns)\n",
    "                },\n",
    "                'labels': {\n",
    "                    'total_labels_available': len(valid_labels),\n",
    "                    'labels_in_timerange': len(common_labels) if len(valid_labels) > 0 else 0,\n",
    "                    'labeled_timestamps': labeled_count if len(valid_labels) > 0 else 0,\n",
    "                    'label_coverage_percent': label_coverage if len(valid_labels) > 0 else 0,\n",
    "                    'unique_labels': list(non_empty_labels.index) if len(valid_labels) > 0 else []\n",
    "                },\n",
    "                'data_quality': {\n",
    "                    'missing_values': missing_data.to_dict(),\n",
    "                    'coverage_by_sensor': {name: stats['coverage_percent'] for name, stats in sensor_stats.items()},\n",
    "                    'time_coverage_by_sensor': {name: stats['time_coverage_percent'] for name, stats in sensor_stats.items()}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save metadata as JSON\n",
    "            metadata_path = os.path.join(subject_results_dir, f\"{SUBJECT_ID}_combined_data_metadata.json\")\n",
    "            import json\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"✅ Metadata saved: {metadata_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving combined data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # Step 7: Verification and summary\n",
    "        print(f\"\\n✅ COMBINED DATA TABLE COMPLETE!\")\n",
    "        print(f\"📂 Saved to: {pkl_path}\")\n",
    "        print(f\"📊 Shape: {combined_data.shape}\")\n",
    "        print(f\"⏱️ Time span: {combined_data.index.max() - combined_data.index.min()}\")\n",
    "        print(f\"🎯 Frequency: {TARGET_FREQUENCY}Hz\")\n",
    "        print(f\"📈 Sensors: {len(processed_sensors)}\")\n",
    "        print(f\"🏷️ Labels: {'Yes' if len(valid_labels) > 0 else 'No'}\")\n",
    "        print(f\"🕳️ NaN handling: Gaps filled for sensors not covering full time range\")\n",
    "        \n",
    "        print(f\"\\n💡 Usage for AI preprocessing:\")\n",
    "        print(f\"  import pickle\")\n",
    "        print(f\"  with open('{pkl_path}', 'rb') as f:\")\n",
    "        print(f\"      data = pickle.load(f)\")\n",
    "        print(f\"  # data is a pandas DataFrame with:\")\n",
    "        print(f\"  # - Index: synchronized timestamps (longest range)\")\n",
    "        print(f\"  # - Columns: all sensor channels + 'Label'\")\n",
    "        print(f\"  # - NaN values where sensors don't cover full range\")\n",
    "        print(f\"  # - Ready for feature extraction and ML\")\n",
    "        \n",
    "        print(f\"\\n📋 Column structure:\")\n",
    "        sensor_cols = [col for col in combined_data.columns if col != 'Label']\n",
    "        print(f\"  Sensor columns ({len(sensor_cols)}):\")\n",
    "        for sensor_name in processed_sensors.keys():\n",
    "            sensor_prefix = sensor_name.replace(' ', '_').replace('/', '_')\n",
    "            matching_cols = [col for col in sensor_cols if col.startswith(sensor_prefix)]\n",
    "            time_cov = sensor_stats[sensor_name]['time_coverage_percent']\n",
    "            print(f\"    📈 {sensor_name}: {len(matching_cols)} channels (covers {time_cov:.1f}% of time range)\")\n",
    "        print(f\"  🏷️ Label column: 'Label' (activity annotations)\")\n",
    "        \n",
    "        # Show sample of the data\n",
    "        print(f\"\\n👀 Sample data (first 5 rows):\")\n",
    "        print(combined_data.head())\n",
    "        \n",
    "        if len(valid_labels) > 0:\n",
    "            # Show some labeled samples\n",
    "            labeled_samples = combined_data[combined_data['Label'] != '']\n",
    "            if len(labeled_samples) > 0:\n",
    "                print(f\"\\n🏷️ Sample labeled data:\")\n",
    "                print(labeled_samples.head())\n",
    "\n",
    "print(f\"\\n📁 Final results structure:\")\n",
    "print(f\"  {subject_results_dir}/\")\n",
    "for sensor_name in processed_sensors.keys():\n",
    "    clean_name = sensor_name.replace('/', '_').replace(' ', '_')\n",
    "    print(f\"  ├── {clean_name}/\")\n",
    "    print(f\"  │   └── {SUBJECT_ID}_{clean_name}_complete_data.pdf\")\n",
    "print(f\"  ├── {SUBJECT_ID}_combined_data.pkl\")\n",
    "print(f\"  ├── {SUBJECT_ID}_combined_data_metadata.json\")\n",
    "print(f\"  └── {SUBJECT_ID}_data_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
