{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "061a4b03",
   "metadata": {},
   "source": [
    "# Simple Sensor Data Visualization v2\n",
    "\n",
    "**Purpose**: Load sensor data around sync events with individual time axes for manual sync event identification.\n",
    "\n",
    "**Features**:\n",
    "- Load 4 hours around sync start time (configurable)\n",
    "- Each sensor has its own independent time axis\n",
    "- Time shifts controlled by Sync_Parameters.yaml\n",
    "- Preprocessing done before plotting\n",
    "- Simple and focused approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e3011",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4c6fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "  Subject: OutSense-036\n",
      "  Time window: ¬±2 hours around sync start\n",
      "  Target frequency: 25 Hz\n",
      "  Project root: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src\n",
      "  Labels file: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/Final_Labels_corrected.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "SUBJECT_ID = \"OutSense-036\"  # Change this to your subject\n",
    "HOURS_AROUND_SYNC = 2  # Hours to load around sync start time (2 hours before, 2 hours after)\n",
    "TARGET_FREQUENCY = 25  # Hz for resampling\n",
    "\n",
    "# Paths\n",
    "script_dir = '/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/debug_labels_v2.ipynb'\n",
    "project_root = '/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src'\n",
    "sync_params_path = os.path.join(project_root, 'Sync_Parameters_andre.yaml')\n",
    "sync_events_path = os.path.join(project_root, 'Sync_Events_Times.csv')\n",
    "config_path = os.path.join(project_root, 'config.yaml')\n",
    "labels_path = os.path.join(project_root, 'Final_Labels_corrected.csv')\n",
    "\n",
    "print(f\"üìã Configuration:\")\n",
    "print(f\"  Subject: {SUBJECT_ID}\")\n",
    "print(f\"  Time window: ¬±{HOURS_AROUND_SYNC} hours around sync start\")\n",
    "print(f\"  Target frequency: {TARGET_FREQUENCY} Hz\")\n",
    "print(f\"  Project root: {project_root}\")\n",
    "print(f\"  Labels file: {labels_path}\")\n",
    "\n",
    "# Generate a consistent set of colors for labels\n",
    "def generate_label_colors(labels_list):\n",
    "    \"\"\"Generate consistent random colors for each unique label\"\"\"\n",
    "    unique_labels = list(set(labels_list))\n",
    "    random.seed(42)  # For consistent colors across runs\n",
    "    colors = []\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # Use HSV color space for better color distribution\n",
    "        hue = (i * 137.5) % 360  # Golden angle for good distribution\n",
    "        saturation = 0.7 + (i % 3) * 0.1  # Vary saturation\n",
    "        value = 0.8 + (i % 2) * 0.15  # Vary brightness\n",
    "        \n",
    "        # Convert HSV to RGB\n",
    "        rgb = mcolors.hsv_to_rgb([hue/360, saturation, value])\n",
    "        colors.append(rgb)\n",
    "    \n",
    "    return dict(zip(unique_labels, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7535c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Label Drift Correction Configuration:\n",
      "  Manual offset: 0.0s\n",
      "  Linear drift: disabled\n",
      "  Reference events: disabled\n"
     ]
    }
   ],
   "source": [
    "# ========== LABEL DRIFT CORRECTION CONFIGURATION ==========\n",
    "# Label time drift correction parameters (separate from sensor drift)\n",
    "LABEL_DRIFT_PARAMS = {\n",
    "    \"enabled\": True,\n",
    "    \"manual_offset_seconds\": 0.0,  # Manual offset in seconds (+ = shift labels forward, - = shift backward)\n",
    "    \"linear_drift_correction\": {\n",
    "        \"enabled\": False,\n",
    "        \"start_time\": None,  # Will be set to sync_start_time by default\n",
    "        \"end_time\": None,    # Will be set to sync_end_time by default\n",
    "        \"drift_seconds\": 0.0  # Total drift over the time period\n",
    "    },\n",
    "    \"reference_events\": {\n",
    "        # Define reference points for label alignment\n",
    "        \"enabled\": False,\n",
    "        \"events\": [\n",
    "            # {\"label_time\": \"2024-01-01 10:00:00\", \"true_time\": \"2024-01-01 10:00:05\", \"description\": \"Manual sync point\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"üìã Label Drift Correction Configuration:\")\n",
    "print(f\"  Manual offset: {LABEL_DRIFT_PARAMS['manual_offset_seconds']}s\")\n",
    "print(f\"  Linear drift: {'enabled' if LABEL_DRIFT_PARAMS['linear_drift_correction']['enabled'] else 'disabled'}\")\n",
    "print(f\"  Reference events: {'enabled' if LABEL_DRIFT_PARAMS['reference_events']['enabled'] else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ec783",
   "metadata": {},
   "source": [
    "## 2. Load Configuration and Sync Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1a7aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded Final_Labels.csv with 7214 entries\n",
      "üìä Found 258 labels for subject OutSense-036\n",
      "üìÖ 258 labels have valid timestamps\n",
      "üé® Generated colors for 29 unique labels\n",
      "\n",
      "üìã Label distribution:\n",
      "  conversation: 54 instances\n",
      "  self_propulsion: 50 instances\n",
      "  dark: 48 instances\n",
      "  cycling: 22 instances\n",
      "  sitting_wheelchair: 13 instances\n",
      "  drinking: 8 instances\n",
      "  eating: 6 instances\n",
      "  bending: 6 instances\n",
      "  reading_newspaper: 5 instances\n",
      "  pressure_relief: 5 instances\n",
      "  ... and 19 more labels\n",
      "‚úÖ Loaded configurations:\n",
      "  Main config: 64 sections\n",
      "  Sync parameters: 16 subjects\n",
      "  Sync events: 16 entries\n",
      "  Labels: 258 for OutSense-036\n",
      "\n",
      "üéØ Sync times for OutSense-036:\n",
      "  Sync Start: 2024-02-06 09:51:10\n",
      "  Sync End: 2024-02-08 10:23:35\n",
      "  Duration: 2 days 00:32:25\n",
      "\n",
      "üìä Data window (includes both sync events + 1.0h buffer each side):\n",
      "  Window Start: 2024-02-06 08:51:10 (sync start - 1.0h)\n",
      "  Window End: 2024-02-08 11:23:35 (sync end + 1.0h)\n",
      "  Total Window Duration: 2 days 02:32:25\n",
      "  Sync Event Duration: 2 days 00:32:25\n",
      "  Buffer Coverage: 0 days 01:00:00 before sync start, 0 days 01:00:00 after sync end\n",
      "\n",
      "üè∑Ô∏è Labels in data window: 258\n",
      "  conversation: 54 instances\n",
      "  self_propulsion: 50 instances\n",
      "  dark: 48 instances\n",
      "  cycling: 22 instances\n",
      "  sitting_wheelchair: 13 instances\n"
     ]
    }
   ],
   "source": [
    "# Load main configuration\n",
    "with open('/scai_data3/scratch/stirnimann_r/config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Load sync parameters\n",
    "with open('/scai_data3/scratch/stirnimann_r/Sync_Parameters_andre.yaml', 'r') as f:\n",
    "    sync_params = yaml.safe_load(f)\n",
    "\n",
    "# Load sync events\n",
    "sync_events_df = pd.read_csv('/scai_data3/scratch/stirnimann_r/Sync_Events_Times.csv')\n",
    "\n",
    "# Load Final_Labels.csv\n",
    "try:\n",
    "    labels_df = pd.read_csv('/scai_data3/scratch/stirnimann_r/Final_Labels_corrected.csv')\n",
    "    print(f\"‚úÖ Loaded Final_Labels.csv with {len(labels_df)} entries\")\n",
    "    \n",
    "    # Filter labels for the current subject\n",
    "    subject_labels = labels_df[labels_df['Video_File'].str.contains(SUBJECT_ID, na=False)]\n",
    "    print(f\"üìä Found {len(subject_labels)} labels for subject {SUBJECT_ID}\")\n",
    "    \n",
    "    if len(subject_labels) > 0:\n",
    "        # Parse the Real_Start_Time and Real_End_Time columns\n",
    "        subject_labels = subject_labels.copy()\n",
    "        subject_labels['Real_Start_Time'] = pd.to_datetime(subject_labels['Real_Start_Time'], errors='coerce')\n",
    "        subject_labels['Real_End_Time'] = pd.to_datetime(subject_labels['Real_End_Time'], errors='coerce')\n",
    "        \n",
    "        # Remove any rows with invalid timestamps\n",
    "        valid_labels = subject_labels.dropna(subset=['Real_Start_Time', 'Real_End_Time'])\n",
    "        print(f\"üìÖ {len(valid_labels)} labels have valid timestamps\")\n",
    "        \n",
    "        # Generate colors for labels\n",
    "        label_colors = generate_label_colors(valid_labels['Label'].tolist())\n",
    "        print(f\"üé® Generated colors for {len(label_colors)} unique labels\")\n",
    "        \n",
    "        # Show label summary\n",
    "        label_summary = valid_labels['Label'].value_counts()\n",
    "        print(f\"\\nüìã Label distribution:\")\n",
    "        for label, count in label_summary.head(10).items():\n",
    "            print(f\"  {label}: {count} instances\")\n",
    "        if len(label_summary) > 10:\n",
    "            print(f\"  ... and {len(label_summary) - 10} more labels\")\n",
    "    else:\n",
    "        valid_labels = pd.DataFrame()\n",
    "        label_colors = {}\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Final_Labels.csv not found - plots will show without labels\")\n",
    "    valid_labels = pd.DataFrame()\n",
    "    label_colors = {}\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading Final_Labels.csv: {e}\")\n",
    "    valid_labels = pd.DataFrame()\n",
    "    label_colors = {}\n",
    "\n",
    "print(f\"‚úÖ Loaded configurations:\")\n",
    "print(f\"  Main config: {len(cfg)} sections\")\n",
    "print(f\"  Sync parameters: {len(sync_params)} subjects\")\n",
    "print(f\"  Sync events: {len(sync_events_df)} entries\")\n",
    "print(f\"  Labels: {len(valid_labels)} for {SUBJECT_ID}\")\n",
    "\n",
    "\n",
    "if SUBJECT_ID == \"OutSense-425\":\n",
    "    SUBJECT_ID_FIXED = \"OutSense-425_48h\"\n",
    "else:\n",
    "    SUBJECT_ID_FIXED = SUBJECT_ID\n",
    "\n",
    "# Get sync start time for the subject\n",
    "subject_sync = sync_events_df[sync_events_df['Subject'] == SUBJECT_ID_FIXED]\n",
    "if subject_sync.empty:\n",
    "    raise ValueError(f\"No sync events found for subject {SUBJECT_ID_FIXED}\")\n",
    "\n",
    "sync_start_str = subject_sync.iloc[0]['Sync Start']\n",
    "sync_end_str = subject_sync.iloc[0]['Sync End']\n",
    "\n",
    "# Parse sync times\n",
    "sync_start_time = pd.to_datetime(sync_start_str, format='%d.%m.%Y.%H.%M.%S')\n",
    "sync_end_time = pd.to_datetime(sync_end_str, format='%d.%m.%Y.%H.%M.%S')\n",
    "\n",
    "print(f\"\\nüéØ Sync times for {SUBJECT_ID}:\")\n",
    "print(f\"  Sync Start: {sync_start_time}\")\n",
    "print(f\"  Sync End: {sync_end_time}\")\n",
    "print(f\"  Duration: {sync_end_time - sync_start_time}\")\n",
    "\n",
    "# Calculate data window to include both sync start and sync end events\n",
    "# Add buffer time around both events\n",
    "buffer_time = pd.Timedelta(hours=HOURS_AROUND_SYNC/2)\n",
    "data_window_start = sync_start_time - buffer_time\n",
    "data_window_end = sync_end_time + buffer_time\n",
    "\n",
    "# Ensure we capture the full sync event duration plus buffer\n",
    "sync_duration = sync_end_time - sync_start_time\n",
    "total_window_duration = data_window_end - data_window_start\n",
    "\n",
    "print(f\"\\nüìä Data window (includes both sync events + {HOURS_AROUND_SYNC/2}h buffer each side):\")\n",
    "print(f\"  Window Start: {data_window_start} (sync start - {HOURS_AROUND_SYNC/2}h)\")\n",
    "print(f\"  Window End: {data_window_end} (sync end + {HOURS_AROUND_SYNC/2}h)\")\n",
    "print(f\"  Total Window Duration: {total_window_duration}\")\n",
    "print(f\"  Sync Event Duration: {sync_duration}\")\n",
    "print(f\"  Buffer Coverage: {buffer_time} before sync start, {buffer_time} after sync end\")\n",
    "\n",
    "# Show labels in the data window\n",
    "if len(valid_labels) > 0:\n",
    "    window_labels = valid_labels[\n",
    "        (valid_labels['Real_Start_Time'] <= data_window_end) & \n",
    "        (valid_labels['Real_End_Time'] >= data_window_start)\n",
    "    ]\n",
    "    print(f\"\\nüè∑Ô∏è Labels in data window: {len(window_labels)}\")\n",
    "    if len(window_labels) > 0:\n",
    "        window_label_summary = window_labels['Label'].value_counts()\n",
    "        for label, count in window_label_summary.head(5).items():\n",
    "            print(f\"  {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c45dff",
   "metadata": {},
   "source": [
    "## 3. Load and Import Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8adfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imported functions from raw_data_processor\n",
      "\n",
      "üìÇ Data paths:\n",
      "  Raw data dir: /scai_data2/scai_datasets/interim/scai-outsense/\n",
      "  Subject dir: /scai_data2/scai_datasets/interim/scai-outsense/OutSense-036\n",
      "  Available sensors: ['corsano_wrist_acc', 'cosinuss_ear_acc_x_acc_y_acc_z', 'mbient_imu_wc_accelerometer', 'mbient_imu_wc_gyroscope', 'vivalnk_vv330_acceleration', 'sensomative_bottom_logger', 'sensomative_back_logger', 'corsano_bioz_acc']\n"
     ]
    }
   ],
   "source": [
    "# Import data loading functions from the original notebook/scripts\n",
    "import sys\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import necessary functions (you may need to adjust these based on your actual module structure)\n",
    "try:\n",
    "    from raw_data_processor import (\n",
    "        select_data_loader,\n",
    "        modify_modality_names,\n",
    "        process_modality_duplicates,\n",
    "        handle_missing_data_interpolation,\n",
    "        correct_timestamp_drift\n",
    "    )\n",
    "    print(\"‚úÖ Imported functions from raw_data_processor\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import from raw_data_processor: {e}\")\n",
    "    print(\"You may need to adjust the import paths or copy the required functions\")\n",
    "    \n",
    "    # Define minimal data loader selection function\n",
    "    def select_data_loader(sensor_name):\n",
    "        \"\"\"Simple data loader selector - you may need to implement based on your data structure\"\"\"\n",
    "        def simple_csv_loader(subject_dir, sensor_name, sensor_settings):\n",
    "            # This is a placeholder - implement based on your actual data structure\n",
    "            csv_path = os.path.join(subject_dir, f\"{sensor_name}.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                return pd.read_csv(csv_path)\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "        return simple_csv_loader\n",
    "    \n",
    "    def modify_modality_names(data, sensor_name):\n",
    "        \"\"\"Simple modality name modifier\"\"\"\n",
    "        return sensor_name, data\n",
    "    \n",
    "    def process_modality_duplicates(data, sample_rate):\n",
    "        \"\"\"Simple duplicate processor\"\"\"\n",
    "        return data.drop_duplicates()\n",
    "    \n",
    "    def handle_missing_data_interpolation(data, max_interp_gap_s=2, target_freq=50):\n",
    "        \"\"\"Simple interpolation\"\"\"\n",
    "        return data.interpolate(method='linear', limit=int(max_interp_gap_s * target_freq))\n",
    "    \n",
    "    def correct_timestamp_drift(timestamp, t0, t1, drift_secs):\n",
    "        \"\"\"Simple drift correction\"\"\"\n",
    "        if t0 <= timestamp <= t1:\n",
    "            progress = (timestamp - t0) / (t1 - t0)\n",
    "            return timestamp + (drift_secs * progress)\n",
    "        return timestamp\n",
    "    \n",
    "    print(\"üìù Using simplified placeholder functions\")\n",
    "\n",
    "# Get raw data configuration\n",
    "raw_data_parsing_config = cfg.get('raw_data_parsing_config', {})\n",
    "raw_data_base_dir = os.path.join(project_root, cfg.get('raw_data_input_dir', 'data'))\n",
    "subject_dir = os.path.join(raw_data_base_dir, SUBJECT_ID)\n",
    "\n",
    "print(f\"\\nüìÇ Data paths:\")\n",
    "print(f\"  Raw data dir: {raw_data_base_dir}\")\n",
    "print(f\"  Subject dir: {subject_dir}\")\n",
    "print(f\"  Available sensors: {list(raw_data_parsing_config.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae41fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Label drift correction functions defined\n"
     ]
    }
   ],
   "source": [
    "def apply_label_time_corrections(labels_df, drift_params, sync_start_time, sync_end_time):\n",
    "    \"\"\"\n",
    "    Apply time drift corrections to label timestamps\n",
    "    \n",
    "    Parameters:\n",
    "    - labels_df: DataFrame with Real_Start_Time and Real_End_Time columns\n",
    "    - drift_params: Label drift correction parameters\n",
    "    - sync_start_time: Reference sync start time\n",
    "    - sync_end_time: Reference sync end time\n",
    "    \n",
    "    Returns:\n",
    "    - corrected_labels_df: DataFrame with corrected timestamps\n",
    "    - correction_log: Dictionary with applied corrections\n",
    "    \"\"\"\n",
    "    if labels_df.empty:\n",
    "        return labels_df, {}\n",
    "    \n",
    "    corrected_labels = labels_df.copy()\n",
    "    correction_log = {\n",
    "        \"manual_offset_applied\": 0.0,\n",
    "        \"linear_drift_applied\": False,\n",
    "        \"reference_events_applied\": 0,\n",
    "        \"total_labels_corrected\": len(labels_df)\n",
    "    }\n",
    "    \n",
    "    print(f\"üîß Applying label time corrections to {len(labels_df)} labels...\")\n",
    "    \n",
    "    # 1. Apply manual offset\n",
    "    if drift_params.get(\"enabled\", False):\n",
    "        manual_offset = drift_params.get(\"manual_offset_seconds\", 0.0)\n",
    "        if manual_offset != 0.0:\n",
    "            offset_timedelta = pd.Timedelta(seconds=manual_offset)\n",
    "            corrected_labels['Real_Start_Time'] += offset_timedelta\n",
    "            corrected_labels['Real_End_Time'] += offset_timedelta\n",
    "            correction_log[\"manual_offset_applied\"] = manual_offset\n",
    "            print(f\"  ‚è±Ô∏è Applied manual offset: {manual_offset}s\")\n",
    "    \n",
    "    # 2. Apply linear drift correction\n",
    "    linear_drift = drift_params.get(\"linear_drift_correction\", {})\n",
    "    if linear_drift.get(\"enabled\", False):\n",
    "        start_time = pd.to_datetime(linear_drift.get(\"start_time\")) if linear_drift.get(\"start_time\") else sync_start_time\n",
    "        end_time = pd.to_datetime(linear_drift.get(\"end_time\")) if linear_drift.get(\"end_time\") else sync_end_time\n",
    "        drift_seconds = linear_drift.get(\"drift_seconds\", 0.0)\n",
    "        \n",
    "        if drift_seconds != 0.0 and start_time < end_time:\n",
    "            duration_seconds = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            def apply_linear_drift(timestamp):\n",
    "                if pd.isna(timestamp):\n",
    "                    return timestamp\n",
    "                \n",
    "                if start_time <= timestamp <= end_time:\n",
    "                    # Calculate progress through the drift period (0 to 1)\n",
    "                    progress = (timestamp - start_time).total_seconds() / duration_seconds\n",
    "                    # Apply proportional drift correction\n",
    "                    drift_correction = pd.Timedelta(seconds=drift_seconds * progress)\n",
    "                    return timestamp + drift_correction\n",
    "                return timestamp\n",
    "            \n",
    "            corrected_labels['Real_Start_Time'] = corrected_labels['Real_Start_Time'].apply(apply_linear_drift)\n",
    "            corrected_labels['Real_End_Time'] = corrected_labels['Real_End_Time'].apply(apply_linear_drift)\n",
    "            correction_log[\"linear_drift_applied\"] = True\n",
    "            print(f\"  üìê Applied linear drift correction: {drift_seconds}s over {duration_seconds/60:.1f} minutes\")\n",
    "    \n",
    "    # 3. Apply reference event corrections\n",
    "    ref_events = drift_params.get(\"reference_events\", {})\n",
    "    if ref_events.get(\"enabled\", False) and ref_events.get(\"events\"):\n",
    "        events_applied = 0\n",
    "        for event in ref_events[\"events\"]:\n",
    "            label_time = pd.to_datetime(event[\"label_time\"])\n",
    "            true_time = pd.to_datetime(event[\"true_time\"])\n",
    "            correction = true_time - label_time\n",
    "            \n",
    "            # Apply correction to labels near this reference point\n",
    "            # (You could implement more sophisticated interpolation between reference points)\n",
    "            tolerance = pd.Timedelta(minutes=30)  # Apply to labels within 30 minutes\n",
    "            mask = (\n",
    "                (corrected_labels['Real_Start_Time'] >= label_time - tolerance) &\n",
    "                (corrected_labels['Real_Start_Time'] <= label_time + tolerance)\n",
    "            )\n",
    "            \n",
    "            if mask.any():\n",
    "                corrected_labels.loc[mask, 'Real_Start_Time'] += correction\n",
    "                corrected_labels.loc[mask, 'Real_End_Time'] += correction\n",
    "                events_applied += mask.sum()\n",
    "                print(f\"  üéØ Applied reference event correction: {correction} to {mask.sum()} labels\")\n",
    "        \n",
    "        correction_log[\"reference_events_applied\"] = events_applied\n",
    "    \n",
    "    return corrected_labels, correction_log\n",
    "\n",
    "def create_label_drift_controls():\n",
    "    \"\"\"Create interactive controls for label drift correction\"\"\"\n",
    "    \n",
    "    # Manual offset control\n",
    "    manual_offset_slider = widgets.FloatSlider(\n",
    "        value=LABEL_DRIFT_PARAMS[\"manual_offset_seconds\"],\n",
    "        min=-300.0,  # -5 minutes\n",
    "        max=300.0,   # +5 minutes\n",
    "        step=0.1,\n",
    "        description='Manual Offset (s):',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Linear drift controls\n",
    "    enable_linear_drift = widgets.Checkbox(\n",
    "        value=LABEL_DRIFT_PARAMS[\"linear_drift_correction\"][\"enabled\"],\n",
    "        description='Enable Linear Drift Correction',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    linear_drift_slider = widgets.FloatSlider(\n",
    "        value=LABEL_DRIFT_PARAMS[\"linear_drift_correction\"][\"drift_seconds\"],\n",
    "        min=-60.0,   # -1 minute total drift\n",
    "        max=60.0,    # +1 minute total drift\n",
    "        step=0.1,\n",
    "        description='Linear Drift (s):',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Quick preset buttons\n",
    "    reset_corrections = widgets.Button(\n",
    "        description='üîÑ Reset All Corrections',\n",
    "        button_style='warning',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    apply_corrections = widgets.Button(\n",
    "        description='‚úÖ Apply & Replot',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    # Drift analysis display\n",
    "    drift_analysis_output = widgets.Output()\n",
    "    \n",
    "    return {\n",
    "        'manual_offset_slider': manual_offset_slider,\n",
    "        'enable_linear_drift': enable_linear_drift,\n",
    "        'linear_drift_slider': linear_drift_slider,\n",
    "        'reset_corrections': reset_corrections,\n",
    "        'apply_corrections': apply_corrections,\n",
    "        'drift_analysis_output': drift_analysis_output\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Label drift correction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59895fa",
   "metadata": {},
   "source": [
    "## 4. Load and Process Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ef9416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING SENSOR DATA ===\n",
      "Processing sensors for subject: OutSense-036\n",
      "Time window: 2024-02-06 08:51:10 to 2024-02-08 11:23:35\n",
      "\n",
      "--- Processing sensor: corsano_wrist_acc ---\n",
      "üìä Loaded 5344000 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3598s\n",
      "üìê Applied drift correction: 6s over 174745.0s interval\n",
      "üîç Filtered from 5344000 to 4366237 samples (81.7% retained)\n",
      "‚úÖ Final shape: (4366237, 3)\n",
      "‚úÖ Time range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "\n",
      "--- Processing sensor: cosinuss_ear_acc_x_acc_y_acc_z ---\n",
      "üìä Loaded 8422770 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3600s\n",
      "üîç Filtered from 8422770 to 5526048 samples (65.6% retained)\n",
      "‚úÖ Final shape: (5526048, 3)\n",
      "‚úÖ Time range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "\n",
      "--- Processing sensor: mbient_imu_wc_accelerometer ---\n",
      "üìä Loaded 9387669 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3621s\n",
      "üìê Applied drift correction: -15s over 174745.0s interval\n",
      "üîç Filtered from 9387669 to 9187973 samples (97.9% retained)\n",
      "‚úÖ Final shape: (9187973, 3)\n",
      "‚úÖ Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "\n",
      "--- Processing sensor: mbient_imu_wc_gyroscope ---\n",
      "üìä Loaded 9387670 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3621s\n",
      "üìê Applied drift correction: -15s over 174745.0s interval\n",
      "üîç Filtered from 9387670 to 9187973 samples (97.9% retained)\n",
      "‚úÖ Final shape: (9187973, 3)\n",
      "‚úÖ Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "\n",
      "--- Processing sensor: vivalnk_vv330_acceleration ---\n",
      "üìä Loaded 2099295 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3603s\n",
      "üìê Applied drift correction: -9s over 174745.0s interval\n",
      "üîç Filtered from 2099295 to 483997 samples (23.1% retained)\n",
      "‚úÖ Final shape: (483997, 3)\n",
      "‚úÖ Time range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "\n",
      "--- Processing sensor: sensomative_bottom_logger ---\n",
      "üìä Loaded 4600049 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3599s\n",
      "üîç Filtered from 4600049 to 2781544 samples (60.5% retained)\n",
      "‚úÖ Final shape: (2774494, 11)\n",
      "‚úÖ Time range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "\n",
      "--- Processing sensor: sensomative_back_logger ---\n",
      "‚ùå No data loaded for sensomative_back_logger\n",
      "\n",
      "--- Processing sensor: corsano_bioz_acc ---\n",
      "üìä Loaded 4438176 raw samples\n",
      "‚è±Ô∏è Applied time shift: 3599s\n",
      "üîç Filtered from 4438176 to 4406913 samples (99.3% retained)\n",
      "‚úÖ Final shape: (4406913, 3)\n",
      "‚úÖ Time range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "\n",
      "üìà Successfully processed 7 sensors:\n",
      "  üìä corsano_wrist: 4366237 samples, duration 2 days 01:34:03.001763105\n",
      "  üìä cosinuss_ear: 5526048 samples, duration 1 days 11:05:42.597000122\n",
      "  üìä mbient_acc: 9187973 samples, duration 2 days 02:32:24.981617451\n",
      "  üìä mbient_gyro: 9187973 samples, duration 2 days 02:32:24.981617451\n",
      "  üìä vivalnk_acc: 483997 samples, duration 2 days 01:33:36.792416096\n",
      "  üìä sensomative_bottom: 2774494 samples, duration 2 days 02:32:24.995999814\n",
      "  üìä corsano_bioz: 4406913 samples, duration 2 days 01:34:05\n"
     ]
    }
   ],
   "source": [
    "# Load and process each sensor with time shifts from Sync_Parameters.yaml\n",
    "print(f\"\\n=== LOADING SENSOR DATA ===\")\n",
    "print(f\"Processing sensors for subject: {SUBJECT_ID}\")\n",
    "print(f\"Time window: {data_window_start} to {data_window_end}\")\n",
    "\n",
    "processed_sensors = {}\n",
    "subject_correction_params = sync_params.get(SUBJECT_ID, {})\n",
    "\n",
    "if SUBJECT_ID == \"OutSense-425\":\n",
    "    subject_dir = os.path.join(raw_data_base_dir, \"OutSense-425_48h\")\n",
    "\n",
    "for sensor_name, sensor_settings in raw_data_parsing_config.items():\n",
    "    print(f\"\\n--- Processing sensor: {sensor_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Load raw sensor data\n",
    "        loader = select_data_loader(sensor_name)\n",
    "        sensor_data_raw = loader(subject_dir, sensor_name, sensor_settings)\n",
    "        \n",
    "        if sensor_data_raw.empty or 'time' not in sensor_data_raw.columns:\n",
    "            print(f\"‚ùå No data loaded for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"üìä Loaded {len(sensor_data_raw)} raw samples\")\n",
    "        \n",
    "        # Get time correction parameters for this sensor\n",
    "        sensor_corr_params = subject_correction_params.get(sensor_name, {'unit': 's'})\n",
    "        time_unit = sensor_corr_params.get('unit', 's')\n",
    "        shift_val = sensor_corr_params.get('shift', 0)\n",
    "        \n",
    "        # Apply time corrections\n",
    "        time_col_num = sensor_data_raw['time'].astype(float)\n",
    "        \n",
    "        # Convert to seconds if needed\n",
    "        if time_unit == 'ms':\n",
    "            time_col_num = time_col_num / 1000.0\n",
    "        \n",
    "        # Apply shift correction\n",
    "        if shift_val != 0:\n",
    "            time_col_num = time_col_num + shift_val\n",
    "            print(f\"‚è±Ô∏è Applied time shift: {shift_val}s\")\n",
    "        \n",
    "        # Apply drift correction if available\n",
    "        drift_params = sensor_corr_params.get('drift')\n",
    "        if drift_params and all(k in drift_params for k in ['t0', 't1', 'drift_secs']):\n",
    "            t0_ts = pd.Timestamp(drift_params['t0'])\n",
    "            t1_ts = pd.Timestamp(drift_params['t1'])\n",
    "            if not pd.isna(t0_ts) and not pd.isna(t1_ts):\n",
    "                t0, t1 = t0_ts.timestamp(), t1_ts.timestamp()\n",
    "                drift = drift_params['drift_secs']\n",
    "                time_col_num = time_col_num.apply(correct_timestamp_drift, args=(t0, t1, drift))\n",
    "                print(f\"üìê Applied drift correction: {drift}s over {t1-t0:.1f}s interval\")\n",
    "        \n",
    "        # Convert to datetime\n",
    "        corrected_timestamps = pd.to_datetime(time_col_num, unit='s', errors='coerce')\n",
    "        sensor_data_corrected = sensor_data_raw.drop(columns=['time']).copy()\n",
    "        sensor_data_corrected['time'] = corrected_timestamps\n",
    "        sensor_data_corrected.dropna(subset=['time'], inplace=True)\n",
    "        \n",
    "        if sensor_data_corrected.empty:\n",
    "            print(f\"‚ùå No valid data after time correction for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to data window\n",
    "        original_count = len(sensor_data_corrected)\n",
    "        time_mask = (sensor_data_corrected['time'] >= data_window_start) & (sensor_data_corrected['time'] <= data_window_end)\n",
    "        sensor_data_filtered = sensor_data_corrected[time_mask].copy()\n",
    "        \n",
    "        filtered_count = len(sensor_data_filtered)\n",
    "        retention_pct = (filtered_count / original_count * 100) if original_count > 0 else 0\n",
    "        print(f\"üîç Filtered from {original_count} to {filtered_count} samples ({retention_pct:.1f}% retained)\")\n",
    "        \n",
    "        if sensor_data_filtered.empty:\n",
    "            print(f\"‚ùå No data in time window for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Set time as index\n",
    "        sensor_data_filtered.set_index('time', inplace=True)\n",
    "        sensor_data_filtered.sort_index(inplace=True)\n",
    "        \n",
    "        # Apply basic preprocessing\n",
    "        sample_rate = sensor_settings.get('sample_rate', TARGET_FREQUENCY)\n",
    "        processed_data = process_modality_duplicates(sensor_data_filtered, sample_rate)\n",
    "        processed_data = handle_missing_data_interpolation(processed_data, max_interp_gap_s=2, target_freq=TARGET_FREQUENCY)\n",
    "        \n",
    "        # Apply column renaming\n",
    "        new_name, processed_data = modify_modality_names(processed_data, sensor_name)\n",
    "        \n",
    "        if processed_data.empty:\n",
    "            print(f\"‚ùå No data after preprocessing for {sensor_name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"‚úÖ Final shape: {processed_data.shape}\")\n",
    "        print(f\"‚úÖ Time range: {processed_data.index.min()} to {processed_data.index.max()}\")\n",
    "        \n",
    "        processed_sensors[new_name] = processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing sensor {sensor_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüìà Successfully processed {len(processed_sensors)} sensors:\")\n",
    "for sensor_name, data in processed_sensors.items():\n",
    "    duration = data.index.max() - data.index.min()\n",
    "    print(f\"  üìä {sensor_name}: {len(data)} samples, duration {duration}\")\n",
    "\n",
    "if not processed_sensors:\n",
    "    raise ValueError(\"No sensor data was successfully processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d7615",
   "metadata": {},
   "source": [
    "## 5. Interactive Plotting with Independent Time Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246a8d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INTERACTIVE SENSOR VISUALIZATION WITH LABEL DRIFT CORRECTION ===\n",
      "üéØ Each sensor has its own independent time axis\n",
      "üîç Perfect for manual sync event identification\n",
      "üè∑Ô∏è Labels from Final_Labels.csv with real-time drift correction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52290927780b4fe1a576174d6c0333e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>üéõÔ∏è Controls</h3>'), SelectMultiple(description='Select Sensors:'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying label time corrections to 258 labels...\n",
      "\n",
      "üöÄ Interactive visualization with label drift correction ready!\n",
      "\n",
      "üìù Instructions:\n",
      "  1. Select sensors to visualize\n",
      "  2. Adjust label drift corrections:\n",
      "     ‚Ä¢ Manual Offset: Move all labels forward/backward in time\n",
      "     ‚Ä¢ Linear Drift: Apply gradual time correction over sync period\n",
      "  3. Use 'Apply & Replot' to see corrections\n",
      "  4. Navigate through time to verify label alignment\n",
      "  5. Corrected labels show with ‚úì marker and green background\n",
      "\n",
      "üí° Label Drift Correction Features:\n",
      "  ‚úÖ Real-time manual offset adjustment (-5 to +5 minutes)\n",
      "  ‚úÖ Linear drift correction over sync period\n",
      "  ‚úÖ Visual feedback of corrections applied\n",
      "  ‚úÖ Reset button to clear all corrections\n",
      "  ‚úÖ Live preview during adjustment\n",
      "  ‚úÖ Corrected labels marked with ‚úì symbol\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9525e2d6598d46c9b236ac64620e1e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h3>üéõÔ∏è Controls</h3>'), SelectMultiple(description='Select Sensors:'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying label time corrections to 258 labels...\n",
      "\n",
      "üöÄ Interactive visualization with label drift correction ready!\n",
      "\n",
      "üìù Instructions:\n",
      "  1. Select sensors to visualize\n",
      "  2. Adjust label drift corrections:\n",
      "     ‚Ä¢ Manual Offset: Move all labels forward/backward in time\n",
      "     ‚Ä¢ Linear Drift: Apply gradual time correction over sync period\n",
      "  3. Use 'Apply & Replot' to see corrections\n",
      "  4. Navigate through time to verify label alignment\n",
      "  5. Corrected labels show with ‚úì marker and green background\n",
      "\n",
      "üí° Label Drift Correction Features:\n",
      "  ‚úÖ Real-time manual offset adjustment (-5 to +5 minutes)\n",
      "  ‚úÖ Linear drift correction over sync period\n",
      "  ‚úÖ Visual feedback of corrections applied\n",
      "  ‚úÖ Reset button to clear all corrections\n",
      "  ‚úÖ Live preview during adjustment\n",
      "  ‚úÖ Corrected labels marked with ‚úì symbol\n"
     ]
    }
   ],
   "source": [
    "# Create interactive plotting tool with independent time axes AND label drift correction\n",
    "print(\"=== INTERACTIVE SENSOR VISUALIZATION WITH LABEL DRIFT CORRECTION ===\")\n",
    "print(\"üéØ Each sensor has its own independent time axis\")\n",
    "print(\"üîç Perfect for manual sync event identification\")\n",
    "print(\"üè∑Ô∏è Labels from Final_Labels.csv with real-time drift correction\")\n",
    "\n",
    "# Create controls\n",
    "sensor_names = list(processed_sensors.keys())\n",
    "\n",
    "# Sensor selection (existing)\n",
    "sensor_selection = widgets.SelectMultiple(\n",
    "    options=sensor_names,\n",
    "    value=sensor_names[:3] if len(sensor_names) >= 3 else sensor_names,\n",
    "    description='Select Sensors:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='200px', width='300px')\n",
    ")\n",
    "\n",
    "# Label display controls (existing)\n",
    "show_labels = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show Labels',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "label_alpha = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.1,\n",
    "    max=0.8,\n",
    "    step=0.1,\n",
    "    description='Label Alpha:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create label drift correction controls\n",
    "label_drift_controls = create_label_drift_controls()\n",
    "\n",
    "# Store corrected labels globally for reuse\n",
    "corrected_labels_global = valid_labels.copy() if len(valid_labels) > 0 else pd.DataFrame()\n",
    "current_correction_log = {}\n",
    "\n",
    "def update_label_corrections():\n",
    "    \"\"\"Update label corrections based on current control values\"\"\"\n",
    "    global corrected_labels_global, current_correction_log\n",
    "    \n",
    "    if len(valid_labels) == 0:\n",
    "        return\n",
    "    \n",
    "    # Get current drift parameters from controls\n",
    "    current_drift_params = {\n",
    "        \"enabled\": True,\n",
    "        \"manual_offset_seconds\": label_drift_controls['manual_offset_slider'].value,\n",
    "        \"linear_drift_correction\": {\n",
    "            \"enabled\": label_drift_controls['enable_linear_drift'].value,\n",
    "            \"start_time\": sync_start_time,\n",
    "            \"end_time\": sync_end_time,\n",
    "            \"drift_seconds\": label_drift_controls['linear_drift_slider'].value\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Apply corrections\n",
    "    corrected_labels_global, current_correction_log = apply_label_time_corrections(\n",
    "        valid_labels, current_drift_params, sync_start_time, sync_end_time\n",
    "    )\n",
    "    \n",
    "    # Update drift analysis display\n",
    "    with label_drift_controls['drift_analysis_output']:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üìä Current Label Drift Corrections:\")\n",
    "        print(f\"  Manual Offset: {current_correction_log.get('manual_offset_applied', 0)}s\")\n",
    "        if current_correction_log.get('linear_drift_applied', False):\n",
    "            print(f\"  Linear Drift: {current_drift_params['linear_drift_correction']['drift_seconds']}s\")\n",
    "        print(f\"  Labels Affected: {current_correction_log.get('total_labels_corrected', 0)}\")\n",
    "        \n",
    "        if len(corrected_labels_global) > 0:\n",
    "            original_time_range = f\"{valid_labels['Real_Start_Time'].min()} to {valid_labels['Real_End_Time'].max()}\"\n",
    "            corrected_time_range = f\"{corrected_labels_global['Real_Start_Time'].min()} to {corrected_labels_global['Real_End_Time'].max()}\"\n",
    "            print(f\"\\n‚è±Ô∏è Time Range Comparison:\")\n",
    "            print(f\"  Original:  {original_time_range}\")\n",
    "            print(f\"  Corrected: {corrected_time_range}\")\n",
    "\n",
    "# Label filter (updated to use corrected labels)\n",
    "if len(valid_labels) > 0:\n",
    "    unique_labels_in_window = sorted(valid_labels['Label'].unique())\n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=unique_labels_in_window,\n",
    "        value=unique_labels_in_window[:10] if len(unique_labels_in_window) > 10 else unique_labels_in_window,\n",
    "        description='Show Labels:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='150px', width='300px')\n",
    "    )\n",
    "else:\n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        value=[],\n",
    "        description='Show Labels:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='150px', width='300px')\n",
    "    )\n",
    "\n",
    "# Time window controls\n",
    "center_time_text = widgets.Text(\n",
    "    value=sync_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    description='Center Time:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "window_minutes = widgets.IntSlider(\n",
    "    value=60,  # 1 hour window\n",
    "    min=1,\n",
    "    max=240,  # 4 hours max\n",
    "    step=1,\n",
    "    description='Window (min):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Time navigation buttons (10 minutes forward/backward)\n",
    "nav_backward_10min = widgets.Button(description='‚è™ -10min', button_style='', \n",
    "                                   layout=widgets.Layout(width='100px'))\n",
    "nav_forward_10min = widgets.Button(description='‚è© +10min', button_style='', \n",
    "                                  layout=widgets.Layout(width='100px'))\n",
    "\n",
    "# Quick jump buttons\n",
    "jump_sync_start = widgets.Button(description='üéØ Jump to Sync Start', button_style='success')\n",
    "jump_sync_end = widgets.Button(description='üéØ Jump to Sync End', button_style='warning')\n",
    "jump_data_start = widgets.Button(description='üìä Jump to Data Start', button_style='info')\n",
    "jump_data_end = widgets.Button(description='üìä Jump to Data End', button_style='info')\n",
    "\n",
    "# Plot button\n",
    "plot_button = widgets.Button(description='üìà Plot Sensors', button_style='primary', layout=widgets.Layout(width='150px'))\n",
    "\n",
    "# Auto-plot checkbox\n",
    "auto_plot = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Auto-plot on navigation',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Output area\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "def get_center_time():\n",
    "    \"\"\"Get center time from text widget\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(center_time_text.value)\n",
    "    except:\n",
    "        return sync_start_time\n",
    "\n",
    "def update_center_time(new_time):\n",
    "    \"\"\"Update center time text widget\"\"\"\n",
    "    center_time_text.value = new_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def plot_sensors_with_drift_correction(btn):\n",
    "    \"\"\"Plot selected sensors with corrected label overlays\"\"\"\n",
    "    \n",
    "    # Update label corrections first\n",
    "    update_label_corrections()\n",
    "    \n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        try:\n",
    "            selected_sensors = list(sensor_selection.value)\n",
    "            if not selected_sensors:\n",
    "                print(\"‚ùå Please select at least one sensor\")\n",
    "                return\n",
    "            \n",
    "            center_time = get_center_time()\n",
    "            window_mins = window_minutes.value\n",
    "            \n",
    "            # Calculate time window\n",
    "            half_window = pd.Timedelta(minutes=window_mins/2)\n",
    "            plot_start = center_time - half_window\n",
    "            plot_end = center_time + half_window\n",
    "            \n",
    "            print(f\"üìä Plotting {len(selected_sensors)} sensors with corrected labels\")\n",
    "            print(f\"‚è±Ô∏è Time window: {plot_start} to {plot_end} ({window_mins} minutes)\")\n",
    "            print(f\"üéØ Center time: {center_time}\")\n",
    "            \n",
    "            # Use corrected labels for plotting\n",
    "            if show_labels.value and len(corrected_labels_global) > 0:\n",
    "                selected_label_types = list(label_filter.value)\n",
    "                plot_labels = corrected_labels_global[\n",
    "                    (corrected_labels_global['Real_Start_Time'] <= plot_end) & \n",
    "                    (corrected_labels_global['Real_End_Time'] >= plot_start) &\n",
    "                    (corrected_labels_global['Label'].isin(selected_label_types))\n",
    "                ]\n",
    "                print(f\"üè∑Ô∏è Showing {len(plot_labels)} corrected labels in window\")\n",
    "                \n",
    "                # Show correction info\n",
    "                if current_correction_log.get('manual_offset_applied', 0) != 0:\n",
    "                    print(f\"üîß Manual offset applied: {current_correction_log['manual_offset_applied']}s\")\n",
    "                if current_correction_log.get('linear_drift_applied', False):\n",
    "                    print(f\"üîß Linear drift correction applied\")\n",
    "            else:\n",
    "                plot_labels = pd.DataFrame()\n",
    "            \n",
    "            # Create plot\n",
    "            fig, axes = plt.subplots(len(selected_sensors), 1, \n",
    "                                   figsize=(16, 3*len(selected_sensors)), \n",
    "                                   sharex=False)\n",
    "            if len(selected_sensors) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i, sensor_name in enumerate(selected_sensors):\n",
    "                ax = axes[i]\n",
    "                \n",
    "                if sensor_name not in processed_sensors:\n",
    "                    ax.text(0.5, 0.5, f'No data for {sensor_name}', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title(f'{sensor_name} - No Data')\n",
    "                    continue\n",
    "                \n",
    "                sensor_data = processed_sensors[sensor_name]\n",
    "                mask = (sensor_data.index >= plot_start) & (sensor_data.index <= plot_end)\n",
    "                plot_data = sensor_data[mask]\n",
    "                \n",
    "                if plot_data.empty:\n",
    "                    ax.text(0.5, 0.5, f'No data in time window for {sensor_name}', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title(f'{sensor_name} - No Data in Window')\n",
    "                    continue\n",
    "                \n",
    "                # Plot sensor data\n",
    "                numeric_cols = plot_data.select_dtypes(include=[np.number]).columns\n",
    "                for col in numeric_cols:\n",
    "                    ax.plot(plot_data.index, plot_data[col], \n",
    "                           label=col, alpha=0.7, linewidth=1)\n",
    "                \n",
    "                # Add CORRECTED label shading\n",
    "                if show_labels.value and len(plot_labels) > 0:\n",
    "                    y_min, y_max = ax.get_ylim() if len(numeric_cols) > 0 else (0, 1)\n",
    "                    \n",
    "                    label_count = {}\n",
    "                    for _, label_row in plot_labels.iterrows():\n",
    "                        label_name = label_row['Label']\n",
    "                        start_time = max(label_row['Real_Start_Time'], plot_start)\n",
    "                        end_time = min(label_row['Real_End_Time'], plot_end)\n",
    "                        \n",
    "                        if start_time < end_time:\n",
    "                            color = label_colors.get(label_name, 'gray')\n",
    "                            \n",
    "                            if label_name not in label_count:\n",
    "                                label_count[label_name] = 0\n",
    "                            label_count[label_name] += 1\n",
    "                            \n",
    "                            # Add shaded region with corrected times\n",
    "                            ax.axvspan(start_time, end_time, \n",
    "                                     alpha=label_alpha.value, \n",
    "                                     color=color,\n",
    "                                     label=f'{label_name} (corrected)' if label_count[label_name] == 1 else \"\")\n",
    "                            \n",
    "                            # Add label text for longer labels\n",
    "                            duration = end_time - start_time\n",
    "                            if duration > pd.Timedelta(minutes=2):\n",
    "                                mid_time = start_time + (end_time - start_time) / 2\n",
    "                                if len(numeric_cols) > 0:\n",
    "                                    y_pos = y_max - (y_max - y_min) * 0.05\n",
    "                                else:\n",
    "                                    y_pos = 0.5\n",
    "                                \n",
    "                                ax.text(mid_time, y_pos, f'{label_name}‚úì', \n",
    "                                       ha='center', va='top', rotation=0,\n",
    "                                       fontsize=8, alpha=0.8,\n",
    "                                       bbox=dict(boxstyle='round,pad=0.2', \n",
    "                                               facecolor='lightgreen', alpha=0.7))\n",
    "                \n",
    "                # Add sync events and other markers\n",
    "                if plot_start <= sync_start_time <= plot_end:\n",
    "                    ax.axvline(sync_start_time, color='red', linestyle='--', \n",
    "                             linewidth=2, alpha=0.8, label='üéØ Sync Start')\n",
    "                \n",
    "                if plot_start <= sync_end_time <= plot_end:\n",
    "                    ax.axvline(sync_end_time, color='darkred', linestyle='--', \n",
    "                             linewidth=2, alpha=0.8, label='üéØ Sync End')\n",
    "                \n",
    "                ax.axvline(center_time, color='green', linestyle=':', \n",
    "                         linewidth=1, alpha=0.6, label='Center')\n",
    "                \n",
    "                # Formatting\n",
    "                title_text = f'{sensor_name} ({len(numeric_cols)} channels)'\n",
    "                if show_labels.value and len(plot_labels) > 0:\n",
    "                    title_text += f' | {len(plot_labels)} corrected labels'\n",
    "                ax.set_title(title_text)\n",
    "                ax.set_ylabel('Value')\n",
    "                ax.set_xlabel('Time')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, window_mins//10)))\n",
    "                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "                \n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                if len(handles) <= 15:\n",
    "                    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "            \n",
    "            # Title with correction info\n",
    "            title_text = f'Sensor Data with Corrected Labels - Independent Time Axes\\n'\n",
    "            title_text += f'Window: {plot_start} to {plot_end}'\n",
    "            if current_correction_log.get('manual_offset_applied', 0) != 0:\n",
    "                title_text += f' | Manual Offset: {current_correction_log[\"manual_offset_applied\"]}s'\n",
    "            if current_correction_log.get('linear_drift_applied', False):\n",
    "                title_text += f' | Linear Drift Applied'\n",
    "            \n",
    "            plt.suptitle(title_text, fontsize=14, y=0.98)\n",
    "            plt.tight_layout()\n",
    "            plt.subplots_adjust(right=0.85, top=0.92)\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating plot: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "# Button event handlers\n",
    "def reset_label_corrections(btn):\n",
    "    \"\"\"Reset all label corrections to zero\"\"\"\n",
    "    label_drift_controls['manual_offset_slider'].value = 0.0\n",
    "    label_drift_controls['enable_linear_drift'].value = False\n",
    "    label_drift_controls['linear_drift_slider'].value = 0.0\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def apply_and_replot(btn):\n",
    "    \"\"\"Apply current corrections and replot\"\"\"\n",
    "    plot_sensors_with_drift_correction(None)\n",
    "\n",
    "# Connect button events\n",
    "plot_button.on_click(plot_sensors_with_drift_correction)\n",
    "label_drift_controls['reset_corrections'].on_click(reset_label_corrections)\n",
    "label_drift_controls['apply_corrections'].on_click(apply_and_replot)\n",
    "\n",
    "# Update existing navigation functions to use corrected plotting\n",
    "def navigate_backward_10min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time - pd.Timedelta(minutes=10)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def navigate_forward_10min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time + pd.Timedelta(minutes=10)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_sync_start(btn):\n",
    "    update_center_time(sync_start_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_sync_end(btn):\n",
    "    update_center_time(sync_end_time)\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_data_start(btn):\n",
    "    all_starts = [data.index.min() for data in processed_sensors.values()]\n",
    "    earliest = min(all_starts)\n",
    "    update_center_time(earliest + pd.Timedelta(minutes=window_minutes.value/2))\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "def jump_to_data_end(btn):\n",
    "    all_ends = [data.index.max() for data in processed_sensors.values()]\n",
    "    latest = max(all_ends)\n",
    "    update_center_time(latest - pd.Timedelta(minutes=window_minutes.value/2))\n",
    "    if auto_plot.value:\n",
    "        plot_sensors_with_drift_correction(None)\n",
    "\n",
    "# Connect navigation buttons\n",
    "nav_backward_10min.on_click(navigate_backward_10min)\n",
    "nav_forward_10min.on_click(navigate_forward_10min)\n",
    "jump_sync_start.on_click(jump_to_sync_start)\n",
    "jump_sync_end.on_click(jump_to_sync_end)\n",
    "jump_data_start.on_click(jump_to_data_start)\n",
    "jump_data_end.on_click(jump_to_data_end)\n",
    "\n",
    "# Layout with label drift correction controls\n",
    "label_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>üè∑Ô∏è Label Controls & Drift Correction</h4>\"),\n",
    "    show_labels,\n",
    "    label_alpha,\n",
    "    label_filter,\n",
    "    widgets.HTML(\"<hr><h5>‚öôÔ∏è Label Time Drift Correction</h5>\"),\n",
    "    label_drift_controls['manual_offset_slider'],\n",
    "    label_drift_controls['enable_linear_drift'],\n",
    "    label_drift_controls['linear_drift_slider'],\n",
    "    widgets.HBox([\n",
    "        label_drift_controls['reset_corrections'], \n",
    "        label_drift_controls['apply_corrections']\n",
    "    ]),\n",
    "    label_drift_controls['drift_analysis_output']\n",
    "]) if len(valid_labels) > 0 else widgets.HTML(\"<p>No labels available</p>\")\n",
    "\n",
    "time_navigation = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>‚è±Ô∏è Time Navigation</h4>\"),\n",
    "    center_time_text,\n",
    "    window_minutes,\n",
    "    widgets.HBox([nav_backward_10min, nav_forward_10min]),\n",
    "    auto_plot,\n",
    "    widgets.HBox([jump_sync_start, jump_sync_end]),\n",
    "    widgets.HBox([jump_data_start, jump_data_end]),\n",
    "])\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üéõÔ∏è Controls</h3>\"),\n",
    "    sensor_selection,\n",
    "    label_controls,\n",
    "    time_navigation,\n",
    "    plot_button\n",
    "])\n",
    "\n",
    "display(widgets.VBox([controls, plot_output]))\n",
    "\n",
    "# Initialize with default corrections\n",
    "update_label_corrections()\n",
    "\n",
    "print(\"\\nüöÄ Interactive visualization with label drift correction ready!\")\n",
    "print(\"\\nüìù Instructions:\")\n",
    "print(\"  1. Select sensors to visualize\")\n",
    "print(\"  2. Adjust label drift corrections:\")\n",
    "print(\"     ‚Ä¢ Manual Offset: Move all labels forward/backward in time\")\n",
    "print(\"     ‚Ä¢ Linear Drift: Apply gradual time correction over sync period\")\n",
    "print(\"  3. Use 'Apply & Replot' to see corrections\")\n",
    "print(\"  4. Navigate through time to verify label alignment\")\n",
    "print(\"  5. Corrected labels show with ‚úì marker and green background\")\n",
    "print(\"\\nüí° Label Drift Correction Features:\")\n",
    "print(\"  ‚úÖ Real-time manual offset adjustment (-5 to +5 minutes)\")\n",
    "print(\"  ‚úÖ Linear drift correction over sync period\")\n",
    "print(\"  ‚úÖ Visual feedback of corrections applied\")\n",
    "print(\"  ‚úÖ Reset button to clear all corrections\")\n",
    "print(\"  ‚úÖ Live preview during adjustment\")\n",
    "print(\"  ‚úÖ Corrected labels marked with ‚úì symbol\")\n",
    "\n",
    "# Layout with label drift correction controls\n",
    "label_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>üè∑Ô∏è Label Controls & Drift Correction</h4>\"),\n",
    "    show_labels,\n",
    "    label_alpha,\n",
    "    label_filter,\n",
    "    widgets.HTML(\"<hr><h5>‚öôÔ∏è Label Time Drift Correction</h5>\"),\n",
    "    label_drift_controls['manual_offset_slider'],\n",
    "    label_drift_controls['enable_linear_drift'],\n",
    "    label_drift_controls['linear_drift_slider'],\n",
    "    widgets.HBox([\n",
    "        label_drift_controls['reset_corrections'], \n",
    "        label_drift_controls['apply_corrections']\n",
    "    ]),\n",
    "    label_drift_controls['drift_analysis_output']\n",
    "]) if len(valid_labels) > 0 else widgets.HTML(\"<p>No labels available</p>\")\n",
    "\n",
    "time_navigation = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>‚è±Ô∏è Time Navigation</h4>\"),\n",
    "    center_time_text,\n",
    "    window_minutes,\n",
    "    widgets.HBox([nav_backward_10min, nav_forward_10min]),\n",
    "    auto_plot,\n",
    "    widgets.HBox([jump_sync_start, jump_sync_end]),\n",
    "    widgets.HBox([jump_data_start, jump_data_end]),\n",
    "])\n",
    "\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üéõÔ∏è Controls</h3>\"),\n",
    "    sensor_selection,\n",
    "    label_controls,\n",
    "    time_navigation,\n",
    "    plot_button\n",
    "])\n",
    "\n",
    "display(widgets.VBox([controls, plot_output]))\n",
    "\n",
    "# Initialize with default corrections\n",
    "update_label_corrections()\n",
    "\n",
    "print(\"\\nüöÄ Interactive visualization with label drift correction ready!\")\n",
    "print(\"\\nüìù Instructions:\")\n",
    "print(\"  1. Select sensors to visualize\")\n",
    "print(\"  2. Adjust label drift corrections:\")\n",
    "print(\"     ‚Ä¢ Manual Offset: Move all labels forward/backward in time\")\n",
    "print(\"     ‚Ä¢ Linear Drift: Apply gradual time correction over sync period\")\n",
    "print(\"  3. Use 'Apply & Replot' to see corrections\")\n",
    "print(\"  4. Navigate through time to verify label alignment\")\n",
    "print(\"  5. Corrected labels show with ‚úì marker and green background\")\n",
    "print(\"\\nüí° Label Drift Correction Features:\")\n",
    "print(\"  ‚úÖ Real-time manual offset adjustment (-5 to +5 minutes)\")\n",
    "print(\"  ‚úÖ Linear drift correction over sync period\")\n",
    "print(\"  ‚úÖ Visual feedback of corrections applied\")\n",
    "print(\"  ‚úÖ Reset button to clear all corrections\")\n",
    "print(\"  ‚úÖ Live preview during adjustment\")\n",
    "print(\"  ‚úÖ Corrected labels marked with ‚úì symbol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c6899",
   "metadata": {},
   "source": [
    "## 6. Summary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56472a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUMMARY ===\n",
      "Subject: OutSense-036\n",
      "Data window: 2h around sync start\n",
      "Sync start: 2024-02-06 09:51:10\n",
      "Sync end: 2024-02-08 10:23:35\n",
      "Processed sensors: 7\n",
      "Available labels: 258\n",
      "\n",
      "üìä Sensor Details:\n",
      "  üìà corsano_wrist:\n",
      "    Samples: 4366237\n",
      "    Time range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "    Duration: 2 days 01:34:03.001763105\n",
      "    Columns: ['wrist_acc_x', 'wrist_acc_y', 'wrist_acc_z']\n",
      "    Time shift applied: 0s\n",
      "  üìà cosinuss_ear:\n",
      "    Samples: 5526048\n",
      "    Time range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "    Duration: 1 days 11:05:42.597000122\n",
      "    Columns: ['ear_acc_x', 'ear_acc_y', 'ear_acc_z']\n",
      "    Time shift applied: 0s\n",
      "  üìà mbient_acc:\n",
      "    Samples: 9187973\n",
      "    Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    Duration: 2 days 02:32:24.981617451\n",
      "    Columns: ['x_axis_g', 'y_axis_g', 'z_axis_g']\n",
      "    Time shift applied: 0s\n",
      "  üìà mbient_gyro:\n",
      "    Samples: 9187973\n",
      "    Time range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    Duration: 2 days 02:32:24.981617451\n",
      "    Columns: ['x_axis_dps', 'y_axis_dps', 'z_axis_dps']\n",
      "    Time shift applied: 0s\n",
      "  üìà vivalnk_acc:\n",
      "    Samples: 483997\n",
      "    Time range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "    Duration: 2 days 01:33:36.792416096\n",
      "    Columns: ['vivalnk_acc_x', 'vivalnk_acc_y', 'vivalnk_acc_z']\n",
      "    Time shift applied: 0s\n",
      "  üìà sensomative_bottom:\n",
      "    Samples: 2774494\n",
      "    Time range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "    Duration: 2 days 02:32:24.995999814\n",
      "    Columns: ['bottom_value_1', 'bottom_value_2', 'bottom_value_3', 'bottom_value_4', 'bottom_value_5', 'bottom_value_6', 'bottom_value_7', 'bottom_value_8', 'bottom_value_9', 'bottom_value_10', 'bottom_value_11']\n",
      "    Time shift applied: 0s\n",
      "  üìà corsano_bioz:\n",
      "    Samples: 4406913\n",
      "    Time range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "    Duration: 2 days 01:34:05\n",
      "    Columns: ['bioz_acc_x', 'bioz_acc_y', 'bioz_acc_z']\n",
      "    Time shift applied: 0s\n",
      "\n",
      "üè∑Ô∏è Label Details:\n",
      "  Total labels for OutSense-036: 258\n",
      "  Label time range: 2024-02-06 10:52:27 to 2024-02-08 10:18:02\n",
      "  Label duration span: 1 days 23:25:35\n",
      "  Top 5 most common labels:\n",
      "    üìã conversation: 54 instances (avg: 149.1s)\n",
      "    üìã self_propulsion: 50 instances (avg: 27.4s)\n",
      "    üìã dark: 48 instances (avg: 1494.9s)\n",
      "    üìã cycling: 22 instances (avg: 239.6s)\n",
      "    üìã sitting_wheelchair: 13 instances (avg: 47.2s)\n",
      "  Labels in current data window: 258\n",
      "\n",
      "üé® Label Colors (29 unique labels):\n",
      "  üü¶ arm_raises\n",
      "  üü¶ arm_sleeve\n",
      "  üü¶ assisted_propulsion\n",
      "  üü¶ bending\n",
      "  üü¶ chair_to_wheelchair\n",
      "  üü¶ conversation\n",
      "  üü¶ cycling\n",
      "  üü¶ dark\n",
      "  üü¶ drinking\n",
      "  üü¶ eating\n",
      "  ... and 19 more labels\n",
      "\n",
      "üéØ Ready for manual sync event identification with label overlay!\n",
      "Use the interactive plot above to examine each sensor independently.\n",
      "Labels from Final_Labels.csv will be displayed as colored shaded areas.\n"
     ]
    }
   ],
   "source": [
    "# Display summary information\n",
    "print(\"=== SUMMARY ===\")\n",
    "print(f\"Subject: {SUBJECT_ID}\")\n",
    "print(f\"Data window: {HOURS_AROUND_SYNC}h around sync start\")\n",
    "print(f\"Sync start: {sync_start_time}\")\n",
    "print(f\"Sync end: {sync_end_time}\")\n",
    "print(f\"Processed sensors: {len(processed_sensors)}\")\n",
    "print(f\"Available labels: {len(valid_labels)}\")\n",
    "\n",
    "print(\"\\nüìä Sensor Details:\")\n",
    "for sensor_name, data in processed_sensors.items():\n",
    "    # Get time shift applied\n",
    "    original_sensor_name = sensor_name  # May be modified by modify_modality_names\n",
    "    for orig_name in raw_data_parsing_config.keys():\n",
    "        if orig_name in sensor_name:\n",
    "            original_sensor_name = orig_name\n",
    "            break\n",
    "    \n",
    "    sensor_corr_params = subject_correction_params.get(original_sensor_name, {})\n",
    "    shift_applied = sensor_corr_params.get('shift', 0)\n",
    "    \n",
    "    print(f\"  üìà {sensor_name}:\")\n",
    "    print(f\"    Samples: {len(data)}\")\n",
    "    print(f\"    Time range: {data.index.min()} to {data.index.max()}\")\n",
    "    print(f\"    Duration: {data.index.max() - data.index.min()}\")\n",
    "    print(f\"    Columns: {list(data.columns)}\")\n",
    "    print(f\"    Time shift applied: {shift_applied}s\")\n",
    "\n",
    "if len(valid_labels) > 0:\n",
    "    print(f\"\\nüè∑Ô∏è Label Details:\")\n",
    "    print(f\"  Total labels for {SUBJECT_ID}: {len(valid_labels)}\")\n",
    "    \n",
    "    # Time range of labels\n",
    "    label_start = valid_labels['Real_Start_Time'].min()\n",
    "    label_end = valid_labels['Real_End_Time'].max()\n",
    "    print(f\"  Label time range: {label_start} to {label_end}\")\n",
    "    print(f\"  Label duration span: {label_end - label_start}\")\n",
    "    \n",
    "    # Most common labels\n",
    "    print(f\"  Top 5 most common labels:\")\n",
    "    for label, count in valid_labels['Label'].value_counts().head(5).items():\n",
    "        total_duration = 0\n",
    "        label_instances = valid_labels[valid_labels['Label'] == label]\n",
    "        for _, row in label_instances.iterrows():\n",
    "            duration = row['Real_End_Time'] - row['Real_Start_Time']\n",
    "            total_duration += duration.total_seconds()\n",
    "        avg_duration = total_duration / count if count > 0 else 0\n",
    "        print(f\"    üìã {label}: {count} instances (avg: {avg_duration:.1f}s)\")\n",
    "    \n",
    "    # Labels in the data window\n",
    "    window_labels = valid_labels[\n",
    "        (valid_labels['Real_Start_Time'] <= data_window_end) & \n",
    "        (valid_labels['Real_End_Time'] >= data_window_start)\n",
    "    ]\n",
    "    print(f\"  Labels in current data window: {len(window_labels)}\")\n",
    "    \n",
    "    # Color legend\n",
    "    print(f\"\\nüé® Label Colors ({len(label_colors)} unique labels):\")\n",
    "    for i, (label, color) in enumerate(sorted(label_colors.items())[:10]):\n",
    "        print(f\"  üü¶ {label}\")\n",
    "    if len(label_colors) > 10:\n",
    "        print(f\"  ... and {len(label_colors) - 10} more labels\")\n",
    "\n",
    "print(\"\\nüéØ Ready for manual sync event identification with label overlay!\")\n",
    "print(\"Use the interactive plot above to examine each sensor independently.\")\n",
    "print(\"Labels from Final_Labels.csv will be displayed as colored shaded areas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522d736",
   "metadata": {},
   "source": [
    "## 7. Export Corrected Labels\n",
    "\n",
    "Export the label drift corrections for reuse and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42955768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Label Export Tool:\n",
      "  Use this to save your corrected labels for reuse in other workflows\n",
      "  Exports both the corrected CSV and correction parameters for reproducibility\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e82cd421664409b41fd71b370e9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='üíæ Export Corrected Labels', layout=Layout(width='200px'), style=Bu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export corrected labels for reuse\n",
    "def export_corrected_labels():\n",
    "    \"\"\"Export the currently corrected labels to CSV and show drift parameters\"\"\"\n",
    "    \n",
    "    if len(corrected_labels_global) == 0:\n",
    "        print(\"‚ùå No corrected labels to export\")\n",
    "        return\n",
    "    \n",
    "    # Create export filename\n",
    "    export_filename = f\"{SUBJECT_ID}_corrected_labels.csv\"\n",
    "    export_path = os.path.join(project_root, export_filename)\n",
    "    \n",
    "    # Export corrected labels\n",
    "    try:\n",
    "        corrected_labels_global.to_csv(export_path, index=False)\n",
    "        print(f\"‚úÖ Corrected labels exported to: {export_path}\")\n",
    "        print(f\"üìä Exported {len(corrected_labels_global)} corrected labels\")\n",
    "        \n",
    "        # Show correction summary\n",
    "        print(f\"\\nüîß Applied Corrections Summary:\")\n",
    "        print(f\"  Manual Offset: {current_correction_log.get('manual_offset_applied', 0)}s\")\n",
    "        if current_correction_log.get('linear_drift_applied', False):\n",
    "            print(f\"  Linear Drift: {label_drift_controls['linear_drift_slider'].value}s over sync period\")\n",
    "        print(f\"  Labels Corrected: {current_correction_log.get('total_labels_corrected', 0)}\")\n",
    "        \n",
    "        # Export correction parameters for reproducibility\n",
    "        correction_params = {\n",
    "            \"subject_id\": SUBJECT_ID,\n",
    "            \"export_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"sync_start_time\": sync_start_time.isoformat(),\n",
    "            \"sync_end_time\": sync_end_time.isoformat(),\n",
    "            \"corrections_applied\": {\n",
    "                \"manual_offset_seconds\": current_correction_log.get('manual_offset_applied', 0),\n",
    "                \"linear_drift_enabled\": current_correction_log.get('linear_drift_applied', False),\n",
    "                \"linear_drift_seconds\": label_drift_controls['linear_drift_slider'].value if current_correction_log.get('linear_drift_applied', False) else 0,\n",
    "                \"total_labels_corrected\": current_correction_log.get('total_labels_corrected', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        params_filename = f\"{SUBJECT_ID}_label_correction_params.yaml\"\n",
    "        params_path = os.path.join(project_root, params_filename)\n",
    "        \n",
    "        with open(params_path, 'w') as f:\n",
    "            yaml.dump(correction_params, f, default_flow_style=False)\n",
    "        \n",
    "        print(f\"‚úÖ Correction parameters saved to: {params_path}\")\n",
    "        \n",
    "        # Show time shift comparison\n",
    "        if len(valid_labels) > 0:\n",
    "            original_time_range = f\"{valid_labels['Real_Start_Time'].min()} to {valid_labels['Real_End_Time'].max()}\"\n",
    "            corrected_time_range = f\"{corrected_labels_global['Real_Start_Time'].min()} to {corrected_labels_global['Real_End_Time'].max()}\"\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è Time Range Comparison:\")\n",
    "            print(f\"  Original:  {original_time_range}\")\n",
    "            print(f\"  Corrected: {corrected_time_range}\")\n",
    "            \n",
    "            # Calculate total shift\n",
    "            original_start = valid_labels['Real_Start_Time'].min()\n",
    "            corrected_start = corrected_labels_global['Real_Start_Time'].min()\n",
    "            total_shift = (corrected_start - original_start).total_seconds()\n",
    "            print(f\"  Net Shift: {total_shift}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting corrected labels: {e}\")\n",
    "\n",
    "# Create export button\n",
    "export_button = widgets.Button(\n",
    "    description='üíæ Export Corrected Labels',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "export_button.on_click(lambda btn: export_corrected_labels())\n",
    "\n",
    "print(\"üì§ Label Export Tool:\")\n",
    "print(\"  Use this to save your corrected labels for reuse in other workflows\")\n",
    "print(\"  Exports both the corrected CSV and correction parameters for reproducibility\")\n",
    "\n",
    "display(export_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0978a",
   "metadata": {},
   "source": [
    "## 7. Final Data Visualization Export\n",
    "\n",
    "**Purpose**: Generate comprehensive PDF visualizations of all processed sensor data with labels for final quality check before AI model preprocessing.\n",
    "\n",
    "**Features**:\n",
    "- Create subject-specific folder structure in results directory\n",
    "- One PDF per sensor with all data plotted\n",
    "- 10-minute segments per page in landscape format\n",
    "- Synchronized time axis across all sensors and labels\n",
    "- Labels shown as shaded areas with consistent colors\n",
    "- Page-by-page navigation through entire dataset\n",
    "- Final validation before AI preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66489818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"=== FINAL DATA VISUALIZATION EXPORT ===\")\\nprint(\"üéØ Creating comprehensive PDF plots for quality check before AI preprocessing\")\\n\\n# Create results directory structure\\nresults_base_dir = os.path.join(project_root, \\'stirnimann_r/results\\')\\nsubject_results_dir = os.path.join(results_base_dir, SUBJECT_ID)\\nos.makedirs(subject_results_dir, exist_ok=True)\\n\\nprint(f\"üìÇ Results directory: {subject_results_dir}\")\\n\\n# Configuration for plotting\\nMINUTES_PER_PAGE = 10  # 10 minutes per page\\nPAGE_WIDTH = 16       # Landscape format\\nPAGE_HEIGHT = 12      # Landscape format\\nDPI = 150            # Good quality for PDFs\\n\\n# Determine overall time range across all sensors\\nprint(\"\\nüìä Determining overall time range...\")\\nall_start_times = []\\nall_end_times = []\\n\\nfor sensor_name, sensor_data in processed_sensors.items():\\n    all_start_times.append(sensor_data.index.min())\\n    all_end_times.append(sensor_data.index.max())\\n\\nif all_start_times and all_end_times:\\n    overall_start = min(all_start_times)\\n    overall_end = max(all_end_times)\\n    total_duration = overall_end - overall_start\\n    \\n    print(f\"‚è±Ô∏è Overall time range: {overall_start} to {overall_end}\")\\n    print(f\"‚è±Ô∏è Total duration: {total_duration}\")\\n    \\n    # Calculate number of pages needed\\n    total_minutes = total_duration.total_seconds() / 60\\n    num_pages = math.ceil(total_minutes / MINUTES_PER_PAGE)\\n    \\n    print(f\"üìÑ Will create {num_pages} pages ({MINUTES_PER_PAGE} minutes each)\")\\n    \\n    # Prepare labels for the entire time range\\n    if len(valid_labels) > 0:\\n        # Filter labels to the overall time range\\n        full_range_labels = valid_labels[\\n            (valid_labels[\\'Real_Start_Time\\'] <= overall_end) & \\n            (valid_labels[\\'Real_End_Time\\'] >= overall_start)\\n        ]\\n        print(f\"üè∑Ô∏è {len(full_range_labels)} labels in full time range\")\\n    else:\\n        full_range_labels = pd.DataFrame()\\n    \\n    # Process each sensor\\n    for sensor_name, sensor_data in processed_sensors.items():\\n        print(f\"\\n--- Processing sensor: {sensor_name} ---\")\\n        \\n        # Create sensor-specific directory\\n        sensor_dir = os.path.join(subject_results_dir, sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\'))\\n        os.makedirs(sensor_dir, exist_ok=True)\\n        \\n        # Define PDF filename\\n        pdf_filename = f\"{SUBJECT_ID}_{sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\')}_complete_data.pdf\"\\n        pdf_path = os.path.join(sensor_dir, pdf_filename)\\n        \\n        print(f\"üìÑ Creating PDF: {pdf_path}\")\\n        \\n        # Get numeric columns for this sensor\\n        numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\\n        print(f\"üìà Plotting {len(numeric_cols)} channels: {list(numeric_cols)}\")\\n        \\n        # Create PDF with multiple pages\\n        with PdfPages(pdf_path) as pdf:\\n            for page_num in range(num_pages):\\n                print(f\"  üìÑ Creating page {page_num + 1}/{num_pages}...\")\\n                \\n                # Calculate time window for this page\\n                page_start = overall_start + pd.Timedelta(minutes=page_num * MINUTES_PER_PAGE)\\n                page_end = overall_start + pd.Timedelta(minutes=(page_num + 1) * MINUTES_PER_PAGE)\\n                \\n                # Don\\'t go beyond the actual data range\\n                page_end = min(page_end, overall_end)\\n                \\n                # Filter sensor data for this page\\n                page_mask = (sensor_data.index >= page_start) & (sensor_data.index <= page_end)\\n                page_data = sensor_data[page_mask]\\n                \\n                # Filter labels for this page\\n                if len(full_range_labels) > 0:\\n                    page_labels = full_range_labels[\\n                        (full_range_labels[\\'Real_Start_Time\\'] <= page_end) & \\n                        (full_range_labels[\\'Real_End_Time\\'] >= page_start)\\n                    ]\\n                else:\\n                    page_labels = pd.DataFrame()\\n                \\n                # Create figure\\n                fig, ax = plt.subplots(1, 1, figsize=(PAGE_WIDTH, PAGE_HEIGHT), dpi=DPI)\\n                \\n                # Plot sensor data\\n                if not page_data.empty and len(numeric_cols) > 0:\\n                    for col in numeric_cols:\\n                        ax.plot(page_data.index, page_data[col], \\n                               label=col, alpha=0.8, linewidth=1.5)\\n                    \\n                    # Get y-axis limits for label positioning\\n                    y_min, y_max = ax.get_ylim()\\n                    y_range = y_max - y_min\\n                    \\n                    # Add label shading\\n                    if len(page_labels) > 0:\\n                        label_count = {}\\n                        for _, label_row in page_labels.iterrows():\\n                            label_name = label_row[\\'Label\\']\\n                            start_time = max(label_row[\\'Real_Start_Time\\'], page_start)\\n                            end_time = min(label_row[\\'Real_End_Time\\'], page_end)\\n                            \\n                            if start_time < end_time:  # Valid time range\\n                                color = label_colors.get(label_name, \\'gray\\')\\n                                \\n                                # Count occurrences for legend\\n                                if label_name not in label_count:\\n                                    label_count[label_name] = 0\\n                                label_count[label_name] += 1\\n                                \\n                                # Add shaded region\\n                                ax.axvspan(start_time, end_time, \\n                                         alpha=0.3, \\n                                         color=color,\\n                                         label=f\\'{label_name}\\' if label_count[label_name] == 1 else \"\",\\n                                         zorder=0)  # Behind the data\\n                                \\n                                # Add label text for longer labels\\n                                duration = end_time - start_time\\n                                if duration > pd.Timedelta(minutes=1):  # Only show text for labels > 1 minute\\n                                    mid_time = start_time + (end_time - start_time) / 2\\n                                    y_pos = y_max - y_range * 0.05\\n                                    \\n                                    ax.text(mid_time, y_pos, label_name, \\n                                           ha=\\'center\\', va=\\'top\\', rotation=0,\\n                                           fontsize=10, alpha=0.9,\\n                                           bbox=dict(boxstyle=\\'round,pad=0.3\\', \\n                                                   facecolor=\\'white\\', alpha=0.8))\\n                    \\n                    # Add sync event markers if in range\\n                    if page_start <= sync_start_time <= page_end:\\n                        ax.axvline(sync_start_time, color=\\'red\\', linestyle=\\'--\\', \\n                                 linewidth=3, alpha=0.9, label=\\'üéØ Sync Start\\')\\n                    \\n                    if page_start <= sync_end_time <= page_end:\\n                        ax.axvline(sync_end_time, color=\\'darkred\\', linestyle=\\'--\\', \\n                                 linewidth=3, alpha=0.9, label=\\'üéØ Sync End\\')\\n                    \\n                else:\\n                    # No data on this page\\n                    ax.text(0.5, 0.5, f\\'No {sensor_name} data in this time window\\', \\n                           ha=\\'center\\', va=\\'center\\', transform=ax.transAxes,\\n                           fontsize=16, alpha=0.7)\\n                \\n                # Set time axis limits to exactly match the page window\\n                ax.set_xlim(page_start, page_end)\\n                \\n                # Formatting\\n                page_title = f\\'{sensor_name} - Page {page_num + 1}/{num_pages}\\n\\'\\n                page_title += f\\'Time: {page_start.strftime(\"%Y-%m-%d %H:%M:%S\")} to {page_end.strftime(\"%H:%M:%S\")}\\'\\n                if len(page_labels) > 0:\\n                    page_title += f\\' | {len(page_labels)} labels\\'\\n                \\n                ax.set_title(page_title, fontsize=14, fontweight=\\'bold\\')\\n                ax.set_xlabel(\\'Time\\', fontsize=12)\\n                ax.set_ylabel(\\'Value\\', fontsize=12)\\n                ax.grid(True, alpha=0.3)\\n                \\n                # Format time axis\\n                ax.xaxis.set_major_formatter(mdates.DateFormatter(\\'%H:%M\\'))\\n                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, MINUTES_PER_PAGE//10)))\\n                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\\n                \\n                # Legend (only if not too many items)\\n                handles, labels = ax.get_legend_handles_labels()\\n                if len(handles) <= 20:  # Reasonable number for legend\\n                    ax.legend(bbox_to_anchor=(1.02, 1), loc=\\'upper left\\', fontsize=10)\\n                \\n                # Add metadata text\\n                metadata_text = f\\'Subject: {SUBJECT_ID} | Sensor: {sensor_name}\\n\\'\\n                metadata_text += f\\'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n\\'\\n                metadata_text += f\\'Total Duration: {total_duration} | Target Freq: {TARGET_FREQUENCY}Hz\\'\\n                \\n                fig.text(0.02, 0.02, metadata_text, fontsize=8, alpha=0.7,\\n                        verticalalignment=\\'bottom\\')\\n                \\n                plt.tight_layout()\\n                plt.subplots_adjust(right=0.85, bottom=0.1)\\n                \\n                # Save page to PDF\\n                pdf.savefig(fig, dpi=DPI, bbox_inches=\\'tight\\')\\n                plt.close(fig)\\n        \\n        print(f\"‚úÖ PDF saved: {pdf_path}\")\\n        print(f\"   üìÑ {num_pages} pages, {MINUTES_PER_PAGE} minutes each\")\\n        if len(numeric_cols) > 0:\\n            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\\n            print(f\"   ‚è±Ô∏è Sensor duration: {sensor_duration}\")\\n            print(f\"   üìä {len(sensor_data)} samples, {len(numeric_cols)} channels\")\\n    \\n    # Create summary report\\n    print(f\"\\nüìã Creating summary report...\")\\n    summary_path = os.path.join(subject_results_dir, f\"{SUBJECT_ID}_data_summary.txt\")\\n    \\n    with open(summary_path, \\'w\\') as f:\\n        f.write(f\"DATA VISUALIZATION SUMMARY REPORT\\n\")\\n        f.write(f\"=================================\\n\\n\")\\n        f.write(f\"Subject: {SUBJECT_ID}\\n\")\\n        f.write(f\"Generated: {datetime.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')}\\n\")\\n        f.write(f\"Project Root: {project_root}\\n\\n\")\\n        \\n        f.write(f\"TIME RANGE:\\n\")\\n        f.write(f\"  Overall Start: {overall_start}\\n\")\\n        f.write(f\"  Overall End: {overall_end}\\n\")\\n        f.write(f\"  Total Duration: {total_duration}\\n\")\\n        f.write(f\"  Pages Generated: {num_pages} ({MINUTES_PER_PAGE} minutes each)\\n\\n\")\\n        \\n        f.write(f\"SYNC EVENTS:\\n\")\\n        f.write(f\"  Sync Start: {sync_start_time}\\n\")\\n        f.write(f\"  Sync End: {sync_end_time}\\n\")\\n        f.write(f\"  Sync Duration: {sync_end_time - sync_start_time}\\n\\n\")\\n        \\n        f.write(f\"SENSORS PROCESSED ({len(processed_sensors)}):\\n\")\\n        for sensor_name, sensor_data in processed_sensors.items():\\n            numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\\n            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\\n            f.write(f\"  üìà {sensor_name}:\\n\")\\n            f.write(f\"    Samples: {len(sensor_data)}\\n\")\\n            f.write(f\"    Duration: {sensor_duration}\\n\")\\n            f.write(f\"    Channels: {len(numeric_cols)} {list(numeric_cols)}\\n\")\\n            f.write(f\"    Time Range: {sensor_data.index.min()} to {sensor_data.index.max()}\\n\")\\n            f.write(f\"    PDF: {sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\')}_complete_data.pdf\\n\\n\")\\n        \\n        if len(full_range_labels) > 0:\\n            f.write(f\"LABELS ({len(full_range_labels)}):\\n\")\\n            label_summary = full_range_labels[\\'Label\\'].value_counts()\\n            for label, count in label_summary.items():\\n                f.write(f\"  üè∑Ô∏è {label}: {count} instances\\n\")\\n            f.write(f\"\\n  Label Time Range: {full_range_labels[\\'Real_Start_Time\\'].min()} to {full_range_labels[\\'Real_End_Time\\'].max()}\\n\")\\n        else:\\n            f.write(f\"LABELS: No labels available\\n\")\\n        \\n        f.write(f\"\\nCONFIGURATION:\\n\")\\n        f.write(f\"  Target Frequency: {TARGET_FREQUENCY}Hz\\n\")\\n        f.write(f\"  Minutes per Page: {MINUTES_PER_PAGE}\\n\")\\n        f.write(f\"  Page Format: {PAGE_WIDTH}x{PAGE_HEIGHT} inches (landscape)\\n\")\\n        f.write(f\"  DPI: {DPI}\\n\")\\n        \\n        # Time corrections applied\\n        f.write(f\"\\nTIME CORRECTIONS APPLIED:\\n\")\\n        subject_correction_params = sync_params.get(SUBJECT_ID, {})\\n        for sensor_name in processed_sensors.keys():\\n            # Find original sensor name\\n            original_sensor_name = sensor_name\\n            for orig_name in raw_data_parsing_config.keys():\\n                if orig_name in sensor_name:\\n                    original_sensor_name = orig_name\\n                    break\\n            \\n            sensor_corr_params = subject_correction_params.get(original_sensor_name, {})\\n            shift_applied = sensor_corr_params.get(\\'shift\\', 0)\\n            unit = sensor_corr_params.get(\\'unit\\', \\'s\\')\\n            \\n            f.write(f\"  {sensor_name}: {shift_applied}{unit} shift\")\\n            \\n            # Check for drift correction\\n            drift_params = sensor_corr_params.get(\\'drift\\')\\n            if drift_params and all(k in drift_params for k in [\\'t0\\', \\'t1\\', \\'drift_secs\\']):\\n                f.write(f\", {drift_params[\\'drift_secs\\']}s drift correction\")\\n            f.write(f\"\\n\")\\n    \\n    print(f\"‚úÖ Summary report saved: {summary_path}\")\\n    \\n    print(f\"\\nüéØ FINAL DATA VISUALIZATION COMPLETE!\")\\n    print(f\"üìÇ Results directory: {subject_results_dir}\")\\n    print(f\"üìÑ PDFs created: {len(processed_sensors)} (one per sensor)\")\\n    print(f\"üìã Pages per PDF: {num_pages} ({MINUTES_PER_PAGE} minutes each)\")\\n    print(f\"‚è±Ô∏è Total time span: {total_duration}\")\\n    print(f\"üè∑Ô∏è Labels included: {len(full_range_labels)}\")\\n    print(f\"\\n‚úÖ Ready for AI model preprocessing!\")\\n    print(f\"\\nüí° Use these PDFs to:\")\\n    print(f\"  ‚Ä¢ Verify data quality across all sensors\")\\n    print(f\"  ‚Ä¢ Check time synchronization accuracy\")\\n    print(f\"  ‚Ä¢ Validate label alignment with sensor data\")\\n    print(f\"  ‚Ä¢ Identify any remaining artifacts or issues\")\\n    print(f\"  ‚Ä¢ Confirm preprocessing parameters\")\\n\\nelse:\\n    print(\"‚ùå No sensor data available for visualization\")\\n    \\nprint(f\"\\nüìÅ Results structure:\")\\nprint(f\"  {subject_results_dir}/\")\\nfor sensor_name in processed_sensors.keys():\\n    clean_name = sensor_name.replace(\\'/\\', \\'_\\').replace(\\' \\', \\'_\\')\\n    print(f\"  ‚îú‚îÄ‚îÄ {clean_name}/\")\\n    print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ {SUBJECT_ID}_{clean_name}_complete_data.pdf\")\\nprint(f\"  ‚îî‚îÄ‚îÄ {SUBJECT_ID}_data_summary.txt\")\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comprehensive visualization export for final data quality check\n",
    "import matplotlib.backends.backend_pdf as backend_pdf\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import math\n",
    "\"\"\"\n",
    "print(\"=== FINAL DATA VISUALIZATION EXPORT ===\")\n",
    "print(\"üéØ Creating comprehensive PDF plots for quality check before AI preprocessing\")\n",
    "\n",
    "# Create results directory structure\n",
    "results_base_dir = os.path.join(project_root, 'stirnimann_r/results')\n",
    "subject_results_dir = os.path.join(results_base_dir, SUBJECT_ID)\n",
    "os.makedirs(subject_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Results directory: {subject_results_dir}\")\n",
    "\n",
    "# Configuration for plotting\n",
    "MINUTES_PER_PAGE = 10  # 10 minutes per page\n",
    "PAGE_WIDTH = 16       # Landscape format\n",
    "PAGE_HEIGHT = 12      # Landscape format\n",
    "DPI = 150            # Good quality for PDFs\n",
    "\n",
    "# Determine overall time range across all sensors\n",
    "print(\"\\nüìä Determining overall time range...\")\n",
    "all_start_times = []\n",
    "all_end_times = []\n",
    "\n",
    "for sensor_name, sensor_data in processed_sensors.items():\n",
    "    all_start_times.append(sensor_data.index.min())\n",
    "    all_end_times.append(sensor_data.index.max())\n",
    "\n",
    "if all_start_times and all_end_times:\n",
    "    overall_start = min(all_start_times)\n",
    "    overall_end = max(all_end_times)\n",
    "    total_duration = overall_end - overall_start\n",
    "    \n",
    "    print(f\"‚è±Ô∏è Overall time range: {overall_start} to {overall_end}\")\n",
    "    print(f\"‚è±Ô∏è Total duration: {total_duration}\")\n",
    "    \n",
    "    # Calculate number of pages needed\n",
    "    total_minutes = total_duration.total_seconds() / 60\n",
    "    num_pages = math.ceil(total_minutes / MINUTES_PER_PAGE)\n",
    "    \n",
    "    print(f\"üìÑ Will create {num_pages} pages ({MINUTES_PER_PAGE} minutes each)\")\n",
    "    \n",
    "    # Prepare labels for the entire time range\n",
    "    if len(valid_labels) > 0:\n",
    "        # Filter labels to the overall time range\n",
    "        full_range_labels = valid_labels[\n",
    "            (valid_labels['Real_Start_Time'] <= overall_end) & \n",
    "            (valid_labels['Real_End_Time'] >= overall_start)\n",
    "        ]\n",
    "        print(f\"üè∑Ô∏è {len(full_range_labels)} labels in full time range\")\n",
    "    else:\n",
    "        full_range_labels = pd.DataFrame()\n",
    "    \n",
    "    # Process each sensor\n",
    "    for sensor_name, sensor_data in processed_sensors.items():\n",
    "        print(f\"\\n--- Processing sensor: {sensor_name} ---\")\n",
    "        \n",
    "        # Create sensor-specific directory\n",
    "        sensor_dir = os.path.join(subject_results_dir, sensor_name.replace('/', '_').replace(' ', '_'))\n",
    "        os.makedirs(sensor_dir, exist_ok=True)\n",
    "        \n",
    "        # Define PDF filename\n",
    "        pdf_filename = f\"{SUBJECT_ID}_{sensor_name.replace('/', '_').replace(' ', '_')}_complete_data.pdf\"\n",
    "        pdf_path = os.path.join(sensor_dir, pdf_filename)\n",
    "        \n",
    "        print(f\"üìÑ Creating PDF: {pdf_path}\")\n",
    "        \n",
    "        # Get numeric columns for this sensor\n",
    "        numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\n",
    "        print(f\"üìà Plotting {len(numeric_cols)} channels: {list(numeric_cols)}\")\n",
    "        \n",
    "        # Create PDF with multiple pages\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            for page_num in range(num_pages):\n",
    "                print(f\"  üìÑ Creating page {page_num + 1}/{num_pages}...\")\n",
    "                \n",
    "                # Calculate time window for this page\n",
    "                page_start = overall_start + pd.Timedelta(minutes=page_num * MINUTES_PER_PAGE)\n",
    "                page_end = overall_start + pd.Timedelta(minutes=(page_num + 1) * MINUTES_PER_PAGE)\n",
    "                \n",
    "                # Don't go beyond the actual data range\n",
    "                page_end = min(page_end, overall_end)\n",
    "                \n",
    "                # Filter sensor data for this page\n",
    "                page_mask = (sensor_data.index >= page_start) & (sensor_data.index <= page_end)\n",
    "                page_data = sensor_data[page_mask]\n",
    "                \n",
    "                # Filter labels for this page\n",
    "                if len(full_range_labels) > 0:\n",
    "                    page_labels = full_range_labels[\n",
    "                        (full_range_labels['Real_Start_Time'] <= page_end) & \n",
    "                        (full_range_labels['Real_End_Time'] >= page_start)\n",
    "                    ]\n",
    "                else:\n",
    "                    page_labels = pd.DataFrame()\n",
    "                \n",
    "                # Create figure\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(PAGE_WIDTH, PAGE_HEIGHT), dpi=DPI)\n",
    "                \n",
    "                # Plot sensor data\n",
    "                if not page_data.empty and len(numeric_cols) > 0:\n",
    "                    for col in numeric_cols:\n",
    "                        ax.plot(page_data.index, page_data[col], \n",
    "                               label=col, alpha=0.8, linewidth=1.5)\n",
    "                    \n",
    "                    # Get y-axis limits for label positioning\n",
    "                    y_min, y_max = ax.get_ylim()\n",
    "                    y_range = y_max - y_min\n",
    "                    \n",
    "                    # Add label shading\n",
    "                    if len(page_labels) > 0:\n",
    "                        label_count = {}\n",
    "                        for _, label_row in page_labels.iterrows():\n",
    "                            label_name = label_row['Label']\n",
    "                            start_time = max(label_row['Real_Start_Time'], page_start)\n",
    "                            end_time = min(label_row['Real_End_Time'], page_end)\n",
    "                            \n",
    "                            if start_time < end_time:  # Valid time range\n",
    "                                color = label_colors.get(label_name, 'gray')\n",
    "                                \n",
    "                                # Count occurrences for legend\n",
    "                                if label_name not in label_count:\n",
    "                                    label_count[label_name] = 0\n",
    "                                label_count[label_name] += 1\n",
    "                                \n",
    "                                # Add shaded region\n",
    "                                ax.axvspan(start_time, end_time, \n",
    "                                         alpha=0.3, \n",
    "                                         color=color,\n",
    "                                         label=f'{label_name}' if label_count[label_name] == 1 else \"\",\n",
    "                                         zorder=0)  # Behind the data\n",
    "                                \n",
    "                                # Add label text for longer labels\n",
    "                                duration = end_time - start_time\n",
    "                                if duration > pd.Timedelta(minutes=1):  # Only show text for labels > 1 minute\n",
    "                                    mid_time = start_time + (end_time - start_time) / 2\n",
    "                                    y_pos = y_max - y_range * 0.05\n",
    "                                    \n",
    "                                    ax.text(mid_time, y_pos, label_name, \n",
    "                                           ha='center', va='top', rotation=0,\n",
    "                                           fontsize=10, alpha=0.9,\n",
    "                                           bbox=dict(boxstyle='round,pad=0.3', \n",
    "                                                   facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    # Add sync event markers if in range\n",
    "                    if page_start <= sync_start_time <= page_end:\n",
    "                        ax.axvline(sync_start_time, color='red', linestyle='--', \n",
    "                                 linewidth=3, alpha=0.9, label='üéØ Sync Start')\n",
    "                    \n",
    "                    if page_start <= sync_end_time <= page_end:\n",
    "                        ax.axvline(sync_end_time, color='darkred', linestyle='--', \n",
    "                                 linewidth=3, alpha=0.9, label='üéØ Sync End')\n",
    "                    \n",
    "                else:\n",
    "                    # No data on this page\n",
    "                    ax.text(0.5, 0.5, f'No {sensor_name} data in this time window', \n",
    "                           ha='center', va='center', transform=ax.transAxes,\n",
    "                           fontsize=16, alpha=0.7)\n",
    "                \n",
    "                # Set time axis limits to exactly match the page window\n",
    "                ax.set_xlim(page_start, page_end)\n",
    "                \n",
    "                # Formatting\n",
    "                page_title = f'{sensor_name} - Page {page_num + 1}/{num_pages}\\n'\n",
    "                page_title += f'Time: {page_start.strftime(\"%Y-%m-%d %H:%M:%S\")} to {page_end.strftime(\"%H:%M:%S\")}'\n",
    "                if len(page_labels) > 0:\n",
    "                    page_title += f' | {len(page_labels)} labels'\n",
    "                \n",
    "                ax.set_title(page_title, fontsize=14, fontweight='bold')\n",
    "                ax.set_xlabel('Time', fontsize=12)\n",
    "                ax.set_ylabel('Value', fontsize=12)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Format time axis\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, MINUTES_PER_PAGE//10)))\n",
    "                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "                \n",
    "                # Legend (only if not too many items)\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                if len(handles) <= 20:  # Reasonable number for legend\n",
    "                    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=10)\n",
    "                \n",
    "                # Add metadata text\n",
    "                metadata_text = f'Subject: {SUBJECT_ID} | Sensor: {sensor_name}\\n'\n",
    "                metadata_text += f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n'\n",
    "                metadata_text += f'Total Duration: {total_duration} | Target Freq: {TARGET_FREQUENCY}Hz'\n",
    "                \n",
    "                fig.text(0.02, 0.02, metadata_text, fontsize=8, alpha=0.7,\n",
    "                        verticalalignment='bottom')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.subplots_adjust(right=0.85, bottom=0.1)\n",
    "                \n",
    "                # Save page to PDF\n",
    "                pdf.savefig(fig, dpi=DPI, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "        \n",
    "        print(f\"‚úÖ PDF saved: {pdf_path}\")\n",
    "        print(f\"   üìÑ {num_pages} pages, {MINUTES_PER_PAGE} minutes each\")\n",
    "        if len(numeric_cols) > 0:\n",
    "            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\n",
    "            print(f\"   ‚è±Ô∏è Sensor duration: {sensor_duration}\")\n",
    "            print(f\"   üìä {len(sensor_data)} samples, {len(numeric_cols)} channels\")\n",
    "    \n",
    "    # Create summary report\n",
    "    print(f\"\\nüìã Creating summary report...\")\n",
    "    summary_path = os.path.join(subject_results_dir, f\"{SUBJECT_ID}_data_summary.txt\")\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(f\"DATA VISUALIZATION SUMMARY REPORT\\n\")\n",
    "        f.write(f\"=================================\\n\\n\")\n",
    "        f.write(f\"Subject: {SUBJECT_ID}\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Project Root: {project_root}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"TIME RANGE:\\n\")\n",
    "        f.write(f\"  Overall Start: {overall_start}\\n\")\n",
    "        f.write(f\"  Overall End: {overall_end}\\n\")\n",
    "        f.write(f\"  Total Duration: {total_duration}\\n\")\n",
    "        f.write(f\"  Pages Generated: {num_pages} ({MINUTES_PER_PAGE} minutes each)\\n\\n\")\n",
    "        \n",
    "        f.write(f\"SYNC EVENTS:\\n\")\n",
    "        f.write(f\"  Sync Start: {sync_start_time}\\n\")\n",
    "        f.write(f\"  Sync End: {sync_end_time}\\n\")\n",
    "        f.write(f\"  Sync Duration: {sync_end_time - sync_start_time}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"SENSORS PROCESSED ({len(processed_sensors)}):\\n\")\n",
    "        for sensor_name, sensor_data in processed_sensors.items():\n",
    "            numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\n",
    "            sensor_duration = sensor_data.index.max() - sensor_data.index.min()\n",
    "            f.write(f\"  üìà {sensor_name}:\\n\")\n",
    "            f.write(f\"    Samples: {len(sensor_data)}\\n\")\n",
    "            f.write(f\"    Duration: {sensor_duration}\\n\")\n",
    "            f.write(f\"    Channels: {len(numeric_cols)} {list(numeric_cols)}\\n\")\n",
    "            f.write(f\"    Time Range: {sensor_data.index.min()} to {sensor_data.index.max()}\\n\")\n",
    "            f.write(f\"    PDF: {sensor_name.replace('/', '_').replace(' ', '_')}_complete_data.pdf\\n\\n\")\n",
    "        \n",
    "        if len(full_range_labels) > 0:\n",
    "            f.write(f\"LABELS ({len(full_range_labels)}):\\n\")\n",
    "            label_summary = full_range_labels['Label'].value_counts()\n",
    "            for label, count in label_summary.items():\n",
    "                f.write(f\"  üè∑Ô∏è {label}: {count} instances\\n\")\n",
    "            f.write(f\"\\n  Label Time Range: {full_range_labels['Real_Start_Time'].min()} to {full_range_labels['Real_End_Time'].max()}\\n\")\n",
    "        else:\n",
    "            f.write(f\"LABELS: No labels available\\n\")\n",
    "        \n",
    "        f.write(f\"\\nCONFIGURATION:\\n\")\n",
    "        f.write(f\"  Target Frequency: {TARGET_FREQUENCY}Hz\\n\")\n",
    "        f.write(f\"  Minutes per Page: {MINUTES_PER_PAGE}\\n\")\n",
    "        f.write(f\"  Page Format: {PAGE_WIDTH}x{PAGE_HEIGHT} inches (landscape)\\n\")\n",
    "        f.write(f\"  DPI: {DPI}\\n\")\n",
    "        \n",
    "        # Time corrections applied\n",
    "        f.write(f\"\\nTIME CORRECTIONS APPLIED:\\n\")\n",
    "        subject_correction_params = sync_params.get(SUBJECT_ID, {})\n",
    "        for sensor_name in processed_sensors.keys():\n",
    "            # Find original sensor name\n",
    "            original_sensor_name = sensor_name\n",
    "            for orig_name in raw_data_parsing_config.keys():\n",
    "                if orig_name in sensor_name:\n",
    "                    original_sensor_name = orig_name\n",
    "                    break\n",
    "            \n",
    "            sensor_corr_params = subject_correction_params.get(original_sensor_name, {})\n",
    "            shift_applied = sensor_corr_params.get('shift', 0)\n",
    "            unit = sensor_corr_params.get('unit', 's')\n",
    "            \n",
    "            f.write(f\"  {sensor_name}: {shift_applied}{unit} shift\")\n",
    "            \n",
    "            # Check for drift correction\n",
    "            drift_params = sensor_corr_params.get('drift')\n",
    "            if drift_params and all(k in drift_params for k in ['t0', 't1', 'drift_secs']):\n",
    "                f.write(f\", {drift_params['drift_secs']}s drift correction\")\n",
    "            f.write(f\"\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Summary report saved: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL DATA VISUALIZATION COMPLETE!\")\n",
    "    print(f\"üìÇ Results directory: {subject_results_dir}\")\n",
    "    print(f\"üìÑ PDFs created: {len(processed_sensors)} (one per sensor)\")\n",
    "    print(f\"üìã Pages per PDF: {num_pages} ({MINUTES_PER_PAGE} minutes each)\")\n",
    "    print(f\"‚è±Ô∏è Total time span: {total_duration}\")\n",
    "    print(f\"üè∑Ô∏è Labels included: {len(full_range_labels)}\")\n",
    "    print(f\"\\n‚úÖ Ready for AI model preprocessing!\")\n",
    "    print(f\"\\nüí° Use these PDFs to:\")\n",
    "    print(f\"  ‚Ä¢ Verify data quality across all sensors\")\n",
    "    print(f\"  ‚Ä¢ Check time synchronization accuracy\")\n",
    "    print(f\"  ‚Ä¢ Validate label alignment with sensor data\")\n",
    "    print(f\"  ‚Ä¢ Identify any remaining artifacts or issues\")\n",
    "    print(f\"  ‚Ä¢ Confirm preprocessing parameters\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No sensor data available for visualization\")\n",
    "    \n",
    "print(f\"\\nüìÅ Results structure:\")\n",
    "print(f\"  {subject_results_dir}/\")\n",
    "for sensor_name in processed_sensors.keys():\n",
    "    clean_name = sensor_name.replace('/', '_').replace(' ', '_')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {clean_name}/\")\n",
    "    print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ {SUBJECT_ID}_{clean_name}_complete_data.pdf\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ {SUBJECT_ID}_data_summary.txt\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c695ac",
   "metadata": {},
   "source": [
    "## 8. Combined Data Table Creation\n",
    "\n",
    "**Purpose**: Create a single synchronized table combining all sensor data with labels using a shared timestamp axis.\n",
    "\n",
    "**Features**:\n",
    "- Unified timestamp axis across all sensors\n",
    "- All sensor channels as separate columns\n",
    "- Label column indicating activity at each timestamp\n",
    "- Empty label cells when no activity is present\n",
    "- Saved as PKL file for efficient loading in AI preprocessing\n",
    "- Perfect synchronization for machine learning workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac4bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Results directory: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036\n",
      "=== COMBINED DATA TABLE CREATION ===\n",
      "üîÑ Creating unified table with synchronized timestamps, all sensors, and labels\n",
      "üìä Processing 7 sensors...\n",
      "\n",
      "üìÖ Step 1: Determining longest time range...\n",
      "  üìà corsano_wrist: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "  üìà cosinuss_ear: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "  üìà mbient_acc: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "  üìà mbient_gyro: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "  üìà vivalnk_acc: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "  üìà sensomative_bottom: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "  üìà corsano_bioz: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "\n",
      "‚è±Ô∏è Longest time range (union of all sensors):\n",
      "  Start: 2024-02-06 08:51:10.002000093\n",
      "  End: 2024-02-08 11:23:35\n",
      "  Duration: 2 days 02:32:24.997999907\n",
      "  ‚ÑπÔ∏è Sensors not covering full range will have NaN values in gaps\n",
      "\n",
      "‚öôÔ∏è Step 2: Creating unified timestamp index at 25Hz...\n",
      "  üìä Created 4548624 timestamps\n",
      "  üïê Frequency: 25Hz\n",
      "  üìè Interval: 0 days 00:00:00.040000017\n",
      "\n",
      "üîÑ Step 3: Resampling and aligning sensor data...\n",
      "  üìà Processing corsano_wrist...\n",
      "    üìä 3 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "    üéØ Coverage in full range: 2024-02-06 09:49:31.996635199 to 2024-02-08 11:23:34.998398304\n",
      "    ‚úÖ 3 channels added\n",
      "    üìä Data coverage: 75.0% (10238844/13645872 samples)\n",
      "    ‚è∞ Time coverage: 98.1% of full range\n",
      "    üï≥Ô∏è 3407028 samples filled with NaN (outside sensor range)\n",
      "  üìà Processing cosinuss_ear...\n",
      "    üìä 3 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "    üéØ Coverage in full range: 2024-02-06 09:57:02.789999962 to 2024-02-07 21:02:45.387000084\n",
      "    ‚úÖ 3 channels added\n",
      "    üìä Data coverage: 30.5% (4164582/13645872 samples)\n",
      "    ‚è∞ Time coverage: 69.4% of full range\n",
      "    üï≥Ô∏è 9481290 samples filled with NaN (outside sensor range)\n",
      "  üìà Processing mbient_acc...\n",
      "    üìä 3 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    üéØ Coverage in full range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    ‚úÖ 3 channels added\n",
      "    üìä Data coverage: 100.0% (13645872/13645872 samples)\n",
      "    ‚è∞ Time coverage: 100.0% of full range\n",
      "  üìà Processing mbient_gyro...\n",
      "    üìä 3 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    üéØ Coverage in full range: 2024-02-06 08:51:10.014046907 to 2024-02-08 11:23:34.995664358\n",
      "    ‚úÖ 3 channels added\n",
      "    üìä Data coverage: 100.0% (13645872/13645872 samples)\n",
      "    ‚è∞ Time coverage: 100.0% of full range\n",
      "  üìà Processing vivalnk_acc...\n",
      "    üìä 3 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "    üéØ Coverage in full range: 2024-02-06 08:51:10.072418928 to 2024-02-08 10:24:46.864835024\n",
      "    ‚úÖ 3 channels added\n",
      "    üìä Data coverage: 53.2% (7260561/13645872 samples)\n",
      "    ‚è∞ Time coverage: 98.1% of full range\n",
      "    üï≥Ô∏è 6385311 samples filled with NaN (outside sensor range)\n",
      "  üìà Processing sensomative_bottom...\n",
      "    üìä 11 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "    üéØ Coverage in full range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:34.997999907\n",
      "    ‚úÖ 11 channels added\n",
      "    üìä Data coverage: 77.4% (38707042/50034864 samples)\n",
      "    ‚è∞ Time coverage: 100.0% of full range\n",
      "    üï≥Ô∏è 11327822 samples filled with NaN (outside sensor range)\n",
      "  üìà Processing corsano_bioz...\n",
      "    üìä 3 numeric columns\n",
      "    üìÖ Sensor range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "    üéØ Coverage in full range: 2024-02-06 09:49:30 to 2024-02-08 11:23:35\n",
      "    ‚úÖ 3 channels added\n",
      "    üìä Data coverage: 75.7% (10333056/13645872 samples)\n",
      "    ‚è∞ Time coverage: 98.1% of full range\n",
      "    üï≥Ô∏è 3312816 samples filled with NaN (outside sensor range)\n",
      "\n",
      "üè∑Ô∏è Step 4: Adding corrected label information...\n",
      "  üìã Processing 258 corrected labels...\n",
      "  üîß Label corrections applied:\n",
      "  üéØ 258 corrected labels in longest time range\n",
      "  ‚úÖ 2328938 timestamps labeled\n",
      "  ‚ö†Ô∏è 20499 overlapping label instances handled\n",
      "  üìä Label coverage: 50.8% (2308439/4548624 timestamps)\n",
      "  üìã Label distribution (top 10):\n",
      "    üè∑Ô∏è dark: 1793777 samples (71751.1s)\n",
      "    üè∑Ô∏è conversation: 184311 samples (7372.4s)\n",
      "    üè∑Ô∏è cycling: 131739 samples (5269.6s)\n",
      "    üè∑Ô∏è reading_newspaper: 46272 samples (1850.9s)\n",
      "    üè∑Ô∏è eating: 42397 samples (1695.9s)\n",
      "    üè∑Ô∏è self_propulsion: 34199 samples (1368.0s)\n",
      "    üè∑Ô∏è eating+conversation: 16874 samples (675.0s)\n",
      "    üè∑Ô∏è sitting_wheelchair: 15350 samples (614.0s)\n",
      "    üè∑Ô∏è toilet_routine: 13474 samples (539.0s)\n",
      "    üè∑Ô∏è assisted_propulsion+using_phone: 3175 samples (127.0s)\n",
      "    ... and 21 more labels\n",
      "\n",
      "üìä Step 5: Data quality summary...\n",
      "  üìã Final combined table:\n",
      "    Timestamps: 4548624\n",
      "    Sensor columns: 29\n",
      "    Label column: 1\n",
      "    Total columns: 30\n",
      "    Time range: 2024-02-06 08:51:10.002000093 to 2024-02-08 11:23:35\n",
      "    Duration: 2 days 02:32:24.997999907\n",
      "    Frequency: 25Hz\n",
      "  üï≥Ô∏è Missing data summary (NaN values where sensors don't cover full range):\n",
      "    corsano_wrist_wrist_acc_x: 1135676 samples (25.0%)\n",
      "    corsano_wrist_wrist_acc_y: 1135676 samples (25.0%)\n",
      "    corsano_wrist_wrist_acc_z: 1135676 samples (25.0%)\n",
      "    cosinuss_ear_ear_acc_x: 3160430 samples (69.5%)\n",
      "    cosinuss_ear_ear_acc_y: 3160430 samples (69.5%)\n",
      "    cosinuss_ear_ear_acc_z: 3160430 samples (69.5%)\n",
      "    vivalnk_acc_vivalnk_acc_x: 2128437 samples (46.8%)\n",
      "    vivalnk_acc_vivalnk_acc_y: 2128437 samples (46.8%)\n",
      "    vivalnk_acc_vivalnk_acc_z: 2128437 samples (46.8%)\n",
      "    sensomative_bottom_bottom_value_1: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_2: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_3: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_4: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_5: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_6: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_7: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_8: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_9: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_10: 1029802 samples (22.6%)\n",
      "    sensomative_bottom_bottom_value_11: 1029802 samples (22.6%)\n",
      "    corsano_bioz_bioz_acc_x: 1104272 samples (24.3%)\n",
      "    corsano_bioz_bioz_acc_y: 1104272 samples (24.3%)\n",
      "    corsano_bioz_bioz_acc_z: 1104272 samples (24.3%)\n",
      "\n",
      "üíæ Step 6: Saving combined data...\n",
      "‚úÖ Combined data saved: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data.pkl\n",
      "  üìÅ File size: 1063.2 MB\n",
      "‚úÖ Metadata saved: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data_metadata.json\n",
      "\n",
      "‚úÖ COMBINED DATA TABLE COMPLETE!\n",
      "üìÇ Saved to: /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data.pkl\n",
      "üìä Shape: (4548624, 30)\n",
      "‚è±Ô∏è Time span: 2 days 02:32:24.997999907\n",
      "üéØ Frequency: 25Hz\n",
      "üìà Sensors: 7\n",
      "üè∑Ô∏è Labels: Yes\n",
      "üï≥Ô∏è NaN handling: Gaps filled for sensors not covering full time range\n",
      "\n",
      "üí° Usage for AI preprocessing:\n",
      "  import pickle\n",
      "  with open('/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/OutSense-036_combined_data.pkl', 'rb') as f:\n",
      "      data = pickle.load(f)\n",
      "  # data is a pandas DataFrame with:\n",
      "  # - Index: synchronized timestamps (longest range)\n",
      "  # - Columns: all sensor channels + 'Label'\n",
      "  # - NaN values where sensors don't cover full range\n",
      "  # - Ready for feature extraction and ML\n",
      "\n",
      "üìã Column structure:\n",
      "  Sensor columns (29):\n",
      "    üìà corsano_wrist: 3 channels (covers 98.1% of time range)\n",
      "    üìà cosinuss_ear: 3 channels (covers 69.4% of time range)\n",
      "    üìà mbient_acc: 3 channels (covers 100.0% of time range)\n",
      "    üìà mbient_gyro: 3 channels (covers 100.0% of time range)\n",
      "    üìà vivalnk_acc: 3 channels (covers 98.1% of time range)\n",
      "    üìà sensomative_bottom: 11 channels (covers 100.0% of time range)\n",
      "    üìà corsano_bioz: 3 channels (covers 98.1% of time range)\n",
      "  üè∑Ô∏è Label column: 'Label' (activity annotations)\n",
      "\n",
      "üëÄ Sample data (first 5 rows):\n",
      "                               corsano_wrist_wrist_acc_x  \\\n",
      "2024-02-06 08:51:10.002000093                        NaN   \n",
      "2024-02-06 08:51:10.042000110                        NaN   \n",
      "2024-02-06 08:51:10.082000127                        NaN   \n",
      "2024-02-06 08:51:10.122000144                        NaN   \n",
      "2024-02-06 08:51:10.162000161                        NaN   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_y  \\\n",
      "2024-02-06 08:51:10.002000093                        NaN   \n",
      "2024-02-06 08:51:10.042000110                        NaN   \n",
      "2024-02-06 08:51:10.082000127                        NaN   \n",
      "2024-02-06 08:51:10.122000144                        NaN   \n",
      "2024-02-06 08:51:10.162000161                        NaN   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_z  \\\n",
      "2024-02-06 08:51:10.002000093                        NaN   \n",
      "2024-02-06 08:51:10.042000110                        NaN   \n",
      "2024-02-06 08:51:10.082000127                        NaN   \n",
      "2024-02-06 08:51:10.122000144                        NaN   \n",
      "2024-02-06 08:51:10.162000161                        NaN   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_x  cosinuss_ear_ear_acc_y  \\\n",
      "2024-02-06 08:51:10.002000093                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.042000110                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.082000127                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.122000144                     NaN                     NaN   \n",
      "2024-02-06 08:51:10.162000161                     NaN                     NaN   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_z  mbient_acc_x_axis_g  \\\n",
      "2024-02-06 08:51:10.002000093                     NaN              -0.0350   \n",
      "2024-02-06 08:51:10.042000110                     NaN              -0.0350   \n",
      "2024-02-06 08:51:10.082000127                     NaN              -0.0360   \n",
      "2024-02-06 08:51:10.122000144                     NaN              -0.0355   \n",
      "2024-02-06 08:51:10.162000161                     NaN              -0.0360   \n",
      "\n",
      "                               mbient_acc_y_axis_g  mbient_acc_z_axis_g  \\\n",
      "2024-02-06 08:51:10.002000093               0.0875               1.0265   \n",
      "2024-02-06 08:51:10.042000110               0.0870               1.0245   \n",
      "2024-02-06 08:51:10.082000127               0.0860               1.0250   \n",
      "2024-02-06 08:51:10.122000144               0.0865               1.0260   \n",
      "2024-02-06 08:51:10.162000161               0.0875               1.0255   \n",
      "\n",
      "                               mbient_gyro_x_axis_dps  ...  \\\n",
      "2024-02-06 08:51:10.002000093                  0.0610  ...   \n",
      "2024-02-06 08:51:10.042000110                 -0.0305  ...   \n",
      "2024-02-06 08:51:10.082000127                 -0.1525  ...   \n",
      "2024-02-06 08:51:10.122000144                 -0.0305  ...   \n",
      "2024-02-06 08:51:10.162000161                 -0.0305  ...   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_6  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_7  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_8  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_9  \\\n",
      "2024-02-06 08:51:10.002000093                                0.0   \n",
      "2024-02-06 08:51:10.042000110                                0.0   \n",
      "2024-02-06 08:51:10.082000127                                0.0   \n",
      "2024-02-06 08:51:10.122000144                                0.0   \n",
      "2024-02-06 08:51:10.162000161                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_10  \\\n",
      "2024-02-06 08:51:10.002000093                                 0.0   \n",
      "2024-02-06 08:51:10.042000110                                 0.0   \n",
      "2024-02-06 08:51:10.082000127                                 0.0   \n",
      "2024-02-06 08:51:10.122000144                                 0.0   \n",
      "2024-02-06 08:51:10.162000161                                 0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_11  \\\n",
      "2024-02-06 08:51:10.002000093                                 0.0   \n",
      "2024-02-06 08:51:10.042000110                                 0.0   \n",
      "2024-02-06 08:51:10.082000127                                 0.0   \n",
      "2024-02-06 08:51:10.122000144                                 0.0   \n",
      "2024-02-06 08:51:10.162000161                                 0.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_x  \\\n",
      "2024-02-06 08:51:10.002000093                      NaN   \n",
      "2024-02-06 08:51:10.042000110                      NaN   \n",
      "2024-02-06 08:51:10.082000127                      NaN   \n",
      "2024-02-06 08:51:10.122000144                      NaN   \n",
      "2024-02-06 08:51:10.162000161                      NaN   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_y  \\\n",
      "2024-02-06 08:51:10.002000093                      NaN   \n",
      "2024-02-06 08:51:10.042000110                      NaN   \n",
      "2024-02-06 08:51:10.082000127                      NaN   \n",
      "2024-02-06 08:51:10.122000144                      NaN   \n",
      "2024-02-06 08:51:10.162000161                      NaN   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_z  Label  \n",
      "2024-02-06 08:51:10.002000093                      NaN         \n",
      "2024-02-06 08:51:10.042000110                      NaN         \n",
      "2024-02-06 08:51:10.082000127                      NaN         \n",
      "2024-02-06 08:51:10.122000144                      NaN         \n",
      "2024-02-06 08:51:10.162000161                      NaN         \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "üè∑Ô∏è Sample labeled data:\n",
      "                               corsano_wrist_wrist_acc_x  \\\n",
      "2024-02-06 10:52:27.005119747                       28.5   \n",
      "2024-02-06 10:52:27.045119764                       24.0   \n",
      "2024-02-06 10:52:27.085119781                       26.0   \n",
      "2024-02-06 10:52:27.125119799                       22.5   \n",
      "2024-02-06 10:52:27.165119816                       23.0   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_y  \\\n",
      "2024-02-06 10:52:27.005119747                     -509.0   \n",
      "2024-02-06 10:52:27.045119764                     -510.0   \n",
      "2024-02-06 10:52:27.085119781                     -510.0   \n",
      "2024-02-06 10:52:27.125119799                     -513.0   \n",
      "2024-02-06 10:52:27.165119816                     -509.0   \n",
      "\n",
      "                               corsano_wrist_wrist_acc_z  \\\n",
      "2024-02-06 10:52:27.005119747                       22.5   \n",
      "2024-02-06 10:52:27.045119764                       18.0   \n",
      "2024-02-06 10:52:27.085119781                       15.0   \n",
      "2024-02-06 10:52:27.125119799                       19.0   \n",
      "2024-02-06 10:52:27.165119816                       22.0   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_x  cosinuss_ear_ear_acc_y  \\\n",
      "2024-02-06 10:52:27.005119747                  0.6660                 -0.7495   \n",
      "2024-02-06 10:52:27.045119764                  0.6670                 -0.7500   \n",
      "2024-02-06 10:52:27.085119781                  0.6665                 -0.7550   \n",
      "2024-02-06 10:52:27.125119799                  0.6635                 -0.7500   \n",
      "2024-02-06 10:52:27.165119816                  0.6675                 -0.7495   \n",
      "\n",
      "                               cosinuss_ear_ear_acc_z  mbient_acc_x_axis_g  \\\n",
      "2024-02-06 10:52:27.005119747                -0.05850               0.2060   \n",
      "2024-02-06 10:52:27.045119764                -0.05950               0.1725   \n",
      "2024-02-06 10:52:27.085119781                -0.06225               0.1665   \n",
      "2024-02-06 10:52:27.125119799                -0.06500              -0.0210   \n",
      "2024-02-06 10:52:27.165119816                -0.06500               0.1995   \n",
      "\n",
      "                               mbient_acc_y_axis_g  mbient_acc_z_axis_g  \\\n",
      "2024-02-06 10:52:27.005119747               0.9430              -0.0885   \n",
      "2024-02-06 10:52:27.045119764               0.9350              -0.1095   \n",
      "2024-02-06 10:52:27.085119781               0.9555              -0.0855   \n",
      "2024-02-06 10:52:27.125119799               1.0560              -0.1350   \n",
      "2024-02-06 10:52:27.165119816               1.2450              -0.0165   \n",
      "\n",
      "                               mbient_gyro_x_axis_dps  ...  \\\n",
      "2024-02-06 10:52:27.005119747                  4.4815  ...   \n",
      "2024-02-06 10:52:27.045119764                  5.0305  ...   \n",
      "2024-02-06 10:52:27.085119781                  4.6340  ...   \n",
      "2024-02-06 10:52:27.125119799                  6.3110  ...   \n",
      "2024-02-06 10:52:27.165119816                  5.2135  ...   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_6  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_7  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_8  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_9  \\\n",
      "2024-02-06 10:52:27.005119747                                0.0   \n",
      "2024-02-06 10:52:27.045119764                                0.0   \n",
      "2024-02-06 10:52:27.085119781                                0.0   \n",
      "2024-02-06 10:52:27.125119799                                0.0   \n",
      "2024-02-06 10:52:27.165119816                                0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_10  \\\n",
      "2024-02-06 10:52:27.005119747                                 0.0   \n",
      "2024-02-06 10:52:27.045119764                                 0.0   \n",
      "2024-02-06 10:52:27.085119781                                 0.0   \n",
      "2024-02-06 10:52:27.125119799                                 0.0   \n",
      "2024-02-06 10:52:27.165119816                                 0.0   \n",
      "\n",
      "                               sensomative_bottom_bottom_value_11  \\\n",
      "2024-02-06 10:52:27.005119747                                 0.0   \n",
      "2024-02-06 10:52:27.045119764                                 0.0   \n",
      "2024-02-06 10:52:27.085119781                                 0.0   \n",
      "2024-02-06 10:52:27.125119799                                 0.0   \n",
      "2024-02-06 10:52:27.165119816                                 0.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_x  \\\n",
      "2024-02-06 10:52:27.005119747                    -37.5   \n",
      "2024-02-06 10:52:27.045119764                   -114.0   \n",
      "2024-02-06 10:52:27.085119781                   -126.0   \n",
      "2024-02-06 10:52:27.125119799                   -177.5   \n",
      "2024-02-06 10:52:27.165119816                   -221.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_y  \\\n",
      "2024-02-06 10:52:27.005119747                   -569.0   \n",
      "2024-02-06 10:52:27.045119764                   -629.0   \n",
      "2024-02-06 10:52:27.085119781                   -599.0   \n",
      "2024-02-06 10:52:27.125119799                   -563.0   \n",
      "2024-02-06 10:52:27.165119816                   -493.0   \n",
      "\n",
      "                               corsano_bioz_bioz_acc_z                  Label  \n",
      "2024-02-06 10:52:27.005119747                    101.0  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.045119764                    263.0  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.085119781                    311.0  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.125119799                    202.5  sit_bed_to_wheelchair  \n",
      "2024-02-06 10:52:27.165119816                    211.0  sit_bed_to_wheelchair  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "üìÅ Final results structure:\n",
      "  /scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/src/stirnimann_r/results/OutSense-036/\n",
      "  ‚îú‚îÄ‚îÄ corsano_wrist/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_corsano_wrist_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ cosinuss_ear/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_cosinuss_ear_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ mbient_acc/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_mbient_acc_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ mbient_gyro/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_mbient_gyro_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ vivalnk_acc/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_vivalnk_acc_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ sensomative_bottom/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_sensomative_bottom_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ corsano_bioz/\n",
      "  ‚îÇ   ‚îî‚îÄ‚îÄ OutSense-036_corsano_bioz_complete_data.pdf\n",
      "  ‚îú‚îÄ‚îÄ OutSense-036_combined_data.pkl\n",
      "  ‚îú‚îÄ‚îÄ OutSense-036_combined_data_metadata.json\n",
      "  ‚îî‚îÄ‚îÄ OutSense-036_data_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Create combined synchronized data table with all sensors and labels\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Create results directory structure\n",
    "results_base_dir = '/scai_data3/scratch/stirnimann_r/Master Thesis Repo Ricardo/results/pipeline/models'\n",
    "subject_results_dir = os.path.join(results_base_dir, SUBJECT_ID)\n",
    "os.makedirs(subject_results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Results directory: {subject_results_dir}\")\n",
    "print(\"=== COMBINED DATA TABLE CREATION ===\")\n",
    "print(\"üîÑ Creating unified table with synchronized timestamps, all sensors, and labels\")\n",
    "\n",
    "if not processed_sensors:\n",
    "    print(\"‚ùå No processed sensor data available\")\n",
    "else:\n",
    "    print(f\"üìä Processing {len(processed_sensors)} sensors...\")\n",
    "    \n",
    "    # Step 1: Determine the longest time range across all sensors\n",
    "    print(\"\\nüìÖ Step 1: Determining longest time range...\")\n",
    "    \n",
    "    all_start_times = []\n",
    "    all_end_times = []\n",
    "    \n",
    "    for sensor_name, sensor_data in processed_sensors.items():\n",
    "        all_start_times.append(sensor_data.index.min())\n",
    "        all_end_times.append(sensor_data.index.max())\n",
    "        print(f\"  üìà {sensor_name}: {sensor_data.index.min()} to {sensor_data.index.max()}\")\n",
    "    \n",
    "    # Use the union of all sensor time ranges (longest possible range)\n",
    "    common_start = min(all_start_times)  # Earliest start time\n",
    "    common_end = max(all_end_times)      # Latest end time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Longest time range (union of all sensors):\")\n",
    "    print(f\"  Start: {common_start}\")\n",
    "    print(f\"  End: {common_end}\")\n",
    "    print(f\"  Duration: {common_end - common_start}\")\n",
    "    print(f\"  ‚ÑπÔ∏è Sensors not covering full range will have NaN values in gaps\")\n",
    "    \n",
    "    if common_start >= common_end:\n",
    "        print(\"‚ùå Invalid time range found!\")\n",
    "    else:\n",
    "        # Step 2: Create unified timestamp index\n",
    "        print(f\"\\n‚öôÔ∏è Step 2: Creating unified timestamp index at {TARGET_FREQUENCY}Hz...\")\n",
    "        \n",
    "        # Create regular timestamp index at target frequency\n",
    "        total_seconds = (common_end - common_start).total_seconds()\n",
    "        num_samples = int(total_seconds * TARGET_FREQUENCY)\n",
    "        \n",
    "        # Create timestamp index\n",
    "        timestamp_index = pd.date_range(\n",
    "            start=common_start,\n",
    "            end=common_end,\n",
    "            periods=num_samples\n",
    "        )\n",
    "        \n",
    "        print(f\"  üìä Created {len(timestamp_index)} timestamps\")\n",
    "        print(f\"  üïê Frequency: {TARGET_FREQUENCY}Hz\")\n",
    "        print(f\"  üìè Interval: {timestamp_index[1] - timestamp_index[0]}\")\n",
    "        \n",
    "        # Step 3: Resample and align all sensor data\n",
    "        print(f\"\\nüîÑ Step 3: Resampling and aligning sensor data...\")\n",
    "        \n",
    "        combined_data = pd.DataFrame(index=timestamp_index)\n",
    "        sensor_stats = {}\n",
    "        \n",
    "        for sensor_name, sensor_data in processed_sensors.items():\n",
    "            print(f\"  üìà Processing {sensor_name}...\")\n",
    "            \n",
    "            # Get numeric columns\n",
    "            numeric_cols = sensor_data.select_dtypes(include=[np.number]).columns\n",
    "            print(f\"    üìä {len(numeric_cols)} numeric columns\")\n",
    "            \n",
    "            # Check coverage within the longest time range\n",
    "            sensor_start = sensor_data.index.min()\n",
    "            sensor_end = sensor_data.index.max()\n",
    "            coverage_start = max(sensor_start, common_start)\n",
    "            coverage_end = min(sensor_end, common_end)\n",
    "            \n",
    "            print(f\"    üìÖ Sensor range: {sensor_start} to {sensor_end}\")\n",
    "            print(f\"    üéØ Coverage in full range: {coverage_start} to {coverage_end}\")\n",
    "            \n",
    "            # Process the sensor data (no filtering to common range)\n",
    "            sensor_data_clean = sensor_data.copy()\n",
    "            \n",
    "            # First ensure monotonic index (remove any duplicates)\n",
    "            sensor_data_clean = sensor_data_clean[~sensor_data_clean.index.duplicated(keep='first')]\n",
    "            sensor_data_clean = sensor_data_clean.sort_index()\n",
    "            \n",
    "            # Resample using linear interpolation\n",
    "            resampled_data = sensor_data_clean[numeric_cols].resample(f'{1000//TARGET_FREQUENCY}ms').mean()\n",
    "            \n",
    "            # Interpolate to fill gaps within sensor's own time range\n",
    "            resampled_data = resampled_data.interpolate(method='linear', limit=TARGET_FREQUENCY*2)\n",
    "            \n",
    "            # Align to our full timestamp index (this will create NaN for times outside sensor range)\n",
    "            aligned_data = resampled_data.reindex(timestamp_index, method='nearest', tolerance=pd.Timedelta(f'{2000//TARGET_FREQUENCY}ms'))\n",
    "            \n",
    "            # Add sensor prefix to column names to avoid conflicts\n",
    "            sensor_prefix = sensor_name.replace(' ', '_').replace('/', '_')\n",
    "            aligned_data.columns = [f\"{sensor_prefix}_{col}\" for col in aligned_data.columns]\n",
    "            \n",
    "            # Add to combined dataframe\n",
    "            for col in aligned_data.columns:\n",
    "                combined_data[col] = aligned_data[col]\n",
    "            \n",
    "            # Statistics\n",
    "            valid_samples = aligned_data.notna().sum().sum()\n",
    "            total_samples = len(aligned_data) * len(aligned_data.columns)\n",
    "            coverage = valid_samples / total_samples * 100 if total_samples > 0 else 0\n",
    "            \n",
    "            # Calculate time coverage\n",
    "            time_coverage_start = max(sensor_start, common_start)\n",
    "            time_coverage_end = min(sensor_end, common_end)\n",
    "            time_coverage_duration = time_coverage_end - time_coverage_start\n",
    "            full_duration = common_end - common_start\n",
    "            time_coverage_percent = (time_coverage_duration.total_seconds() / full_duration.total_seconds()) * 100\n",
    "            \n",
    "            sensor_stats[sensor_name] = {\n",
    "                'original_samples': len(sensor_data_clean),\n",
    "                'resampled_samples': len(aligned_data),\n",
    "                'channels': len(aligned_data.columns),\n",
    "                'coverage_percent': coverage,\n",
    "                'missing_samples': total_samples - valid_samples,\n",
    "                'time_coverage_percent': time_coverage_percent,\n",
    "                'sensor_start': sensor_start.isoformat(),\n",
    "                'sensor_end': sensor_end.isoformat(),\n",
    "                'gaps_filled_with_nan': total_samples - valid_samples\n",
    "            }\n",
    "            \n",
    "            print(f\"    ‚úÖ {len(aligned_data.columns)} channels added\")\n",
    "            print(f\"    üìä Data coverage: {coverage:.1f}% ({valid_samples}/{total_samples} samples)\")\n",
    "            print(f\"    ‚è∞ Time coverage: {time_coverage_percent:.1f}% of full range\")\n",
    "            if total_samples - valid_samples > 0:\n",
    "                print(f\"    üï≥Ô∏è {total_samples - valid_samples} samples filled with NaN (outside sensor range)\")\n",
    "        \n",
    "        # Step 4: Add corrected label information\n",
    "        print(f\"\\nüè∑Ô∏è Step 4: Adding corrected label information...\")\n",
    "        \n",
    "        # Initialize label column\n",
    "        combined_data['Label'] = ''\n",
    "        \n",
    "        # Use the globally corrected labels if available, otherwise use original\n",
    "        labels_to_use = corrected_labels_global if len(corrected_labels_global) > 0 else valid_labels\n",
    "        \n",
    "        if len(labels_to_use) > 0:\n",
    "            print(f\"  üìã Processing {len(labels_to_use)} corrected labels...\")\n",
    "            \n",
    "            # Show correction summary\n",
    "            if len(corrected_labels_global) > 0 and current_correction_log:\n",
    "                print(f\"  üîß Label corrections applied:\")\n",
    "                if current_correction_log.get('manual_offset_applied', 0) != 0:\n",
    "                    print(f\"    Manual offset: {current_correction_log['manual_offset_applied']}s\")\n",
    "                if current_correction_log.get('linear_drift_applied', False):\n",
    "                    print(f\"    Linear drift correction: enabled\")\n",
    "            \n",
    "            # Filter labels to longest time range\n",
    "            common_labels = labels_to_use[\n",
    "                (labels_to_use['Real_Start_Time'] <= common_end) & \n",
    "                (labels_to_use['Real_End_Time'] >= common_start)\n",
    "            ].copy()\n",
    "            \n",
    "            print(f\"  üéØ {len(common_labels)} corrected labels in longest time range\")\n",
    "            \n",
    "            # For each label, mark the corresponding timestamps\n",
    "            label_count = 0\n",
    "            overlap_count = 0\n",
    "            \n",
    "            for _, label_row in common_labels.iterrows():\n",
    "                start_time = max(label_row['Real_Start_Time'], common_start)\n",
    "                end_time = min(label_row['Real_End_Time'], common_end)\n",
    "                label_name = label_row['Label']\n",
    "                \n",
    "                if start_time < end_time:\n",
    "                    # Find timestamps within this label period\n",
    "                    mask = (combined_data.index >= start_time) & (combined_data.index <= end_time)\n",
    "                    matching_timestamps = combined_data.index[mask]\n",
    "                    \n",
    "                    if len(matching_timestamps) > 0:\n",
    "                        # Check for overlapping labels\n",
    "                        existing_labels = combined_data.loc[mask, 'Label']\n",
    "                        overlaps = existing_labels[existing_labels != '']\n",
    "                        \n",
    "                        if len(overlaps) > 0:\n",
    "                            overlap_count += len(overlaps)\n",
    "                            # For overlapping labels, create combined label\n",
    "                            for idx in matching_timestamps:\n",
    "                                existing = combined_data.loc[idx, 'Label']\n",
    "                                if existing == '':\n",
    "                                    combined_data.loc[idx, 'Label'] = label_name\n",
    "                                elif label_name not in existing:\n",
    "                                    combined_data.loc[idx, 'Label'] = f\"{existing}+{label_name}\"\n",
    "                        else:\n",
    "                            # No overlap, assign label directly\n",
    "                            combined_data.loc[mask, 'Label'] = label_name\n",
    "                        \n",
    "                        label_count += len(matching_timestamps)\n",
    "            \n",
    "            print(f\"  ‚úÖ {label_count} timestamps labeled\")\n",
    "            print(f\"  ‚ö†Ô∏è {overlap_count} overlapping label instances handled\")\n",
    "            \n",
    "            # Label statistics\n",
    "            labeled_count = (combined_data['Label'] != '').sum()\n",
    "            total_count = len(combined_data)\n",
    "            label_coverage = labeled_count / total_count * 100\n",
    "            \n",
    "            print(f\"  üìä Label coverage: {label_coverage:.1f}% ({labeled_count}/{total_count} timestamps)\")\n",
    "            \n",
    "            # Show label distribution\n",
    "            label_dist = combined_data['Label'].value_counts()\n",
    "            non_empty_labels = label_dist[label_dist.index != '']\n",
    "            \n",
    "            print(f\"  üìã Label distribution (top 10):\")\n",
    "            for label, count in non_empty_labels.head(10).items():\n",
    "                duration_seconds = count / TARGET_FREQUENCY\n",
    "                print(f\"    üè∑Ô∏è {label}: {count} samples ({duration_seconds:.1f}s)\")\n",
    "            \n",
    "            if len(non_empty_labels) > 10:\n",
    "                print(f\"    ... and {len(non_empty_labels) - 10} more labels\")\n",
    "        \n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è No labels available - Label column will remain empty\")\n",
    "        \n",
    "        # Step 5: Data quality summary\n",
    "        print(f\"\\nüìä Step 5: Data quality summary...\")\n",
    "        \n",
    "        total_sensor_cols = len([col for col in combined_data.columns if col != 'Label'])\n",
    "        total_samples = len(combined_data)\n",
    "        \n",
    "        print(f\"  üìã Final combined table:\")\n",
    "        print(f\"    Timestamps: {total_samples}\")\n",
    "        print(f\"    Sensor columns: {total_sensor_cols}\")\n",
    "        print(f\"    Label column: 1\")\n",
    "        print(f\"    Total columns: {len(combined_data.columns)}\")\n",
    "        print(f\"    Time range: {combined_data.index.min()} to {combined_data.index.max()}\")\n",
    "        print(f\"    Duration: {combined_data.index.max() - combined_data.index.min()}\")\n",
    "        print(f\"    Frequency: {TARGET_FREQUENCY}Hz\")\n",
    "        \n",
    "        # Missing data analysis\n",
    "        missing_data = combined_data.select_dtypes(include=[np.number]).isnull().sum()\n",
    "        if missing_data.sum() > 0:\n",
    "            print(f\"  üï≥Ô∏è Missing data summary (NaN values where sensors don't cover full range):\")\n",
    "            for col, missing_count in missing_data[missing_data > 0].items():\n",
    "                missing_pct = missing_count / total_samples * 100\n",
    "                print(f\"    {col}: {missing_count} samples ({missing_pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ No missing data in sensor columns\")\n",
    "        \n",
    "        # Step 6: Save combined data\n",
    "        print(f\"\\nüíæ Step 6: Saving combined data...\")\n",
    "        \n",
    "        # Define save path\n",
    "        pkl_filename = f\"{SUBJECT_ID}_combined_data.pkl\"\n",
    "        pkl_path = os.path.join(subject_results_dir, pkl_filename)\n",
    "        \n",
    "        # Save as pickle file\n",
    "        try:\n",
    "            with open(pkl_path, 'wb') as f:\n",
    "                pickle.dump(combined_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            print(f\"‚úÖ Combined data saved: {pkl_path}\")\n",
    "            \n",
    "            # File size info\n",
    "            file_size = os.path.getsize(pkl_path)\n",
    "            file_size_mb = file_size / (1024 * 1024)\n",
    "            print(f\"  üìÅ File size: {file_size_mb:.1f} MB\")\n",
    "            \n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'subject_id': SUBJECT_ID,\n",
    "                'creation_time': datetime.now().isoformat(),\n",
    "                'time_range': {\n",
    "                    'start': combined_data.index.min().isoformat(),\n",
    "                    'end': combined_data.index.max().isoformat(),\n",
    "                    'duration_seconds': (combined_data.index.max() - combined_data.index.min()).total_seconds(),\n",
    "                    'range_type': 'longest_union'  # Indicates this uses the longest range\n",
    "                },\n",
    "                'sampling': {\n",
    "                    'target_frequency_hz': TARGET_FREQUENCY,\n",
    "                    'total_samples': total_samples,\n",
    "                    'actual_frequency_hz': total_samples / (combined_data.index.max() - combined_data.index.min()).total_seconds()\n",
    "                },\n",
    "                'sensors': sensor_stats,\n",
    "                'columns': {\n",
    "                    'total': len(combined_data.columns),\n",
    "                    'sensor_channels': total_sensor_cols,\n",
    "                    'label_column': 1,\n",
    "                    'column_names': list(combined_data.columns)\n",
    "                },\n",
    "                'labels': {\n",
    "                    'total_labels_available': len(valid_labels),\n",
    "                    'labels_in_timerange': len(common_labels) if len(valid_labels) > 0 else 0,\n",
    "                    'labeled_timestamps': labeled_count if len(valid_labels) > 0 else 0,\n",
    "                    'label_coverage_percent': label_coverage if len(valid_labels) > 0 else 0,\n",
    "                    'unique_labels': list(non_empty_labels.index) if len(valid_labels) > 0 else []\n",
    "                },\n",
    "                'data_quality': {\n",
    "                    'missing_values': missing_data.to_dict(),\n",
    "                    'coverage_by_sensor': {name: stats['coverage_percent'] for name, stats in sensor_stats.items()},\n",
    "                    'time_coverage_by_sensor': {name: stats['time_coverage_percent'] for name, stats in sensor_stats.items()}\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save metadata as JSON\n",
    "            metadata_path = os.path.join(subject_results_dir, f\"{SUBJECT_ID}_combined_data_metadata.json\")\n",
    "            import json\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2, default=str)\n",
    "            \n",
    "            print(f\"‚úÖ Metadata saved: {metadata_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving combined data: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # Step 7: Verification and summary\n",
    "        print(f\"\\n‚úÖ COMBINED DATA TABLE COMPLETE!\")\n",
    "        print(f\"üìÇ Saved to: {pkl_path}\")\n",
    "        print(f\"üìä Shape: {combined_data.shape}\")\n",
    "        print(f\"‚è±Ô∏è Time span: {combined_data.index.max() - combined_data.index.min()}\")\n",
    "        print(f\"üéØ Frequency: {TARGET_FREQUENCY}Hz\")\n",
    "        print(f\"üìà Sensors: {len(processed_sensors)}\")\n",
    "        print(f\"üè∑Ô∏è Labels: {'Yes' if len(valid_labels) > 0 else 'No'}\")\n",
    "        print(f\"üï≥Ô∏è NaN handling: Gaps filled for sensors not covering full time range\")\n",
    "        \n",
    "        print(f\"\\nüí° Usage for AI preprocessing:\")\n",
    "        print(f\"  import pickle\")\n",
    "        print(f\"  with open('{pkl_path}', 'rb') as f:\")\n",
    "        print(f\"      data = pickle.load(f)\")\n",
    "        print(f\"  # data is a pandas DataFrame with:\")\n",
    "        print(f\"  # - Index: synchronized timestamps (longest range)\")\n",
    "        print(f\"  # - Columns: all sensor channels + 'Label'\")\n",
    "        print(f\"  # - NaN values where sensors don't cover full range\")\n",
    "        print(f\"  # - Ready for feature extraction and ML\")\n",
    "        \n",
    "        print(f\"\\nüìã Column structure:\")\n",
    "        sensor_cols = [col for col in combined_data.columns if col != 'Label']\n",
    "        print(f\"  Sensor columns ({len(sensor_cols)}):\")\n",
    "        for sensor_name in processed_sensors.keys():\n",
    "            sensor_prefix = sensor_name.replace(' ', '_').replace('/', '_')\n",
    "            matching_cols = [col for col in sensor_cols if col.startswith(sensor_prefix)]\n",
    "            time_cov = sensor_stats[sensor_name]['time_coverage_percent']\n",
    "            print(f\"    üìà {sensor_name}: {len(matching_cols)} channels (covers {time_cov:.1f}% of time range)\")\n",
    "        print(f\"  üè∑Ô∏è Label column: 'Label' (activity annotations)\")\n",
    "        \n",
    "        # Show sample of the data\n",
    "        print(f\"\\nüëÄ Sample data (first 5 rows):\")\n",
    "        print(combined_data.head())\n",
    "        \n",
    "        if len(valid_labels) > 0:\n",
    "            # Show some labeled samples\n",
    "            labeled_samples = combined_data[combined_data['Label'] != '']\n",
    "            if len(labeled_samples) > 0:\n",
    "                print(f\"\\nüè∑Ô∏è Sample labeled data:\")\n",
    "                print(labeled_samples.head())\n",
    "\n",
    "print(f\"\\nüìÅ Final results structure:\")\n",
    "print(f\"  {subject_results_dir}/\")\n",
    "for sensor_name in processed_sensors.keys():\n",
    "    clean_name = sensor_name.replace('/', '_').replace(' ', '_')\n",
    "    print(f\"  ‚îú‚îÄ‚îÄ {clean_name}/\")\n",
    "    print(f\"  ‚îÇ   ‚îî‚îÄ‚îÄ {SUBJECT_ID}_{clean_name}_complete_data.pdf\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ {SUBJECT_ID}_combined_data.pkl\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ {SUBJECT_ID}_combined_data_metadata.json\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ {SUBJECT_ID}_data_summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
