# src/main_pipeline.py

import os
import sys
import argparse
import logging
import time
from datetime import datetime

# Add src directory to Python path to allow importing modules from src
# This might be needed if running the script directly from the project root
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(script_dir, '..'))
if script_dir not in sys.path:
    sys.path.insert(0, script_dir)
if project_root not in sys.path:
     sys.path.insert(0, project_root)


# Import necessary modules from the src directory
try:
    import config_loader
    import utils
    import raw_data_processor
    import data_loader
    import feature_engineering
    import feature_engineering_tabpfn
    import feature_selector
    import data_preparation
    import models # Needed indirectly by training/evaluation to load model class
    import training
    import evaluation
except ImportError as e:
     print(f"Error importing pipeline modules: {e}")
     print("Ensure all .py files (config_loader, utils, data_loader, etc.) are in the 'src' directory or accessible via PYTHONPATH.")
     sys.exit(1)


def main():
    """Main function to run the ML pipeline stages."""
    parser = argparse.ArgumentParser(description="Run stages of the ADL Classification Pipeline.")
    parser.add_argument('--config', type=str, default='config.yaml',
                        help='Path to the YAML configuration file (default: config.yaml in project root).')
    parser.add_argument('--start-stage', type=str, default='raw_data_processing',
                        choices=['raw_data_processing', 'load_data', 'feature_engineering', 'feature_selection', 'data_preparation', 'training', 'evaluation'],
                        help='The stage to start the pipeline from.')
    parser.add_argument('--stop-stage', type=str, default='evaluation',
                        choices=['raw_data_processing', 'load_data', 'feature_engineering', 'feature_selection', 'data_preparation', 'training', 'evaluation'],
                        help='The stage to stop the pipeline after.')
    parser.add_argument('--log-level', type=str, default='INFO',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                        help='Set the logging level.')
    # Example for forcing: --force-run feature_selection training
    parser.add_argument('--force-run', nargs='*', default=[],
                        choices=['raw_data_processing', 'load_data', 'feature_engineering', 'feature_selection', 'data_preparation', 'training', 'evaluation'],
                        help='Force specific stages to run even if outputs exist.')

    args = parser.parse_args()

    # --- Basic Setup ---
    try:
        # Construct absolute path to config file if relative path is given
        config_path = args.config
        if not os.path.isabs(config_path):
            config_path = os.path.join(project_root, config_path)

        cfg = config_loader.load_config(config_path)

        results_dir = os.path.join(project_root, cfg.get('results_dir', 'results'))
        os.makedirs(results_dir, exist_ok=True) # Ensure results dir exists

        # Setup Logging
        log_filename = cfg.get('base_log_filename', 'pipeline.log')
        log_filepath = os.path.join(results_dir, log_filename)
        log_level = getattr(logging, args.log_level.upper(), logging.INFO)
        utils.setup_logging(log_filepath, level=log_level)

        # Set Seed
        utils.set_seed(cfg.get('seed_number', 42))

        logging.info(f"Pipeline started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logging.info(f"Using config file: {config_path}")
        logging.info(f"Running stages from '{args.start_stage}' up to '{args.stop_stage}'.")
        if args.force_run:
            logging.warning(f"Forcing re-run of stages: {args.force_run}")

    except Exception as e:
        # Catch errors during basic setup before logging might be fully configured
        print(f"FATAL ERROR during setup: {e}")
        logging.error(f"FATAL ERROR during setup: {e}", exc_info=True)
        sys.exit(1)

    # --- Define Pipeline Stages and Outputs ---
    # This helps manage dependencies and skipping logic
    # Keys are stage names matching argparse choices
    pipeline_stages = [
        'raw_data_processing',
        'load_data',
        'feature_engineering',
        'feature_selection',
        'data_preparation',
        'training',
        'evaluation'
    ]
    # Store paths to outputs generated by each stage
    stage_outputs = {}

    # Convert start/stop stages to indices for easier comparison
    try:
        start_index = pipeline_stages.index(args.start_stage)
        stop_index = pipeline_stages.index(args.stop_stage)
    except ValueError:
        logging.error("Invalid start or stop stage provided.")
        sys.exit(1)

    # --- Execute Pipeline ---
    pipeline_successful = True
    overall_start_time = time.time()

    for i, stage_name in enumerate(pipeline_stages):
        # --- Stage Skipping Logic ---
        if i < start_index:
            logging.info(f"Skipping stage '{stage_name}' (starts at '{args.start_stage}').")
            continue
        if i > stop_index:
            logging.info(f"Stopping pipeline before stage '{stage_name}' (stops after '{args.stop_stage}').")
            break

        force_this_stage = stage_name in args.force_run
        stage_start_time = time.time()
        logging.info(f"\n===== Starting Stage: {stage_name.upper()} =====")

        try:
            # --- Stage Execution ---
            if stage_name == 'raw_data_processing': # <<< ADDED STAGE EXECUTION
                output_dir_expected = os.path.join(project_root, cfg.get('processed_data_output_dir', 'processed_subjects'))
                # Simple check: does the directory exist? A more robust check might look for specific files or metadata.
                output_exists = os.path.isdir(output_dir_expected)

                if not force_this_stage and output_exists:
                     logging.warning(f"Output directory exists for '{stage_name}': {output_dir_expected}. Skipping execution.")
                     stage_outputs['processed_subjects_dir'] = output_dir_expected # Store expected path
                else:
                     processed_dir_path = raw_data_processor.run_raw_processing(cfg, project_root)
                     stage_outputs['processed_subjects_dir'] = processed_dir_path

            elif stage_name == 'load_data':
                # <<< MODIFIED: Input dir comes from previous stage output or config >>>
                input_dir = stage_outputs.get('processed_subjects_dir') or os.path.join(project_root, cfg.get('cleaned_data_input_dir', 'processed_subjects'))
                if not os.path.isdir(input_dir):
                     logging.error(f"Cannot run '{stage_name}': Input directory missing: {input_dir}. Run 'raw_data_processing' first.")
                     raise FileNotFoundError(f"Input directory for 'load_data' stage not found: {input_dir}")

                output_df_path = os.path.join(project_root, cfg.get('intermediate_feature_dir', 'features'), 'combined_cleaned_data.pkl')
                output_cols_path = os.path.join(project_root, cfg.get('intermediate_feature_dir', 'features'), 'original_feature_names.pkl') # Path for saved cols

                if not force_this_stage and os.path.exists(output_df_path) and os.path.exists(output_cols_path):
                     logging.warning(f"Outputs exist for '{stage_name}'. Skipping execution.")
                     stage_outputs['processed_dataframe_path'] = output_df_path
                     # Load the saved columns if skipping
                     stage_outputs['original_feature_columns'] = utils.load_pickle(output_cols_path)
                else:
                     # Modify data_loader.run_loading_and_preprocessing to accept input_dir and save columns
                     saved_path, actual_cols = data_loader.run_loading_and_preprocessing(cfg, input_dir=input_dir) # Pass input_dir
                     stage_outputs['processed_dataframe_path'] = saved_path
                     stage_outputs['original_feature_columns'] = actual_cols
                     # Save the columns list for potential skipping later
                     utils.save_pickle(actual_cols, output_cols_path)


            elif stage_name == 'feature_engineering':
                if 'processed_dataframe_path' not in stage_outputs or not os.path.exists(stage_outputs['processed_dataframe_path']):
                     logging.error(f"Cannot run '{stage_name}': Input DataFrame path missing or file not found.")
                     raise FileNotFoundError("Input for feature engineering not available.")
                if 'original_feature_columns' not in stage_outputs:
                     logging.error("Original feature columns list not found in stage outputs. Cannot proceed.")
                     raise ValueError("Missing original feature columns list.")

                # Determine which feature engineering script to use
                use_tabpfn_version = cfg.get('use_tabpfn_feature_engineering', False)
                feature_dir = os.path.join(project_root, cfg.get('intermediate_feature_dir', 'features'))
                
                if use_tabpfn_version:
                    logging.info("Using TabPFN-specific feature engineering script...")
                    # Define expected outputs for TabPFN version with different names
                    expected_outputs = {
                        'tabular_dataframe_for_tabpfn': os.path.join(feature_dir, 'tabular_features_for_tabpfn.pkl'),
                        'X_windows_raw': os.path.join(feature_dir, 'X_windows_raw_tabpfn.npy'),
                        'engineered_features': os.path.join(feature_dir, 'engineered_features_tabpfn.npy'),
                        'y_windows': os.path.join(feature_dir, 'y_windows_tabpfn.npy'),
                        'subject_ids_windows': os.path.join(feature_dir, 'subject_ids_windows_tabpfn.npy'),
                        'engineered_feature_names': os.path.join(feature_dir, 'engineered_feature_names_tabpfn.pkl'),
                        'original_feature_names': os.path.join(feature_dir, 'original_feature_names_tabpfn.pkl'),
                        'window_start_times': os.path.join(feature_dir, 'window_start_times_tabpfn.npy'),
                        'window_end_times': os.path.join(feature_dir, 'window_end_times_tabpfn.npy')
                    }
                    
                    # Check if TabPFN outputs exist (primary check is the tabular dataframe)
                    primary_output = expected_outputs['tabular_dataframe_for_tabpfn']
                    outputs_exist = os.path.exists(primary_output)
                    
                    if not force_this_stage and outputs_exist:
                        logging.warning(f"TabPFN output exists for '{stage_name}': {primary_output}. Skipping execution.")
                        stage_outputs.update(expected_outputs)
                    else:
                        # Modify config to use TabPFN-specific output names
                        cfg_tabpfn = cfg.copy()
                        cfg_tabpfn['save_intermediate_arrays'] = True  # Force saving intermediate arrays for compatibility
                        
                        output_paths = feature_engineering_tabpfn.run_feature_engineering(
                            stage_outputs['processed_dataframe_path'], cfg_tabpfn
                        )
                        
                        # Rename outputs to have TabPFN suffix to avoid conflicts
                        renamed_outputs = {}
                        for key, path in output_paths.items():
                            if key == 'tabular_dataframe_for_tabpfn':
                                renamed_outputs[key] = path  # Keep this name as is
                            else:
                                # Add _tabpfn suffix to other files
                                base_dir = os.path.dirname(path)
                                filename = os.path.basename(path)
                                name, ext = os.path.splitext(filename)
                                new_filename = f"{name}_tabpfn{ext}"
                                new_path = os.path.join(base_dir, new_filename)
                                
                                # Rename the file if it exists
                                if os.path.exists(path):
                                    os.rename(path, new_path)
                                    renamed_outputs[key] = new_path
                                    logging.info(f"Renamed {path} to {new_path}")
                        
                        stage_outputs.update(renamed_outputs)
                else:
                    logging.info("Using standard feature engineering script...")
                    # Standard feature engineering outputs
                    expected_outputs = {
                        'X_windows_raw': os.path.join(feature_dir, 'X_windows_raw.npy'),
                        'engineered_features': os.path.join(feature_dir, 'engineered_features.npy'),
                        'y_windows': os.path.join(feature_dir, 'y_windows.npy'),
                        'subject_ids_windows': os.path.join(feature_dir, 'subject_ids_windows.npy'),
                        'engineered_feature_names': os.path.join(feature_dir, 'engineered_feature_names.pkl'),
                        'original_feature_names': os.path.join(feature_dir, 'original_feature_names.pkl'),
                        'window_start_times': os.path.join(feature_dir, 'window_start_times.npy'),
                        'window_end_times': os.path.join(feature_dir, 'window_end_times.npy')
                    }
                    outputs_exist = all(os.path.exists(p) for p in expected_outputs.values())

                    if not force_this_stage and outputs_exist:
                         logging.warning(f"Outputs exist for '{stage_name}'. Skipping execution.")
                         stage_outputs.update(expected_outputs) # Store paths anyway
                    else:
                         output_paths = feature_engineering.run_feature_engineering(
                              stage_outputs['processed_dataframe_path'], cfg
                         )
                         stage_outputs.update(output_paths) # Store paths to generated files


            elif stage_name == 'feature_selection':
                # This stage is optional based on config, but also depends on previous stage outputs
                if not cfg.get('run_feature_selection', False):
                    logging.info(f"Skipping stage '{stage_name}' as 'run_feature_selection' is False in config.")
                    # Store the expected *output* path even if not run, as data_prep might look for it
                    results_dir_fs = os.path.join(project_root, cfg.get('results_dir', 'results'))
                    output_filename = cfg.get('feature_selection_output_file', 'selected_features_pyimpetus.pkl')
                    
                    # Add suffix if using TabPFN version
                    use_tabpfn_version = cfg.get('use_tabpfn_feature_engineering', False)
                    if use_tabpfn_version:
                        name, ext = os.path.splitext(output_filename)
                        output_filename = f"{name}_tabpfn{ext}"
                    
                    stage_outputs['selected_features_path'] = os.path.join(results_dir_fs, output_filename)
                    continue # Skip to next stage

                # Check required inputs from previous stage
                required_fs_inputs = ['engineered_features', 'y_windows', 'subject_ids_windows', 'engineered_feature_names']
                if not all(key in stage_outputs and os.path.exists(stage_outputs[key]) for key in required_fs_inputs):
                     logging.error(f"Cannot run '{stage_name}': One or more input files missing.")
                     raise FileNotFoundError("Input for feature selection not available.")

                output_filename = cfg.get('feature_selection_output_file', 'selected_features_pyimpetus.pkl')
                
                # Add suffix if using TabPFN version
                use_tabpfn_version = cfg.get('use_tabpfn_feature_engineering', False)
                if use_tabpfn_version:
                    name, ext = os.path.splitext(output_filename)
                    output_filename = f"{name}_tabpfn{ext}"
                
                expected_output_path = os.path.join(project_root, cfg.get('results_dir', 'results'), output_filename)

                if not force_this_stage and os.path.exists(expected_output_path):
                    logging.warning(f"Output exists for '{stage_name}': {expected_output_path}. Skipping execution.")
                    stage_outputs['selected_features_path'] = expected_output_path
                else:
                    saved_path = feature_selector.run_feature_selection(
                        engineered_features_path=stage_outputs['engineered_features'],
                        y_windows_path=stage_outputs['y_windows'],
                        subject_ids_path=stage_outputs['subject_ids_windows'],
                        engineered_names_path=stage_outputs['engineered_feature_names'],
                        config=cfg
                    )
                    if saved_path:
                        # Rename output file to include TabPFN suffix if needed
                        if use_tabpfn_version and os.path.exists(saved_path):
                            base_dir = os.path.dirname(saved_path)
                            filename = os.path.basename(saved_path)
                            name, ext = os.path.splitext(filename)
                            if not name.endswith('_tabpfn'):
                                new_filename = f"{name}_tabpfn{ext}"
                                new_path = os.path.join(base_dir, new_filename)
                                os.rename(saved_path, new_path)
                                saved_path = new_path
                                logging.info(f"Renamed feature selection output to: {new_path}")
                        
                        stage_outputs['selected_features_path'] = saved_path
                    else:
                        # Handle FS failure - maybe default to no selection?
                        logging.error("Feature selection failed to produce an output file.")
                        stage_outputs['selected_features_path'] = None # Indicate failure/no selection


            elif stage_name == 'data_preparation':
                required_prep_inputs = ['X_windows_raw', 'engineered_features', 'y_windows', 'subject_ids_windows', 'original_feature_names', 'engineered_feature_names']
                if not all(key in stage_outputs and os.path.exists(stage_outputs[key]) for key in required_prep_inputs):
                     logging.error(f"Cannot run '{stage_name}': One or more input files missing.")
                     raise FileNotFoundError("Input for data preparation not available.")

                # Define expected output paths
                prep_data_dir = os.path.join(project_root, cfg.get('results_dir', 'results'), 'prepared_data')
                
                # Add suffix if using TabPFN version
                use_tabpfn_version = cfg.get('use_tabpfn_feature_engineering', False)
                suffix = '_tabpfn' if use_tabpfn_version else ''
                
                expected_prep_outputs = {
                    'X_train': os.path.join(prep_data_dir, f'X_train{suffix}.npy'),
                    'y_train': os.path.join(prep_data_dir, f'y_train{suffix}.npy'),
                    'X_test': os.path.join(prep_data_dir, f'X_test{suffix}.npy'),
                    'y_test': os.path.join(prep_data_dir, f'y_test{suffix}.npy'),
                    'scaler': os.path.join(prep_data_dir, f'scaler{suffix}.pkl'),
                    'label_encoder': os.path.join(prep_data_dir, f'label_encoder{suffix}.pkl'),
                    'train_subject_ids': os.path.join(prep_data_dir, f'train_subject_ids{suffix}.npy'),
                    'summary': os.path.join(prep_data_dir, f'data_summary{suffix}.pkl')
                }
                prep_outputs_exist = all(os.path.exists(p) for p in expected_prep_outputs.values())

                # Get path to selected features (might be None if FS was skipped or failed)
                sel_feat_path_input = stage_outputs.get('selected_features_path') # Get path from previous stage output
                # Ensure path exists only if use_selected_features is True
                if cfg.get('use_selected_features', False) and (not sel_feat_path_input or not os.path.exists(sel_feat_path_input)):
                     logging.warning(f"Configured to use selected features, but file missing: {sel_feat_path_input}. Will proceed without selection.")
                     sel_feat_path_input = None # Override path if file is missing but use=True

                if not force_this_stage and prep_outputs_exist:
                    logging.warning(f"Outputs exist for '{stage_name}'. Skipping execution.")
                    stage_outputs.update(expected_prep_outputs) # Store paths
                else:
                    # Modify config to use suffix for output files if needed
                    cfg_prep = cfg.copy()
                    if use_tabpfn_version:
                        cfg_prep['output_file_suffix'] = '_tabpfn'
                    
                    prep_output_paths = data_preparation.run_data_preparation(
                        config=cfg_prep,
                        x_windows_raw_path=stage_outputs['X_windows_raw'],
                        engineered_features_path=stage_outputs['engineered_features'],
                        y_windows_path=stage_outputs['y_windows'],
                        subject_ids_path=stage_outputs['subject_ids_windows'],
                        original_feature_names_path=stage_outputs['original_feature_names'],
                        engineered_names_path=stage_outputs['engineered_feature_names'],
                        selected_features_path=sel_feat_path_input # Pass the potentially None path
                    )
                    
                    # If using TabPFN, rename outputs to have suffix
                    if use_tabpfn_version:
                        renamed_outputs = {}
                        for key, path in prep_output_paths.items():
                            base_dir = os.path.dirname(path)
                            filename = os.path.basename(path)
                            name, ext = os.path.splitext(filename)
                            if not name.endswith('_tabpfn'):
                                new_filename = f"{name}_tabpfn{ext}"
                                new_path = os.path.join(base_dir, new_filename)
                                if os.path.exists(path):
                                    os.rename(path, new_path)
                                    renamed_outputs[key] = new_path
                                    logging.info(f"Renamed {path} to {new_path}")
                                else:
                                    renamed_outputs[key] = new_path
                            else:
                                renamed_outputs[key] = path
                        stage_outputs.update(renamed_outputs)
                    else:
                        stage_outputs.update(prep_output_paths)


            elif stage_name == 'training':
                required_train_inputs = ['X_train', 'y_train', 'X_test', 'y_test', 'label_encoder', 'train_subject_ids', 'summary'] # Scaler maybe optional here
                if not all(key in stage_outputs and os.path.exists(stage_outputs[key]) for key in required_train_inputs):
                     logging.error(f"Cannot run '{stage_name}': One or more input files missing.")
                     raise FileNotFoundError("Input for training not available.")

                model_name = cfg.get('model_name', 'Simple1DCNN')
                
                # Add suffix if using TabPFN version
                use_tabpfn_version = cfg.get('use_tabpfn_feature_engineering', False)
                suffix = '_tabpfn' if use_tabpfn_version else ''
                
                expected_output_path = os.path.join(project_root, cfg.get('results_dir', 'results'), f"{model_name}_final_state_dict{suffix}.pt")

                if not force_this_stage and os.path.exists(expected_output_path):
                     logging.warning(f"Output exists for '{stage_name}': {expected_output_path}. Skipping execution.")
                     stage_outputs['trained_model_path'] = expected_output_path
                else:
                     # Modify config to use suffix for output files if needed
                     cfg_train = cfg.copy()
                     if use_tabpfn_version:
                         cfg_train['output_file_suffix'] = '_tabpfn'
                     
                     # Pass the dictionary of prepared data paths directly
                     model_save_path = training.run_training(cfg_train, stage_outputs)
                     
                     # If using TabPFN, rename output to have suffix if needed
                     if use_tabpfn_version and os.path.exists(model_save_path):
                         base_dir = os.path.dirname(model_save_path)
                         filename = os.path.basename(model_save_path)
                         name, ext = os.path.splitext(filename)
                         if not name.endswith('_tabpfn'):
                             new_filename = f"{name}_tabpfn{ext}"
                             new_path = os.path.join(base_dir, new_filename)
                             os.rename(model_save_path, new_path)
                             model_save_path = new_path
                             logging.info(f"Renamed trained model to: {new_path}")
                     
                     stage_outputs['trained_model_path'] = model_save_path


            elif stage_name == 'evaluation':
                required_eval_inputs = ['X_test', 'y_test', 'label_encoder', 'summary'] # Model path checked separately
                if not all(key in stage_outputs and os.path.exists(stage_outputs[key]) for key in required_eval_inputs):
                     logging.error(f"Cannot run '{stage_name}': One or more input files missing.")
                     raise FileNotFoundError("Input for evaluation not available.")
                if 'trained_model_path' not in stage_outputs or not os.path.exists(stage_outputs['trained_model_path']):
                     logging.error(f"Cannot run '{stage_name}': Trained model path missing or file not found.")
                     raise FileNotFoundError("Trained model for evaluation not available.")

                # Evaluation typically always runs unless explicitly skipped
                # No simple output check to skip, as it produces multiple plots/files

                # Pass necessary paths from stage_outputs (contains paths from data_prep)
                eval_metrics = evaluation.run_evaluation(
                     cfg,
                     stage_outputs['trained_model_path'],
                     stage_outputs # Pass the whole dict containing prep_data_paths
                )
                stage_outputs['evaluation_metrics'] = eval_metrics # Store metrics


            # Log stage completion time
            stage_duration = time.time() - stage_start_time
            logging.info(f"===== Stage '{stage_name}' completed in {stage_duration:.2f} seconds =====")

        except Exception as e:
            pipeline_successful = False
            logging.error(f"!!! Pipeline failed at stage: '{stage_name}' !!!")
            logging.error(f"Error: {e}", exc_info=True)
            break # Stop pipeline on error

    # --- Pipeline Finish ---
    overall_duration = time.time() - overall_start_time
    if pipeline_successful:
        logging.info(f"\n--- Pipeline finished successfully in {overall_duration:.2f} seconds ---")
    else:
        logging.error(f"\n--- Pipeline failed after {overall_duration:.2f} seconds ---")
        sys.exit(1) # Exit with error code


if __name__ == '__main__':
    main()