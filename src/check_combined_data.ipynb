{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db24198b",
   "metadata": {},
   "source": [
    "# Combined Data Checker and Interactive Visualizer\n",
    "\n",
    "**Purpose**: Load and interactively visualize the combined sensor data with labels created by debug_labels_v2.ipynb.\n",
    "\n",
    "**Features**:\n",
    "- Load combined PKL data with all sensors and labels\n",
    "- Interactive plotting with time navigation\n",
    "- Label visualization as shaded areas\n",
    "- Data quality inspection\n",
    "- Sensor selection and filtering\n",
    "- Time window controls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def1dd5",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d53310fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded sync events for OutSense-713:\n",
      "   Sync Start: 2024-07-26 10:55:00\n",
      "   Sync End: 2024-07-28 10:13:00\n",
      "   Duration: 1 days 23:18:00\n",
      "📋 Configuration:\n",
      "  Subject: OutSense-713\n",
      "  Base directory: /scai_data3/scratch/stirnimann_r\n",
      "  Results directory: /scai_data3/scratch/stirnimann_r/results/OutSense-713\n",
      "  Combined data file: /scai_data3/scratch/stirnimann_r/results/OutSense-713/OutSense-713_combined_data.pkl\n",
      "  Metadata file: /scai_data3/scratch/stirnimann_r/results/OutSense-713/OutSense-713_combined_data_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "SUBJECT_ID = \"OutSense-713\"  # Change this to your subject\n",
    "\n",
    "# Paths - Fixed to match actual directory structure\n",
    "base_dir = '/scai_data3/scratch/stirnimann_r'\n",
    "results_dir = os.path.join(base_dir, 'results', SUBJECT_ID)\n",
    "\n",
    "# Load sync events for navigation\n",
    "sync_events_path = os.path.join(base_dir, 'Sync_Events_Times.csv')\n",
    "sync_start_time = None\n",
    "sync_end_time = None\n",
    "\n",
    "try:\n",
    "    if os.path.exists(sync_events_path):\n",
    "        sync_events_df = pd.read_csv(sync_events_path)\n",
    "        subject_sync = sync_events_df[sync_events_df['Subject'] == SUBJECT_ID]\n",
    "        \n",
    "        if len(subject_sync) > 0:\n",
    "            sync_start_str = subject_sync.iloc[0]['Sync Start']\n",
    "            sync_end_str = subject_sync.iloc[0]['Sync End']\n",
    "            \n",
    "            sync_start_time = pd.to_datetime(sync_start_str, format='%d.%m.%Y.%H.%M.%S')\n",
    "            sync_end_time = pd.to_datetime(sync_end_str, format='%d.%m.%Y.%H.%M.%S')\n",
    "            \n",
    "            print(f\"✅ Loaded sync events for {SUBJECT_ID}:\")\n",
    "            print(f\"   Sync Start: {sync_start_time}\")\n",
    "            print(f\"   Sync End: {sync_end_time}\")\n",
    "            print(f\"   Duration: {sync_end_time - sync_start_time}\")\n",
    "        else:\n",
    "            print(f\"⚠️ No sync events found for {SUBJECT_ID}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Sync events file not found: {sync_events_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading sync events: {e}\")\n",
    "\n",
    "# File paths\n",
    "combined_data_path = os.path.join(results_dir, f'{SUBJECT_ID}_combined_data.pkl')\n",
    "metadata_path = os.path.join(results_dir, f'{SUBJECT_ID}_combined_data_metadata.json')\n",
    "\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"  Subject: {SUBJECT_ID}\")\n",
    "print(f\"  Base directory: {base_dir}\")\n",
    "print(f\"  Results directory: {results_dir}\")\n",
    "print(f\"  Combined data file: {combined_data_path}\")\n",
    "print(f\"  Metadata file: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4dc1e",
   "metadata": {},
   "source": [
    "## 2. Load Combined Data and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "093f99fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOADING COMBINED DATA ===\n",
      "✅ Loaded metadata\n",
      "📊 Loading combined data...\n",
      "✅ Successfully loaded combined data!\n",
      "📊 Data shape: (4420268, 30)\n",
      "⏱️ Time range: 2024-07-26 10:06:09.261336565 to 2024-07-28 11:12:59.994135618\n",
      "⏱️ Duration: 2 days 01:06:50.732799053\n",
      "📈 Total columns: 30\n",
      "📁 File size: 1032.8 MB\n",
      "\n",
      "📋 Metadata Summary:\n",
      "  Creation time: 2025-07-15T13:36:18.071323\n",
      "  Sampling frequency: 25 Hz\n",
      "  Total samples: 4420268\n",
      "  Sensors processed: 7\n",
      "  Labels available: 425\n",
      "  Labeled timestamps: 3732432\n",
      "  Label coverage: 84.4%\n"
     ]
    }
   ],
   "source": [
    "# Load the combined data\n",
    "print(\"=== LOADING COMBINED DATA ===\")\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(combined_data_path):\n",
    "    raise FileNotFoundError(f\"Combined data file not found: {combined_data_path}\")\n",
    "    \n",
    "if not os.path.exists(metadata_path):\n",
    "    print(f\"⚠️ Metadata file not found: {metadata_path}\")\n",
    "    metadata = {}\n",
    "else:\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"✅ Loaded metadata\")\n",
    "\n",
    "# Load the combined data\n",
    "print(f\"📊 Loading combined data...\")\n",
    "with open(combined_data_path, 'rb') as f:\n",
    "    combined_data = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Successfully loaded combined data!\")\n",
    "print(f\"📊 Data shape: {combined_data.shape}\")\n",
    "print(f\"⏱️ Time range: {combined_data.index.min()} to {combined_data.index.max()}\")\n",
    "print(f\"⏱️ Duration: {combined_data.index.max() - combined_data.index.min()}\")\n",
    "print(f\"📈 Total columns: {len(combined_data.columns)}\")\n",
    "\n",
    "# Show file size\n",
    "file_size = os.path.getsize(combined_data_path)\n",
    "file_size_mb = file_size / (1024 * 1024)\n",
    "print(f\"📁 File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "# Display metadata summary if available\n",
    "if metadata:\n",
    "    print(f\"\\n📋 Metadata Summary:\")\n",
    "    print(f\"  Creation time: {metadata.get('creation_time', 'Unknown')}\")\n",
    "    print(f\"  Sampling frequency: {metadata.get('sampling', {}).get('target_frequency_hz', 'Unknown')} Hz\")\n",
    "    print(f\"  Total samples: {metadata.get('sampling', {}).get('total_samples', 'Unknown')}\")\n",
    "    \n",
    "    sensors_info = metadata.get('sensors', {})\n",
    "    print(f\"  Sensors processed: {len(sensors_info)}\")\n",
    "    \n",
    "    labels_info = metadata.get('labels', {})\n",
    "    print(f\"  Labels available: {labels_info.get('total_labels_available', 'Unknown')}\")\n",
    "    print(f\"  Labeled timestamps: {labels_info.get('labeled_timestamps', 'Unknown')}\")\n",
    "    print(f\"  Label coverage: {labels_info.get('label_coverage_percent', 0):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ff023",
   "metadata": {},
   "source": [
    "## 3. Data Analysis and Column Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "448ed95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA STRUCTURE ANALYSIS ===\n",
      "📊 Column breakdown:\n",
      "  Sensor columns: 29\n",
      "  Label column: 1 ('Label')\n",
      "\n",
      "📈 Sensor groups identified:\n",
      "  📊 corsano_wrist_wrist_acc: 3 channels\n",
      "    Columns: ['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z']\n",
      "  📊 cosinuss_ear_ear_acc: 3 channels\n",
      "    Columns: ['cosinuss_ear_ear_acc_x', 'cosinuss_ear_ear_acc_y', 'cosinuss_ear_ear_acc_z']\n",
      "  📊 mbient_acc_x_axis: 1 channels\n",
      "    Columns: ['mbient_acc_x_axis_g']\n",
      "  📊 mbient_acc_y_axis: 1 channels\n",
      "    Columns: ['mbient_acc_y_axis_g']\n",
      "  📊 mbient_acc_z_axis: 1 channels\n",
      "    Columns: ['mbient_acc_z_axis_g']\n",
      "  📊 mbient_gyro_x_axis: 1 channels\n",
      "    Columns: ['mbient_gyro_x_axis_dps']\n",
      "  📊 mbient_gyro_y_axis: 1 channels\n",
      "    Columns: ['mbient_gyro_y_axis_dps']\n",
      "  📊 mbient_gyro_z_axis: 1 channels\n",
      "    Columns: ['mbient_gyro_z_axis_dps']\n",
      "  📊 vivalnk_acc_vivalnk_acc: 3 channels\n",
      "    Columns: ['vivalnk_acc_vivalnk_acc_x', 'vivalnk_acc_vivalnk_acc_y', 'vivalnk_acc_vivalnk_acc_z']\n",
      "  📊 sensomative_bottom_bottom_value: 11 channels\n",
      "    Columns: ['sensomative_bottom_bottom_value_1', 'sensomative_bottom_bottom_value_2', 'sensomative_bottom_bottom_value_3']...\n",
      "  📊 corsano_bioz_bioz_acc: 3 channels\n",
      "    Columns: ['corsano_bioz_bioz_acc_x', 'corsano_bioz_bioz_acc_y', 'corsano_bioz_bioz_acc_z']\n",
      "\n",
      "🏷️ Label analysis:\n",
      "  Empty labels: 687836 (15.6%)\n",
      "  Unique activities: 42\n",
      "  Top 10 activities:\n",
      "    🏷️ black_video: 2102660 samples (1401.8 min)\n",
      "    🏷️ wheelchair _in_storage: 1042709 samples (695.1 min)\n",
      "    🏷️ conversation: 94439 samples (63.0 min)\n",
      "    🏷️ self_propulsion: 64542 samples (43.0 min)\n",
      "    🏷️ waiting: 55391 samples (36.9 min)\n",
      "    🏷️ handling: 49904 samples (33.3 min)\n",
      "    🏷️ sitting_table: 44018 samples (29.3 min)\n",
      "    🏷️ watching_tv: 43063 samples (28.7 min)\n",
      "    🏷️ talking_phone: 41663 samples (27.8 min)\n",
      "    🏷️ assisted_propulsion: 37670 samples (25.1 min)\n",
      "\n",
      "🎨 Generated colors for 42 unique labels\n",
      "\n",
      "🔍 Data quality check:\n",
      "  ⚠️ Missing data detected:\n",
      "    corsano_wrist_wrist_acc_x: 649320 samples (14.7%)\n",
      "    corsano_wrist_wrist_acc_y: 649320 samples (14.7%)\n",
      "    corsano_wrist_wrist_acc_z: 649320 samples (14.7%)\n",
      "    cosinuss_ear_ear_acc_x: 3927103 samples (88.8%)\n",
      "    cosinuss_ear_ear_acc_y: 3927103 samples (88.8%)\n",
      "    cosinuss_ear_ear_acc_z: 3927103 samples (88.8%)\n",
      "    vivalnk_acc_vivalnk_acc_x: 13240 samples (0.3%)\n",
      "    vivalnk_acc_vivalnk_acc_y: 13240 samples (0.3%)\n",
      "    vivalnk_acc_vivalnk_acc_z: 13240 samples (0.3%)\n",
      "    sensomative_bottom_bottom_value_1: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_2: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_3: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_4: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_5: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_6: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_7: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_8: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_9: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_10: 1177813 samples (26.6%)\n",
      "    sensomative_bottom_bottom_value_11: 1177813 samples (26.6%)\n",
      "    corsano_bioz_bioz_acc_x: 288790 samples (6.5%)\n",
      "    corsano_bioz_bioz_acc_y: 288790 samples (6.5%)\n",
      "    corsano_bioz_bioz_acc_z: 288790 samples (6.5%)\n",
      "\n",
      "👀 Data sample (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "corsano_wrist_wrist_acc_x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "corsano_wrist_wrist_acc_y",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "corsano_wrist_wrist_acc_z",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cosinuss_ear_ear_acc_x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cosinuss_ear_ear_acc_y",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cosinuss_ear_ear_acc_z",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mbient_acc_x_axis_g",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mbient_acc_y_axis_g",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mbient_acc_z_axis_g",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mbient_gyro_x_axis_dps",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mbient_gyro_y_axis_dps",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mbient_gyro_z_axis_dps",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "vivalnk_acc_vivalnk_acc_x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "vivalnk_acc_vivalnk_acc_y",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "vivalnk_acc_vivalnk_acc_z",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sensomative_bottom_bottom_value_11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "corsano_bioz_bioz_acc_x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "corsano_bioz_bioz_acc_y",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "corsano_bioz_bioz_acc_z",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bdba7187-d456-4cdc-841c-a4234bdd3a09",
       "rows": [
        [
         "2024-07-26 10:06:09.261336565",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.026",
         "-0.022",
         "1.024",
         "0.0915",
         "0.2135",
         "-0.0305",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         ""
        ],
        [
         "2024-07-26 10:06:09.301336576",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.027",
         "-0.0225",
         "1.0325",
         "0.0",
         "-0.0305",
         "-0.0305",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         ""
        ],
        [
         "2024-07-26 10:06:09.341336588",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.0265",
         "-0.023",
         "1.03",
         "-0.0305",
         "0.2135",
         "-0.061",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         ""
        ],
        [
         "2024-07-26 10:06:09.381336600",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.025",
         "-0.023",
         "1.0314999999999999",
         "-0.061",
         "0.2135",
         "-0.1525",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         ""
        ],
        [
         "2024-07-26 10:06:09.421336612",
         null,
         null,
         null,
         null,
         null,
         null,
         "0.026000000000000002",
         "-0.023",
         "1.03",
         "0.0915",
         "0.1525",
         "0.0305",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         ""
        ]
       ],
       "shape": {
        "columns": 30,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corsano_wrist_wrist_acc_x</th>\n",
       "      <th>corsano_wrist_wrist_acc_y</th>\n",
       "      <th>corsano_wrist_wrist_acc_z</th>\n",
       "      <th>cosinuss_ear_ear_acc_x</th>\n",
       "      <th>cosinuss_ear_ear_acc_y</th>\n",
       "      <th>cosinuss_ear_ear_acc_z</th>\n",
       "      <th>mbient_acc_x_axis_g</th>\n",
       "      <th>mbient_acc_y_axis_g</th>\n",
       "      <th>mbient_acc_z_axis_g</th>\n",
       "      <th>mbient_gyro_x_axis_dps</th>\n",
       "      <th>...</th>\n",
       "      <th>sensomative_bottom_bottom_value_6</th>\n",
       "      <th>sensomative_bottom_bottom_value_7</th>\n",
       "      <th>sensomative_bottom_bottom_value_8</th>\n",
       "      <th>sensomative_bottom_bottom_value_9</th>\n",
       "      <th>sensomative_bottom_bottom_value_10</th>\n",
       "      <th>sensomative_bottom_bottom_value_11</th>\n",
       "      <th>corsano_bioz_bioz_acc_x</th>\n",
       "      <th>corsano_bioz_bioz_acc_y</th>\n",
       "      <th>corsano_bioz_bioz_acc_z</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-07-26 10:06:09.261336565</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>-0.0220</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 10:06:09.301336576</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>-0.0225</td>\n",
       "      <td>1.0325</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 10:06:09.341336588</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>-0.0230</td>\n",
       "      <td>1.0300</td>\n",
       "      <td>-0.0305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 10:06:09.381336600</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>-0.0230</td>\n",
       "      <td>1.0315</td>\n",
       "      <td>-0.0610</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-07-26 10:06:09.421336612</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>-0.0230</td>\n",
       "      <td>1.0300</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               corsano_wrist_wrist_acc_x  \\\n",
       "2024-07-26 10:06:09.261336565                        NaN   \n",
       "2024-07-26 10:06:09.301336576                        NaN   \n",
       "2024-07-26 10:06:09.341336588                        NaN   \n",
       "2024-07-26 10:06:09.381336600                        NaN   \n",
       "2024-07-26 10:06:09.421336612                        NaN   \n",
       "\n",
       "                               corsano_wrist_wrist_acc_y  \\\n",
       "2024-07-26 10:06:09.261336565                        NaN   \n",
       "2024-07-26 10:06:09.301336576                        NaN   \n",
       "2024-07-26 10:06:09.341336588                        NaN   \n",
       "2024-07-26 10:06:09.381336600                        NaN   \n",
       "2024-07-26 10:06:09.421336612                        NaN   \n",
       "\n",
       "                               corsano_wrist_wrist_acc_z  \\\n",
       "2024-07-26 10:06:09.261336565                        NaN   \n",
       "2024-07-26 10:06:09.301336576                        NaN   \n",
       "2024-07-26 10:06:09.341336588                        NaN   \n",
       "2024-07-26 10:06:09.381336600                        NaN   \n",
       "2024-07-26 10:06:09.421336612                        NaN   \n",
       "\n",
       "                               cosinuss_ear_ear_acc_x  cosinuss_ear_ear_acc_y  \\\n",
       "2024-07-26 10:06:09.261336565                     NaN                     NaN   \n",
       "2024-07-26 10:06:09.301336576                     NaN                     NaN   \n",
       "2024-07-26 10:06:09.341336588                     NaN                     NaN   \n",
       "2024-07-26 10:06:09.381336600                     NaN                     NaN   \n",
       "2024-07-26 10:06:09.421336612                     NaN                     NaN   \n",
       "\n",
       "                               cosinuss_ear_ear_acc_z  mbient_acc_x_axis_g  \\\n",
       "2024-07-26 10:06:09.261336565                     NaN               0.0260   \n",
       "2024-07-26 10:06:09.301336576                     NaN               0.0270   \n",
       "2024-07-26 10:06:09.341336588                     NaN               0.0265   \n",
       "2024-07-26 10:06:09.381336600                     NaN               0.0250   \n",
       "2024-07-26 10:06:09.421336612                     NaN               0.0260   \n",
       "\n",
       "                               mbient_acc_y_axis_g  mbient_acc_z_axis_g  \\\n",
       "2024-07-26 10:06:09.261336565              -0.0220               1.0240   \n",
       "2024-07-26 10:06:09.301336576              -0.0225               1.0325   \n",
       "2024-07-26 10:06:09.341336588              -0.0230               1.0300   \n",
       "2024-07-26 10:06:09.381336600              -0.0230               1.0315   \n",
       "2024-07-26 10:06:09.421336612              -0.0230               1.0300   \n",
       "\n",
       "                               mbient_gyro_x_axis_dps  ...  \\\n",
       "2024-07-26 10:06:09.261336565                  0.0915  ...   \n",
       "2024-07-26 10:06:09.301336576                  0.0000  ...   \n",
       "2024-07-26 10:06:09.341336588                 -0.0305  ...   \n",
       "2024-07-26 10:06:09.381336600                 -0.0610  ...   \n",
       "2024-07-26 10:06:09.421336612                  0.0915  ...   \n",
       "\n",
       "                               sensomative_bottom_bottom_value_6  \\\n",
       "2024-07-26 10:06:09.261336565                                NaN   \n",
       "2024-07-26 10:06:09.301336576                                NaN   \n",
       "2024-07-26 10:06:09.341336588                                NaN   \n",
       "2024-07-26 10:06:09.381336600                                NaN   \n",
       "2024-07-26 10:06:09.421336612                                NaN   \n",
       "\n",
       "                               sensomative_bottom_bottom_value_7  \\\n",
       "2024-07-26 10:06:09.261336565                                NaN   \n",
       "2024-07-26 10:06:09.301336576                                NaN   \n",
       "2024-07-26 10:06:09.341336588                                NaN   \n",
       "2024-07-26 10:06:09.381336600                                NaN   \n",
       "2024-07-26 10:06:09.421336612                                NaN   \n",
       "\n",
       "                               sensomative_bottom_bottom_value_8  \\\n",
       "2024-07-26 10:06:09.261336565                                NaN   \n",
       "2024-07-26 10:06:09.301336576                                NaN   \n",
       "2024-07-26 10:06:09.341336588                                NaN   \n",
       "2024-07-26 10:06:09.381336600                                NaN   \n",
       "2024-07-26 10:06:09.421336612                                NaN   \n",
       "\n",
       "                               sensomative_bottom_bottom_value_9  \\\n",
       "2024-07-26 10:06:09.261336565                                NaN   \n",
       "2024-07-26 10:06:09.301336576                                NaN   \n",
       "2024-07-26 10:06:09.341336588                                NaN   \n",
       "2024-07-26 10:06:09.381336600                                NaN   \n",
       "2024-07-26 10:06:09.421336612                                NaN   \n",
       "\n",
       "                               sensomative_bottom_bottom_value_10  \\\n",
       "2024-07-26 10:06:09.261336565                                 NaN   \n",
       "2024-07-26 10:06:09.301336576                                 NaN   \n",
       "2024-07-26 10:06:09.341336588                                 NaN   \n",
       "2024-07-26 10:06:09.381336600                                 NaN   \n",
       "2024-07-26 10:06:09.421336612                                 NaN   \n",
       "\n",
       "                               sensomative_bottom_bottom_value_11  \\\n",
       "2024-07-26 10:06:09.261336565                                 NaN   \n",
       "2024-07-26 10:06:09.301336576                                 NaN   \n",
       "2024-07-26 10:06:09.341336588                                 NaN   \n",
       "2024-07-26 10:06:09.381336600                                 NaN   \n",
       "2024-07-26 10:06:09.421336612                                 NaN   \n",
       "\n",
       "                               corsano_bioz_bioz_acc_x  \\\n",
       "2024-07-26 10:06:09.261336565                      NaN   \n",
       "2024-07-26 10:06:09.301336576                      NaN   \n",
       "2024-07-26 10:06:09.341336588                      NaN   \n",
       "2024-07-26 10:06:09.381336600                      NaN   \n",
       "2024-07-26 10:06:09.421336612                      NaN   \n",
       "\n",
       "                               corsano_bioz_bioz_acc_y  \\\n",
       "2024-07-26 10:06:09.261336565                      NaN   \n",
       "2024-07-26 10:06:09.301336576                      NaN   \n",
       "2024-07-26 10:06:09.341336588                      NaN   \n",
       "2024-07-26 10:06:09.381336600                      NaN   \n",
       "2024-07-26 10:06:09.421336612                      NaN   \n",
       "\n",
       "                               corsano_bioz_bioz_acc_z  Label  \n",
       "2024-07-26 10:06:09.261336565                      NaN         \n",
       "2024-07-26 10:06:09.301336576                      NaN         \n",
       "2024-07-26 10:06:09.341336588                      NaN         \n",
       "2024-07-26 10:06:09.381336600                      NaN         \n",
       "2024-07-26 10:06:09.421336612                      NaN         \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 All columns (30):\n",
      "   1. corsano_wrist_wrist_acc_x\n",
      "   2. corsano_wrist_wrist_acc_y\n",
      "   3. corsano_wrist_wrist_acc_z\n",
      "   4. cosinuss_ear_ear_acc_x\n",
      "   5. cosinuss_ear_ear_acc_y\n",
      "   6. cosinuss_ear_ear_acc_z\n",
      "   7. mbient_acc_x_axis_g\n",
      "   8. mbient_acc_y_axis_g\n",
      "   9. mbient_acc_z_axis_g\n",
      "  10. mbient_gyro_x_axis_dps\n",
      "  11. mbient_gyro_y_axis_dps\n",
      "  12. mbient_gyro_z_axis_dps\n",
      "  13. vivalnk_acc_vivalnk_acc_x\n",
      "  14. vivalnk_acc_vivalnk_acc_y\n",
      "  15. vivalnk_acc_vivalnk_acc_z\n",
      "  16. sensomative_bottom_bottom_value_1\n",
      "  17. sensomative_bottom_bottom_value_2\n",
      "  18. sensomative_bottom_bottom_value_3\n",
      "  19. sensomative_bottom_bottom_value_4\n",
      "  20. sensomative_bottom_bottom_value_5\n",
      "      ... and 10 more columns\n"
     ]
    }
   ],
   "source": [
    "# Analyze the data structure\n",
    "print(\"=== DATA STRUCTURE ANALYSIS ===\")\n",
    "\n",
    "# Separate sensor columns from label column\n",
    "label_column = 'Label'\n",
    "sensor_columns = [col for col in combined_data.columns if col != label_column]\n",
    "\n",
    "print(f\"📊 Column breakdown:\")\n",
    "print(f\"  Sensor columns: {len(sensor_columns)}\")\n",
    "print(f\"  Label column: 1 ('{label_column}')\")\n",
    "\n",
    "# Group columns by sensor type\n",
    "sensor_groups = {}\n",
    "for col in sensor_columns:\n",
    "    # Extract sensor prefix (everything before the last underscore)\n",
    "    parts = col.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        sensor_name = '_'.join(parts[:-1])  # All parts except the last one\n",
    "    else:\n",
    "        sensor_name = col\n",
    "    \n",
    "    if sensor_name not in sensor_groups:\n",
    "        sensor_groups[sensor_name] = []\n",
    "    sensor_groups[sensor_name].append(col)\n",
    "\n",
    "print(f\"\\n📈 Sensor groups identified:\")\n",
    "for sensor_name, columns in sensor_groups.items():\n",
    "    print(f\"  📊 {sensor_name}: {len(columns)} channels\")\n",
    "    print(f\"    Columns: {columns[:3]}{'...' if len(columns) > 3 else ''}\")\n",
    "\n",
    "# Analyze labels\n",
    "if label_column in combined_data.columns:\n",
    "    label_stats = combined_data[label_column].value_counts()\n",
    "    non_empty_labels = label_stats[label_stats.index != '']\n",
    "    empty_count = label_stats.get('', 0)\n",
    "    \n",
    "    print(f\"\\n🏷️ Label analysis:\")\n",
    "    print(f\"  Empty labels: {empty_count} ({empty_count/len(combined_data)*100:.1f}%)\")\n",
    "    print(f\"  Unique activities: {len(non_empty_labels)}\")\n",
    "    \n",
    "    if len(non_empty_labels) > 0:\n",
    "        print(f\"  Top 10 activities:\")\n",
    "        for label, count in non_empty_labels.head(10).items():\n",
    "            duration_min = count / (metadata.get('sampling', {}).get('target_frequency_hz', 25)) / 60\n",
    "            print(f\"    🏷️ {label}: {count} samples ({duration_min:.1f} min)\")\n",
    "else:\n",
    "    print(f\"⚠️ No label column found in data\")\n",
    "    non_empty_labels = pd.Series()\n",
    "\n",
    "# Generate colors for labels\n",
    "def generate_label_colors(labels_list):\n",
    "    \"\"\"Generate consistent random colors for each unique label\"\"\"\n",
    "    unique_labels = list(set(labels_list))\n",
    "    random.seed(42)  # For consistent colors across runs\n",
    "    colors = []\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # Use HSV color space for better color distribution\n",
    "        hue = (i * 137.5) % 360  # Golden angle for good distribution\n",
    "        saturation = 0.7 + (i % 3) * 0.1  # Vary saturation\n",
    "        value = 0.8 + (i % 2) * 0.15  # Vary brightness\n",
    "        \n",
    "        # Convert HSV to RGB\n",
    "        rgb = mcolors.hsv_to_rgb([hue/360, saturation, value])\n",
    "        colors.append(rgb)\n",
    "    \n",
    "    return dict(zip(unique_labels, colors))\n",
    "\n",
    "# Generate colors for labels\n",
    "if len(non_empty_labels) > 0:\n",
    "    label_colors = generate_label_colors(non_empty_labels.index.tolist())\n",
    "    print(f\"\\n🎨 Generated colors for {len(label_colors)} unique labels\")\n",
    "else:\n",
    "    label_colors = {}\n",
    "    print(f\"\\n🎨 No labels to color\")\n",
    "\n",
    "# Data quality check\n",
    "print(f\"\\n🔍 Data quality check:\")\n",
    "missing_data = combined_data[sensor_columns].isnull().sum()\n",
    "total_missing = missing_data.sum()\n",
    "\n",
    "if total_missing > 0:\n",
    "    print(f\"  ⚠️ Missing data detected:\")\n",
    "    for col, missing_count in missing_data[missing_data > 0].items():\n",
    "        missing_pct = missing_count / len(combined_data) * 100\n",
    "        print(f\"    {col}: {missing_count} samples ({missing_pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"  ✅ No missing data in sensor columns\")\n",
    "\n",
    "# Show sample of data\n",
    "print(f\"\\n👀 Data sample (first 5 rows):\")\n",
    "display(combined_data.head())\n",
    "\n",
    "# Show columns list for reference\n",
    "print(f\"\\n📋 All columns ({len(combined_data.columns)}):\")\n",
    "for i, col in enumerate(combined_data.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")\n",
    "    if i >= 19:  # Show first 20 columns\n",
    "        remaining = len(combined_data.columns) - 20\n",
    "        if remaining > 0:\n",
    "            print(f\"      ... and {remaining} more columns\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23ed8b",
   "metadata": {},
   "source": [
    "## 4. Interactive Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9853c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INTERACTIVE DATA VISUALIZATION ===\n",
      "🎯 Interactive plotting with sensor selection and label overlay\n",
      "🔍 Navigate through the combined synchronized data\n",
      "✅ Interactive controls ready!\n"
     ]
    }
   ],
   "source": [
    "# Create interactive plotting tool\n",
    "print(\"=== INTERACTIVE DATA VISUALIZATION ===\")\n",
    "print(\"🎯 Interactive plotting with sensor selection and label overlay\")\n",
    "print(\"🔍 Navigate through the combined synchronized data\")\n",
    "\n",
    "# Create controls\n",
    "# Sensor group selection\n",
    "sensor_group_selection = widgets.SelectMultiple(\n",
    "    options=list(sensor_groups.keys()),\n",
    "    value=list(sensor_groups.keys())[:2] if len(sensor_groups) >= 2 else list(sensor_groups.keys()),\n",
    "    description='Select Sensors:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='150px', width='300px')\n",
    ")\n",
    "\n",
    "# Individual channel selection (will be updated based on sensor group selection)\n",
    "channel_selection = widgets.SelectMultiple(\n",
    "    options=sensor_columns[:20],  # Start with first 20 channels\n",
    "    value=sensor_columns[:5] if len(sensor_columns) >= 5 else sensor_columns,\n",
    "    description='Select Channels:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(height='200px', width='350px')\n",
    ")\n",
    "\n",
    "# Label display controls\n",
    "show_labels = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show Labels',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "label_alpha = widgets.FloatSlider(\n",
    "    value=0.3,\n",
    "    min=0.1,\n",
    "    max=0.8,\n",
    "    step=0.1,\n",
    "    description='Label Alpha:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Label filter\n",
    "if len(non_empty_labels) > 0:\n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=list(non_empty_labels.index),\n",
    "        value=list(non_empty_labels.index)[:10] if len(non_empty_labels) > 10 else list(non_empty_labels.index),\n",
    "        description='Show Labels:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='150px', width='300px')\n",
    "    )\n",
    "else:\n",
    "    label_filter = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        value=[],\n",
    "        description='Show Labels:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(height='150px', width='300px')\n",
    "    )\n",
    "\n",
    "# Time window controls\n",
    "data_start = combined_data.index.min()\n",
    "data_end = combined_data.index.max()\n",
    "data_center = data_start + (data_end - data_start) / 2\n",
    "\n",
    "center_time_text = widgets.Text(\n",
    "    value=data_center.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    description='Center Time:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "window_minutes = widgets.IntSlider(\n",
    "    value=10,  # 10 minute window\n",
    "    min=1,\n",
    "    max=120,  # 2 hours max\n",
    "    step=1,\n",
    "    description='Window (min):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Navigation buttons\n",
    "nav_backward_5min = widgets.Button(description='⏪ -5min', button_style='', \n",
    "                                  layout=widgets.Layout(width='90px'))\n",
    "nav_forward_5min = widgets.Button(description='⏩ +5min', button_style='', \n",
    "                                 layout=widgets.Layout(width='90px'))\n",
    "\n",
    "nav_backward_30min = widgets.Button(description='⏪ -30min', button_style='', \n",
    "                                   layout=widgets.Layout(width='100px'))\n",
    "nav_forward_30min = widgets.Button(description='⏩ +30min', button_style='', \n",
    "                                  layout=widgets.Layout(width='100px'))\n",
    "\n",
    "# Quick jump buttons\n",
    "jump_data_start = widgets.Button(description='🔝 Data Start', button_style='info')\n",
    "jump_data_center = widgets.Button(description='🎯 Data Center', button_style='success')\n",
    "jump_data_end = widgets.Button(description='🔚 Data End', button_style='info')\n",
    "\n",
    "# Sync jump buttons (if sync times are available)\n",
    "sync_jump_buttons = []\n",
    "if sync_start_time is not None:\n",
    "    jump_sync_start = widgets.Button(description='🎯 Sync Start', button_style='warning',\n",
    "                                   layout=widgets.Layout(width='110px'))\n",
    "    sync_jump_buttons.append(jump_sync_start)\n",
    "\n",
    "if sync_end_time is not None:\n",
    "    jump_sync_end = widgets.Button(description='🎯 Sync End', button_style='warning',\n",
    "                                 layout=widgets.Layout(width='110px'))\n",
    "    sync_jump_buttons.append(jump_sync_end)\n",
    "\n",
    "# Plot controls\n",
    "plot_style = widgets.Dropdown(\n",
    "    options=['overlay', 'subplots'],\n",
    "    value='overlay',\n",
    "    description='Plot Style:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "auto_plot = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Auto-plot on navigation',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "plot_button = widgets.Button(description='📈 Plot Data', button_style='primary', \n",
    "                            layout=widgets.Layout(width='150px'))\n",
    "\n",
    "# Output area\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Function to update channel selection based on sensor groups\n",
    "def update_channel_selection(*args):\n",
    "    selected_sensors = list(sensor_group_selection.value)\n",
    "    available_channels = []\n",
    "    \n",
    "    for sensor in selected_sensors:\n",
    "        if sensor in sensor_groups:\n",
    "            available_channels.extend(sensor_groups[sensor])\n",
    "    \n",
    "    # Update channel selection options\n",
    "    channel_selection.options = available_channels\n",
    "    # Select first few channels by default\n",
    "    default_selection = available_channels[:min(10, len(available_channels))]\n",
    "    channel_selection.value = default_selection\n",
    "\n",
    "# Connect sensor group selection to channel selection update\n",
    "sensor_group_selection.observe(update_channel_selection, names='value')\n",
    "\n",
    "# Helper functions\n",
    "def get_center_time():\n",
    "    \"\"\"Get center time from text widget\"\"\"\n",
    "    try:\n",
    "        return pd.to_datetime(center_time_text.value)\n",
    "    except:\n",
    "        return data_center\n",
    "\n",
    "def update_center_time(new_time):\n",
    "    \"\"\"Update center time text widget\"\"\"\n",
    "    # Ensure time is within data bounds\n",
    "    new_time = max(data_start, min(data_end, new_time))\n",
    "    center_time_text.value = new_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def plot_data(btn):\n",
    "    \"\"\"Plot selected channels with labels\"\"\"\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        try:\n",
    "            from matplotlib.patches import Patch  # For creating legend handles\n",
    "            selected_channels = list(channel_selection.value)\n",
    "            if not selected_channels:\n",
    "                print(\"❌ Please select at least one channel\")\n",
    "                return\n",
    "            \n",
    "            center_time = get_center_time()\n",
    "            window_mins = window_minutes.value\n",
    "            \n",
    "            # Calculate time window\n",
    "            half_window = pd.Timedelta(minutes=window_mins/2)\n",
    "            plot_start = center_time - half_window\n",
    "            plot_end = center_time + half_window\n",
    "            \n",
    "            # Ensure we don't go beyond data bounds\n",
    "            plot_start = max(plot_start, data_start)\n",
    "            plot_end = min(plot_end, data_end)\n",
    "            \n",
    "            print(f\"📊 Plotting {len(selected_channels)} channels\")\n",
    "            print(f\"⏱️ Time window: {plot_start} to {plot_end} ({window_mins} minutes)\")\n",
    "            print(f\"🎯 Center time: {center_time}\")\n",
    "            \n",
    "            # Filter data for plot window\n",
    "            mask = (combined_data.index >= plot_start) & (combined_data.index <= plot_end)\n",
    "            plot_data = combined_data[mask].copy()\n",
    "            \n",
    "            if plot_data.empty:\n",
    "                print(\"❌ No data in selected time window\")\n",
    "                print(f\"   Requested window: {plot_start} to {plot_end}\")\n",
    "                print(f\"   Data available: {data_start} to {data_end}\")\n",
    "                \n",
    "                # Find nearest data\n",
    "                if plot_start > data_end:\n",
    "                    time_diff = plot_start - data_end\n",
    "                    print(f\"   Window starts {time_diff} after data ends\")\n",
    "                elif plot_end < data_start:\n",
    "                    time_diff = data_start - plot_end\n",
    "                    print(f\"   Window ends {time_diff} before data starts\")\n",
    "                else:\n",
    "                    print(f\"   Window overlaps with data range but no samples found\")\n",
    "                    # Check for sparse data\n",
    "                    extended_mask = (combined_data.index >= plot_start - pd.Timedelta(hours=1)) & (combined_data.index <= plot_end + pd.Timedelta(hours=1))\n",
    "                    nearby_data = combined_data[extended_mask]\n",
    "                    if not nearby_data.empty:\n",
    "                        print(f\"   Found {len(nearby_data)} samples within ±1 hour\")\n",
    "                        print(f\"   Closest before: {nearby_data[nearby_data.index <= plot_start].index.max() if len(nearby_data[nearby_data.index <= plot_start]) > 0 else 'None'}\")\n",
    "                        print(f\"   Closest after: {nearby_data[nearby_data.index >= plot_end].index.min() if len(nearby_data[nearby_data.index >= plot_end]) > 0 else 'None'}\")\n",
    "                \n",
    "                return\n",
    "            \n",
    "            print(f\"📊 Plotting {len(plot_data)} samples\")\n",
    "            \n",
    "            # Create plot based on style\n",
    "            if plot_style.value == 'overlay':\n",
    "                # Single plot with all channels overlaid\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "                \n",
    "                # Plot each selected channel\n",
    "                for i, channel in enumerate(selected_channels):\n",
    "                    if channel in plot_data.columns:\n",
    "                        # Normalize data to [0,1] range for better overlay visualization\n",
    "                        data_col = plot_data[channel].dropna()\n",
    "                        if len(data_col) > 0:\n",
    "                            # Normalize to 0-1 range, then shift by channel index\n",
    "                            data_min, data_max = data_col.min(), data_col.max()\n",
    "                            if data_max > data_min:\n",
    "                                normalized_data = (data_col - data_min) / (data_max - data_min) + i\n",
    "                            else:\n",
    "                                normalized_data = data_col + i\n",
    "                            \n",
    "                            ax.plot(data_col.index, normalized_data, \n",
    "                                   label=channel, alpha=0.8, linewidth=1)\n",
    "                \n",
    "                axes = [ax]\n",
    "                \n",
    "            else:\n",
    "                # Subplots for each channel\n",
    "                fig, axes = plt.subplots(len(selected_channels), 1, \n",
    "                                       figsize=(16, 3*len(selected_channels)), \n",
    "                                       sharex=True)\n",
    "                if len(selected_channels) == 1:\n",
    "                    axes = [axes]\n",
    "                \n",
    "                for i, (ax, channel) in enumerate(zip(axes, selected_channels)):\n",
    "                    if channel in plot_data.columns:\n",
    "                        data_col = plot_data[channel].dropna()\n",
    "                        if len(data_col) > 0:\n",
    "                            ax.plot(data_col.index, data_col, \n",
    "                                   color=f'C{i}', alpha=0.8, linewidth=1)\n",
    "                            ax.set_ylabel(channel.split('_')[-1])  # Use last part as ylabel\n",
    "                            ax.set_title(channel)\n",
    "                            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add labels to all subplots\n",
    "            visible_activity_labels = []\n",
    "            if show_labels.value and len(non_empty_labels) > 0:\n",
    "                selected_label_types = list(label_filter.value)\n",
    "                \n",
    "                # Find labels in the plot window\n",
    "                plot_labels_data = plot_data[plot_data[label_column].isin(selected_label_types)]\n",
    "                \n",
    "                if len(plot_labels_data) > 0:\n",
    "                    # Track visible labels for legend\n",
    "                    visible_activity_labels = list(set(plot_labels_data[label_column].values))\n",
    "                    visible_activity_labels = [label for label in visible_activity_labels if label in selected_label_types]\n",
    "                    \n",
    "                    # Group consecutive timestamps with same label\n",
    "                    label_segments = []\n",
    "                    current_label = None\n",
    "                    segment_start = None\n",
    "                    \n",
    "                    for timestamp, row in plot_labels_data.iterrows():\n",
    "                        label = row[label_column]\n",
    "                        if label != current_label:\n",
    "                            if current_label is not None:\n",
    "                                label_segments.append((current_label, segment_start, timestamp))\n",
    "                            current_label = label\n",
    "                            segment_start = timestamp\n",
    "                    \n",
    "                    # Add final segment\n",
    "                    if current_label is not None:\n",
    "                        label_segments.append((current_label, segment_start, plot_labels_data.index[-1]))\n",
    "                    \n",
    "                    # Add shaded regions to all axes\n",
    "                    for ax in axes:\n",
    "                        y_min, y_max = ax.get_ylim()\n",
    "                        \n",
    "                        for label_name, start_time, end_time in label_segments:\n",
    "                            if label_name in selected_label_types:\n",
    "                                color = label_colors.get(label_name, 'gray')\n",
    "                                ax.axvspan(start_time, end_time, \n",
    "                                         alpha=label_alpha.value, \n",
    "                                         color=color,\n",
    "                                         zorder=0)\n",
    "                                \n",
    "                                # Add label text for longer segments\n",
    "                                duration = end_time - start_time\n",
    "                                if duration > pd.Timedelta(minutes=1):\n",
    "                                    mid_time = start_time + duration / 2\n",
    "                                    y_pos = y_max - (y_max - y_min) * 0.05\n",
    "                                    ax.text(mid_time, y_pos, label_name, \n",
    "                                           ha='center', va='top', rotation=0,\n",
    "                                           fontsize=9, alpha=0.9,\n",
    "                                           bbox=dict(boxstyle='round,pad=0.2', \n",
    "                                                   facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    print(f\"🏷️ Showing {len(label_segments)} label segments\")\n",
    "            \n",
    "            # Format all axes\n",
    "            for ax in axes:\n",
    "                ax.set_xlim(plot_start, plot_end)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "                ax.xaxis.set_major_locator(mdates.MinuteLocator(interval=max(1, window_mins//10)))\n",
    "                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "            \n",
    "            # Add legends\n",
    "            legend_elements = []\n",
    "            \n",
    "            # Add sensor channel legend for overlay mode\n",
    "            if plot_style.value == 'overlay' and len(selected_channels) <= 15:\n",
    "                sensor_legend = axes[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left', \n",
    "                                             fontsize=8, title='Sensor Channels')\n",
    "                axes[0].add_artist(sensor_legend)  # Keep this legend when adding activity legend\n",
    "            \n",
    "            # Add activity labels legend if showing labels\n",
    "            if show_labels.value and len(non_empty_labels) > 0 and visible_activity_labels:\n",
    "                # Create legend handles for visible activity labels\n",
    "                activity_handles = []\n",
    "                for label_name in sorted(visible_activity_labels):\n",
    "                    color = label_colors.get(label_name, 'gray')\n",
    "                    activity_handles.append(Patch(facecolor=color, alpha=label_alpha.value, \n",
    "                                                label=label_name))\n",
    "                \n",
    "                # Position activity legend\n",
    "                if plot_style.value == 'overlay':\n",
    "                    # Place below sensor legend if both exist\n",
    "                    y_pos = 0.7 if len(selected_channels) <= 15 else 1.0\n",
    "                    activity_legend = axes[0].legend(handles=activity_handles, \n",
    "                                                    bbox_to_anchor=(1.02, y_pos), \n",
    "                                                    loc='upper left', fontsize=8, \n",
    "                                                    title='Activities')\n",
    "                else:\n",
    "                    # Place on first subplot for subplots mode\n",
    "                    activity_legend = axes[0].legend(handles=activity_handles, \n",
    "                                                    bbox_to_anchor=(1.02, 1), \n",
    "                                                    loc='upper left', fontsize=8, \n",
    "                                                    title='Activities')\n",
    "                \n",
    "                print(f\"🎨 Added legend for {len(visible_activity_labels)} visible activities\")\n",
    "            \n",
    "            # Title and layout\n",
    "            title = f'Combined Data Visualization - {plot_style.value.title()} Mode\\n'\n",
    "            title += f'Window: {plot_start.strftime(\"%H:%M:%S\")} to {plot_end.strftime(\"%H:%M:%S\")} '\n",
    "            title += f'({len(selected_channels)} channels, {len(plot_data)} samples)'\n",
    "            \n",
    "            plt.suptitle(title, fontsize=14, y=0.98)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Adjust layout to accommodate legends\n",
    "            has_sensor_legend = plot_style.value == 'overlay' and len(selected_channels) <= 15\n",
    "            has_activity_legend = show_labels.value and len(non_empty_labels) > 0 and visible_activity_labels\n",
    "            \n",
    "            if plot_style.value == 'overlay':\n",
    "                if has_sensor_legend and has_activity_legend:\n",
    "                    plt.subplots_adjust(right=0.75, top=0.92)  # Space for both legends\n",
    "                elif has_sensor_legend or has_activity_legend:\n",
    "                    plt.subplots_adjust(right=0.85, top=0.92)  # Space for one legend\n",
    "                else:\n",
    "                    plt.subplots_adjust(top=0.92)  # No legends\n",
    "            else:\n",
    "                if has_activity_legend:\n",
    "                    plt.subplots_adjust(right=0.85, top=0.95)  # Space for activity legend\n",
    "                else:\n",
    "                    plt.subplots_adjust(top=0.95)  # No legends\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error creating plot: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Navigation functions\n",
    "def navigate_backward_5min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time - pd.Timedelta(minutes=5)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def navigate_forward_5min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time + pd.Timedelta(minutes=5)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def navigate_backward_30min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time - pd.Timedelta(minutes=30)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def navigate_forward_30min(btn):\n",
    "    current_time = get_center_time()\n",
    "    new_time = current_time + pd.Timedelta(minutes=30)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def jump_to_data_start(btn):\n",
    "    new_time = data_start + pd.Timedelta(minutes=window_minutes.value/2)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def jump_to_data_center(btn):\n",
    "    update_center_time(data_center)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def jump_to_data_end(btn):\n",
    "    new_time = data_end - pd.Timedelta(minutes=window_minutes.value/2)\n",
    "    update_center_time(new_time)\n",
    "    if auto_plot.value:\n",
    "        plot_data(None)\n",
    "\n",
    "def jump_to_sync_start(btn):\n",
    "    \"\"\"Jump to sync start time\"\"\"\n",
    "    if sync_start_time is not None:\n",
    "        print(f\"🎯 Jumping to sync start: {sync_start_time}\")\n",
    "        \n",
    "        # Check if sync time is within data bounds\n",
    "        if not (data_start <= sync_start_time <= data_end):\n",
    "            print(f\"⚠️ Warning: Sync start time is outside data range!\")\n",
    "            print(f\"   Data range: {data_start} to {data_end}\")\n",
    "            print(f\"   Sync time: {sync_start_time}\")\n",
    "            \n",
    "            # Find closest available time\n",
    "            if sync_start_time < data_start:\n",
    "                print(f\"   Using data start instead: {data_start}\")\n",
    "                update_center_time(data_start)\n",
    "            else:\n",
    "                print(f\"   Using data end instead: {data_end}\")\n",
    "                update_center_time(data_end)\n",
    "        else:\n",
    "            # Check for data availability around sync time\n",
    "            window = pd.Timedelta(minutes=window_minutes.value/2)\n",
    "            mask = (combined_data.index >= sync_start_time - window) & (combined_data.index <= sync_start_time + window)\n",
    "            data_count = combined_data[mask].shape[0]\n",
    "            print(f\"   Data available in ±{window_minutes.value/2}min window: {data_count} samples\")\n",
    "            \n",
    "            update_center_time(sync_start_time)\n",
    "        \n",
    "        if auto_plot.value:\n",
    "            plot_data(None)\n",
    "\n",
    "def jump_to_sync_end(btn):\n",
    "    \"\"\"Jump to sync end time\"\"\"\n",
    "    if sync_end_time is not None:\n",
    "        print(f\"🎯 Jumping to sync end: {sync_end_time}\")\n",
    "        \n",
    "        # Check if sync time is within data bounds\n",
    "        if not (data_start <= sync_end_time <= data_end):\n",
    "            print(f\"⚠️ Warning: Sync end time is outside data range!\")\n",
    "            print(f\"   Data range: {data_start} to {data_end}\")\n",
    "            print(f\"   Sync time: {sync_end_time}\")\n",
    "            \n",
    "            # Find closest available time\n",
    "            if sync_end_time < data_start:\n",
    "                print(f\"   Using data start instead: {data_start}\")\n",
    "                update_center_time(data_start)\n",
    "            else:\n",
    "                print(f\"   Using data end instead: {data_end}\")\n",
    "                update_center_time(data_end)\n",
    "        else:\n",
    "            # Check for data availability around sync time\n",
    "            window = pd.Timedelta(minutes=window_minutes.value/2)\n",
    "            mask = (combined_data.index >= sync_end_time - window) & (combined_data.index <= sync_end_time + window)\n",
    "            data_count = combined_data[mask].shape[0]\n",
    "            print(f\"   Data available in ±{window_minutes.value/2}min window: {data_count} samples\")\n",
    "            \n",
    "            update_center_time(sync_end_time)\n",
    "        \n",
    "        if auto_plot.value:\n",
    "            plot_data(None)\n",
    "\n",
    "# Connect buttons\n",
    "plot_button.on_click(plot_data)\n",
    "nav_backward_5min.on_click(navigate_backward_5min)\n",
    "nav_forward_5min.on_click(navigate_forward_5min)\n",
    "nav_backward_30min.on_click(navigate_backward_30min)\n",
    "nav_forward_30min.on_click(navigate_forward_30min)\n",
    "jump_data_start.on_click(jump_to_data_start)\n",
    "jump_data_center.on_click(jump_to_data_center)\n",
    "jump_data_end.on_click(jump_to_data_end)\n",
    "\n",
    "# Connect sync jump buttons if they exist\n",
    "if sync_start_time is not None:\n",
    "    jump_sync_start.on_click(jump_to_sync_start)\n",
    "if sync_end_time is not None:\n",
    "    jump_sync_end.on_click(jump_to_sync_end)\n",
    "\n",
    "# Initialize channel selection\n",
    "update_channel_selection()\n",
    "\n",
    "print(\"✅ Interactive controls ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0368e4",
   "metadata": {},
   "source": [
    "## 5. Interactive Control Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "594e6cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎛️ Interactive Control Panel\n",
      "Use the controls below to explore your combined sensor data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e108bdded3aa4bd797bee17fd735d87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🎛️ Combined Data Interactive Visualizer</h3>'), HBox(children=(VBox(children=(H…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Interactive visualizer ready!\n",
      "\n",
      "📝 Instructions:\n",
      "  1. Select sensor groups to focus on specific sensor types\n",
      "  2. Choose individual channels to plot\n",
      "  3. Set time window and center time\n",
      "  4. Use navigation buttons to move through data\n",
      "  5. Toggle labels and adjust transparency\n",
      "  6. Choose between overlay or subplot visualization\n",
      "  7. Use sync jump buttons to quickly navigate to sync events\n",
      "\n",
      "💡 Tips:\n",
      "  • Use auto-plot for seamless navigation\n",
      "  • Overlay mode normalizes data for comparison\n",
      "  • Subplot mode shows actual data values\n",
      "  • Labels appear as shaded background areas\n",
      "  • Sync buttons jump directly to synchronization events\n",
      "  • Click 'Plot Data' to refresh manually\n"
     ]
    }
   ],
   "source": [
    "# Create the interactive control panel layout\n",
    "print(\"🎛️ Interactive Control Panel\")\n",
    "print(\"Use the controls below to explore your combined sensor data\")\n",
    "\n",
    "# Layout controls in organized sections\n",
    "sensor_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>📊 Sensor Selection</h4>\"),\n",
    "    sensor_group_selection,\n",
    "    channel_selection,\n",
    "    plot_style\n",
    "])\n",
    "\n",
    "label_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>🏷️ Label Controls</h4>\"),\n",
    "    show_labels,\n",
    "    label_alpha,\n",
    "    label_filter\n",
    "]) if len(non_empty_labels) > 0 else widgets.HTML(\"<p>No labels available</p>\")\n",
    "\n",
    "time_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>⏱️ Time Navigation</h4>\"),\n",
    "    center_time_text,\n",
    "    window_minutes,\n",
    "    widgets.HTML(\"<b>Fine Navigation:</b>\"),\n",
    "    widgets.HBox([nav_backward_5min, nav_forward_5min]),\n",
    "    widgets.HTML(\"<b>Coarse Navigation:</b>\"),\n",
    "    widgets.HBox([nav_backward_30min, nav_forward_30min]),\n",
    "    widgets.HTML(\"<b>Quick Jumps:</b>\"),\n",
    "    widgets.HBox([jump_data_start, jump_data_center, jump_data_end]),\n",
    "    widgets.HTML(\"<b>Sync Events:</b>\") if sync_jump_buttons else widgets.HTML(\"\"),\n",
    "    widgets.HBox(sync_jump_buttons) if sync_jump_buttons else widgets.HTML(\"\"),\n",
    "    auto_plot\n",
    "])\n",
    "\n",
    "plot_controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>📈 Plot Controls</h4>\"),\n",
    "    plot_button,\n",
    "    widgets.HTML(\"<p><b>Plot Styles:</b></p>\"\n",
    "                 \"<p>• <b>Overlay</b>: All channels on one plot (normalized)</p>\"\n",
    "                 \"<p>• <b>Subplots</b>: Each channel in separate subplot</p>\")\n",
    "])\n",
    "\n",
    "# Combine all controls\n",
    "all_controls = widgets.HBox([\n",
    "    sensor_controls,\n",
    "    label_controls,\n",
    "    time_controls,\n",
    "    plot_controls\n",
    "])\n",
    "\n",
    "# Display the interface\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🎛️ Combined Data Interactive Visualizer</h3>\"),\n",
    "    all_controls,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    plot_output\n",
    "]))\n",
    "\n",
    "print(\"\\n🚀 Interactive visualizer ready!\")\n",
    "print(\"\\n📝 Instructions:\")\n",
    "print(\"  1. Select sensor groups to focus on specific sensor types\")\n",
    "print(\"  2. Choose individual channels to plot\")\n",
    "print(\"  3. Set time window and center time\")\n",
    "print(\"  4. Use navigation buttons to move through data\")\n",
    "print(\"  5. Toggle labels and adjust transparency\")\n",
    "print(\"  6. Choose between overlay or subplot visualization\")\n",
    "if sync_jump_buttons:\n",
    "    print(\"  7. Use sync jump buttons to quickly navigate to sync events\")\n",
    "print(\"\\n💡 Tips:\")\n",
    "print(\"  • Use auto-plot for seamless navigation\")\n",
    "print(\"  • Overlay mode normalizes data for comparison\")\n",
    "print(\"  • Subplot mode shows actual data values\")\n",
    "print(\"  • Labels appear as shaded background areas\")\n",
    "if sync_jump_buttons:\n",
    "    print(\"  • Sync buttons jump directly to synchronization events\")\n",
    "print(\"  • Click 'Plot Data' to refresh manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ad1c6",
   "metadata": {},
   "source": [
    "## 6. Data Statistics and Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20c5cdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMBINED DATA QUALITY REPORT ===\n",
      "\n",
      "⏱️ Temporal Statistics:\n",
      "  📅 Start time: 2024-07-26 10:06:09.261336565\n",
      "  📅 End time: 2024-07-28 11:12:59.994135618\n",
      "  ⏱️ Total duration: 2 days 01:06:50.732799053\n",
      "  📊 Total samples: 4,420,268\n",
      "  🔄 Actual sampling frequency: 25.00 Hz\n",
      "  📏 Time resolution: 0 days 00:00:00.040000002\n",
      "\n",
      "📈 Sensor Statistics:\n",
      "  🔧 Total sensor groups: 11\n",
      "  📊 Total sensor channels: 29\n",
      "    📊 corsano_wrist_wrist_acc:\n",
      "      Channels: 3\n",
      "      Coverage: 85.3%\n",
      "      Missing values: 1,947,960\n",
      "      Value range: [-3746.000, 4088.000]\n",
      "      Mean ± Std: 135.376 ± 227.108\n",
      "    📊 cosinuss_ear_ear_acc:\n",
      "      Channels: 3\n",
      "      Coverage: 11.2%\n",
      "      Missing values: 11,781,309\n",
      "      Value range: [-2.285, 1.560]\n",
      "      Mean ± Std: -0.249 ± 0.321\n",
      "    📊 mbient_acc_x_axis:\n",
      "      Channels: 1\n",
      "      Coverage: 100.0%\n",
      "      Missing values: 0\n",
      "      Value range: [-3.990, 2.832]\n",
      "      Mean ± Std: 0.557 ± 0.502\n",
      "    📊 mbient_acc_y_axis:\n",
      "      Channels: 1\n",
      "      Coverage: 100.0%\n",
      "      Missing values: 0\n",
      "      Value range: [-3.640, 2.977]\n",
      "      Mean ± Std: -0.045 ± 0.582\n",
      "    📊 mbient_acc_z_axis:\n",
      "      Channels: 1\n",
      "      Coverage: 100.0%\n",
      "      Missing values: 0\n",
      "      Value range: [-2.785, 3.458]\n",
      "      Mean ± Std: 0.125 ± 0.309\n",
      "    📊 mbient_gyro_x_axis:\n",
      "      Channels: 1\n",
      "      Coverage: 100.0%\n",
      "      Missing values: 0\n",
      "      Value range: [-652.988, 948.323]\n",
      "      Mean ± Std: -0.177 ± 4.545\n",
      "    📊 mbient_gyro_y_axis:\n",
      "      Channels: 1\n",
      "      Coverage: 100.0%\n",
      "      Missing values: 0\n",
      "      Value range: [-368.659, 478.750]\n",
      "      Mean ± Std: 0.355 ± 3.959\n",
      "    📊 mbient_gyro_z_axis:\n",
      "      Channels: 1\n",
      "      Coverage: 100.0%\n",
      "      Missing values: 0\n",
      "      Value range: [-432.287, 650.091]\n",
      "      Mean ± Std: -3.082 ± 29.973\n",
      "    📊 vivalnk_acc_vivalnk_acc:\n",
      "      Channels: 3\n",
      "      Coverage: 99.7%\n",
      "      Missing values: 39,720\n",
      "      Value range: [-17257.000, 7445.000]\n",
      "      Mean ± Std: 446.343 ± 830.113\n",
      "    📊 sensomative_bottom_bottom_value:\n",
      "      Channels: 11\n",
      "      Coverage: 73.4%\n",
      "      Missing values: 12,955,943\n",
      "      Value range: [0.000, 255.000]\n",
      "      Mean ± Std: 8.424 ± 16.385\n",
      "    📊 corsano_bioz_bioz_acc:\n",
      "      Channels: 3\n",
      "      Coverage: 93.5%\n",
      "      Missing values: 866,370\n",
      "      Value range: [-3254.000, 3308.000]\n",
      "      Mean ± Std: 118.447 ± 230.698\n",
      "\n",
      "🏷️ Label Statistics:\n",
      "  📋 Unique activities: 42\n",
      "  📊 Labeled samples: 3,732,432 (84.4%)\n",
      "  📊 Unlabeled samples: 687,836 (15.6%)\n",
      "\n",
      "🏷️ Activity Durations:\n",
      "    🏷️ black_video: 1401.8 min (47.6%)\n",
      "    🏷️ wheelchair _in_storage: 695.1 min (23.6%)\n",
      "    🏷️ conversation: 63.0 min (2.1%)\n",
      "    🏷️ self_propulsion: 43.0 min (1.5%)\n",
      "    🏷️ waiting: 36.9 min (1.3%)\n",
      "    🏷️ handling: 33.3 min (1.1%)\n",
      "    🏷️ sitting_table: 29.3 min (1.0%)\n",
      "    🏷️ watching_tv: 28.7 min (1.0%)\n",
      "    🏷️ talking_phone: 27.8 min (0.9%)\n",
      "    🏷️ assisted_propulsion: 25.1 min (0.9%)\n",
      "    🏷️ eating: 19.6 min (0.7%)\n",
      "    🏷️ using_phone: 11.6 min (0.4%)\n",
      "    🏷️ driving_train: 9.6 min (0.3%)\n",
      "    🏷️ reading: 8.4 min (0.3%)\n",
      "    🏷️ sync_event: 8.4 min (0.3%)\n",
      "    ... and 27 more activities\n",
      "\n",
      "🔄 Label Transitions: 483\n",
      "  📊 Average segment length: 9152 samples (366.1s)\n",
      "\n",
      "💾 Memory Usage:\n",
      "  📊 DataFrame size: 1302.4 MB\n",
      "  💽 File size: 1032.8 MB\n",
      "  📈 Compression ratio: 1.3x\n",
      "\n",
      "✅ Overall Data Quality:\n",
      "  📊 Sensor data coverage: 78.48%\n",
      "  🏷️ Label coverage: 84.44%\n",
      "  ⏱️ Temporal consistency: ✅ Regular 25.0Hz sampling\n",
      "  🔄 Synchronization: ✅ All sensors aligned\n",
      "  ❌ Poor data quality. Significant missing data detected.\n",
      "\n",
      "📁 Data ready for use! Load with:\n",
      "import pickle\n",
      "with open('/scai_data3/scratch/stirnimann_r/results/OutSense-713/OutSense-713_combined_data.pkl', 'rb') as f:\n",
      "    data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Generate detailed statistics about the combined data\n",
    "print(\"=== COMBINED DATA QUALITY REPORT ===\")\n",
    "\n",
    "# Time statistics\n",
    "sampling_freq = len(combined_data) / (data_end - data_start).total_seconds()\n",
    "print(f\"\\n⏱️ Temporal Statistics:\")\n",
    "print(f\"  📅 Start time: {data_start}\")\n",
    "print(f\"  📅 End time: {data_end}\")\n",
    "print(f\"  ⏱️ Total duration: {data_end - data_start}\")\n",
    "print(f\"  📊 Total samples: {len(combined_data):,}\")\n",
    "print(f\"  🔄 Actual sampling frequency: {sampling_freq:.2f} Hz\")\n",
    "print(f\"  📏 Time resolution: {(data_end - data_start) / len(combined_data)}\")\n",
    "\n",
    "# Sensor statistics\n",
    "print(f\"\\n📈 Sensor Statistics:\")\n",
    "print(f\"  🔧 Total sensor groups: {len(sensor_groups)}\")\n",
    "print(f\"  📊 Total sensor channels: {len(sensor_columns)}\")\n",
    "\n",
    "for sensor_name, channels in sensor_groups.items():\n",
    "    # Calculate statistics for this sensor group\n",
    "    sensor_data = combined_data[channels]\n",
    "    total_values = sensor_data.size\n",
    "    missing_values = sensor_data.isnull().sum().sum()\n",
    "    coverage = (total_values - missing_values) / total_values * 100\n",
    "    \n",
    "    print(f\"    📊 {sensor_name}:\")\n",
    "    print(f\"      Channels: {len(channels)}\")\n",
    "    print(f\"      Coverage: {coverage:.1f}%\")\n",
    "    print(f\"      Missing values: {missing_values:,}\")\n",
    "    \n",
    "    # Value ranges\n",
    "    numeric_data = sensor_data.select_dtypes(include=[np.number])\n",
    "    if not numeric_data.empty:\n",
    "        overall_min = numeric_data.min().min()\n",
    "        overall_max = numeric_data.max().max()\n",
    "        overall_mean = numeric_data.mean().mean()\n",
    "        overall_std = numeric_data.std().mean()\n",
    "        \n",
    "        print(f\"      Value range: [{overall_min:.3f}, {overall_max:.3f}]\")\n",
    "        print(f\"      Mean ± Std: {overall_mean:.3f} ± {overall_std:.3f}\")\n",
    "\n",
    "# Label statistics\n",
    "if len(non_empty_labels) > 0:\n",
    "    print(f\"\\n🏷️ Label Statistics:\")\n",
    "    total_labeled = (combined_data[label_column] != '').sum()\n",
    "    label_coverage = total_labeled / len(combined_data) * 100\n",
    "    \n",
    "    print(f\"  📋 Unique activities: {len(non_empty_labels)}\")\n",
    "    print(f\"  📊 Labeled samples: {total_labeled:,} ({label_coverage:.1f}%)\")\n",
    "    print(f\"  📊 Unlabeled samples: {len(combined_data) - total_labeled:,} ({100-label_coverage:.1f}%)\")\n",
    "    \n",
    "    # Calculate label durations\n",
    "    print(f\"\\n🏷️ Activity Durations:\")\n",
    "    for label, count in non_empty_labels.head(15).items():\n",
    "        duration_seconds = count / sampling_freq\n",
    "        duration_minutes = duration_seconds / 60\n",
    "        percentage = count / len(combined_data) * 100\n",
    "        \n",
    "        if duration_minutes >= 1:\n",
    "            print(f\"    🏷️ {label}: {duration_minutes:.1f} min ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"    🏷️ {label}: {duration_seconds:.1f} sec ({percentage:.1f}%)\")\n",
    "    \n",
    "    if len(non_empty_labels) > 15:\n",
    "        remaining = len(non_empty_labels) - 15\n",
    "        print(f\"    ... and {remaining} more activities\")\n",
    "        \n",
    "    # Label transitions\n",
    "    label_changes = (combined_data[label_column] != combined_data[label_column].shift()).sum()\n",
    "    print(f\"\\n🔄 Label Transitions: {label_changes:,}\")\n",
    "    avg_segment_length = len(combined_data) / label_changes if label_changes > 0 else 0\n",
    "    avg_segment_duration = avg_segment_length / sampling_freq\n",
    "    print(f\"  📊 Average segment length: {avg_segment_length:.0f} samples ({avg_segment_duration:.1f}s)\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n🏷️ Label Statistics: No labels available\")\n",
    "\n",
    "# Memory usage\n",
    "memory_usage = combined_data.memory_usage(deep=True).sum()\n",
    "memory_mb = memory_usage / (1024 * 1024)\n",
    "print(f\"\\n💾 Memory Usage:\")\n",
    "print(f\"  📊 DataFrame size: {memory_mb:.1f} MB\")\n",
    "print(f\"  💽 File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"  📈 Compression ratio: {memory_mb/file_size_mb:.1f}x\")\n",
    "\n",
    "# Data quality summary\n",
    "total_missing = combined_data[sensor_columns].isnull().sum().sum()\n",
    "total_sensor_values = len(combined_data) * len(sensor_columns)\n",
    "overall_coverage = (total_sensor_values - total_missing) / total_sensor_values * 100\n",
    "\n",
    "print(f\"\\n✅ Overall Data Quality:\")\n",
    "print(f\"  📊 Sensor data coverage: {overall_coverage:.2f}%\")\n",
    "print(f\"  🏷️ Label coverage: {label_coverage:.2f}%\" if len(non_empty_labels) > 0 else \"  🏷️ Label coverage: 0%\")\n",
    "print(f\"  ⏱️ Temporal consistency: ✅ Regular {sampling_freq:.1f}Hz sampling\")\n",
    "print(f\"  🔄 Synchronization: ✅ All sensors aligned\")\n",
    "\n",
    "if overall_coverage > 95:\n",
    "    print(f\"  🌟 Excellent data quality! Ready for AI preprocessing.\")\n",
    "elif overall_coverage > 90:\n",
    "    print(f\"  ✅ Good data quality. Minor gaps acceptable for most applications.\")\n",
    "elif overall_coverage > 80:\n",
    "    print(f\"  ⚠️ Moderate data quality. Consider gap filling strategies.\")\n",
    "else:\n",
    "    print(f\"  ❌ Poor data quality. Significant missing data detected.\")\n",
    "\n",
    "print(f\"\\n📁 Data ready for use! Load with:\")\n",
    "print(f\"import pickle\")\n",
    "print(f\"with open('{combined_data_path}', 'rb') as f:\")\n",
    "print(f\"    data = pickle.load(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6fc96",
   "metadata": {},
   "source": [
    "## 7. Sync Events Data Availability Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "610d5bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYNC EVENTS DATA AVAILABILITY CHECK ===\n",
      "🎯 Checking data availability around sync events for OutSense-713\n",
      "📅 Sync Start: 2024-07-26 10:55:00\n",
      "📅 Sync End: 2024-07-28 10:13:00\n",
      "⏱️ Sync Duration: 1 days 23:18:00\n",
      "\n",
      "📊 Combined Data Time Bounds:\n",
      "  📅 Data Start: 2024-07-26 10:06:09.261336565\n",
      "  📅 Data End: 2024-07-28 11:12:59.994135618\n",
      "  ⏱️ Data Duration: 2 days 01:06:50.732799053\n",
      "\n",
      "🔍 Sync Event Coverage:\n",
      "  🎯 Sync Start in data range: ✅ True\n",
      "  🎯 Sync End in data range: ✅ True\n",
      "\n",
      "📊 Data availability around Sync Start (2024-07-26 10:55:00):\n",
      "    ✅ ±1min window: 1500 samples, 100.0% sensor data, 73.3% labeled\n",
      "        Time range: 2024-07-26 10:54:30 to 2024-07-26 10:55:30\n",
      "        First data: 2024-07-26 10:54:30.022202787\n",
      "        Last data: 2024-07-26 10:55:29.982220692\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 29/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'cosinuss_ear_ear_acc_x', 'cosinuss_ear_ear_acc_y']...)\n",
      "    ✅ ±5min window: 7500 samples, 95.7% sensor data, 54.7% labeled\n",
      "        Time range: 2024-07-26 10:52:30 to 2024-07-26 10:57:30\n",
      "        First data: 2024-07-26 10:52:30.022166953\n",
      "        Last data: 2024-07-26 10:57:29.982256526\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 29/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'cosinuss_ear_ear_acc_x', 'cosinuss_ear_ear_acc_y']...)\n",
      "    ✅ ±10min window: 15000 samples, 91.5% sensor data, 52.3% labeled\n",
      "        Time range: 2024-07-26 10:50:00 to 2024-07-26 11:00:00\n",
      "        First data: 2024-07-26 10:50:00.022122160\n",
      "        Last data: 2024-07-26 10:59:59.982301319\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 29/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'cosinuss_ear_ear_acc_x', 'cosinuss_ear_ear_acc_y']...)\n",
      "    ✅ ±30min window: 45000 samples, 83.7% sensor data, 24.8% labeled\n",
      "        Time range: 2024-07-26 10:40:00 to 2024-07-26 11:10:00\n",
      "        First data: 2024-07-26 10:40:00.021942988\n",
      "        Last data: 2024-07-26 11:09:59.982480491\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 29/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'cosinuss_ear_ear_acc_x', 'cosinuss_ear_ear_acc_y']...)\n",
      "\n",
      "📊 Data availability around Sync End (2024-07-28 10:13:00):\n",
      "    ✅ ±1min window: 1500 samples, 89.7% sensor data, 99.1% labeled\n",
      "        Time range: 2024-07-28 10:12:30 to 2024-07-28 10:13:30\n",
      "        First data: 2024-07-28 10:12:30.033051642\n",
      "        Last data: 2024-07-28 10:13:29.993069547\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 26/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'mbient_acc_x_axis_g', 'mbient_acc_y_axis_g']...)\n",
      "    ✅ ±5min window: 7500 samples, 87.3% sensor data, 62.9% labeled\n",
      "        Time range: 2024-07-28 10:10:30 to 2024-07-28 10:15:30\n",
      "        First data: 2024-07-28 10:10:30.033015808\n",
      "        Last data: 2024-07-28 10:15:29.993105381\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 26/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'mbient_acc_x_axis_g', 'mbient_acc_y_axis_g']...)\n",
      "    ✅ ±10min window: 15000 samples, 83.3% sensor data, 56.5% labeled\n",
      "        Time range: 2024-07-28 10:08:00 to 2024-07-28 10:18:00\n",
      "        First data: 2024-07-28 10:08:00.032971015\n",
      "        Last data: 2024-07-28 10:17:59.993150174\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 26/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'mbient_acc_x_axis_g', 'mbient_acc_y_axis_g']...)\n",
      "    ✅ ±30min window: 45000 samples, 80.8% sensor data, 52.2% labeled\n",
      "        Time range: 2024-07-28 09:58:00 to 2024-07-28 10:28:00\n",
      "        First data: 2024-07-28 09:58:00.032791843\n",
      "        Last data: 2024-07-28 10:27:59.993329346\n",
      "        Sampling: avg gap 0 days 00:00:00.040000011, max gap 0 days 00:00:00.040000012\n",
      "        Available sensors: 26/29 (['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z', 'mbient_acc_x_axis_g', 'mbient_acc_y_axis_g']...)\n",
      "\n",
      "🌍 Timezone and Format Analysis:\n",
      "  📅 Sync Start timezone: None\n",
      "  📅 Data Start timezone: None\n",
      "  📅 Data timezone info: None\n",
      "\n",
      "🔧 Testing Alternative Time Interpretations:\n",
      "    🎯 With -12h offset: Start in range: False, End in range: True\n",
      "      📊 Adjusted End (2024-07-27 22:13:00): 15000 samples in ±5min window\n",
      "    🎯 With -6h offset: Start in range: False, End in range: True\n",
      "      📊 Adjusted End (2024-07-28 04:13:00): 15000 samples in ±5min window\n",
      "    🎯 With -3h offset: Start in range: False, End in range: True\n",
      "      📊 Adjusted End (2024-07-28 07:13:00): 15000 samples in ±5min window\n",
      "    🎯 With -1h offset: Start in range: False, End in range: True\n",
      "      📊 Adjusted End (2024-07-28 09:13:00): 15000 samples in ±5min window\n",
      "    🎯 With +0h offset: Start in range: True, End in range: True\n",
      "      📊 Adjusted Start (2024-07-26 10:55:00): 15000 samples in ±5min window\n",
      "      📊 Adjusted End (2024-07-28 10:13:00): 15000 samples in ±5min window\n",
      "    🎯 With +1h offset: Start in range: True, End in range: False\n",
      "      📊 Adjusted Start (2024-07-26 11:55:00): 15000 samples in ±5min window\n",
      "    🎯 With +3h offset: Start in range: True, End in range: False\n",
      "      📊 Adjusted Start (2024-07-26 13:55:00): 15000 samples in ±5min window\n",
      "    🎯 With +6h offset: Start in range: True, End in range: False\n",
      "      📊 Adjusted Start (2024-07-26 16:55:00): 15000 samples in ±5min window\n",
      "    🎯 With +12h offset: Start in range: True, End in range: False\n",
      "      📊 Adjusted Start (2024-07-26 22:55:00): 15000 samples in ±5min window\n",
      "\n",
      "🎯 Exact Timestamp Matching:\n",
      "  📍 Exact sync start match: ❌ False\n",
      "  📍 Exact sync end match: ❌ False\n",
      "  🎯 Closest to sync start: 2024-07-26 10:54:59.982211734 (diff: 0 days 00:00:00.017788266)\n",
      "  🎯 Closest to sync end: 2024-07-28 10:12:59.993060589 (diff: 0 days 00:00:00.006939411)\n",
      "    📊 Data at closest start timestamp: Available\n",
      "    📊 Sample values: [np.float64(120.0) np.float64(255.0) np.float64(-23.0) np.float64(0.0435)\n",
      " np.float64(0.0275)]\n",
      "    📊 Data at closest end timestamp: Available\n",
      "    📊 Sample values: [np.float64(-512.0) np.float64(41.0) np.float64(128.0) np.float64(nan)\n",
      " np.float64(nan)]\n",
      "\n",
      "💡 Recommendations:\n",
      "  ✅ Sync events are within data time range\n",
      "     - Data should be available for plotting around sync events\n",
      "     - Use the interactive visualizer to navigate to sync times\n",
      "  📊 To investigate further:\n",
      "     - Use the sync jump buttons in the interactive visualizer\n",
      "     - Check the original data files before preprocessing\n",
      "     - Verify the Data_Preprocessing script alignment logic\n"
     ]
    }
   ],
   "source": [
    "# Debug sync events data availability\n",
    "print(\"=== SYNC EVENTS DATA AVAILABILITY CHECK ===\")\n",
    "\n",
    "if sync_start_time is not None and sync_end_time is not None:\n",
    "    print(f\"🎯 Checking data availability around sync events for {SUBJECT_ID}\")\n",
    "    print(f\"📅 Sync Start: {sync_start_time}\")\n",
    "    print(f\"📅 Sync End: {sync_end_time}\")\n",
    "    print(f\"⏱️ Sync Duration: {sync_end_time - sync_start_time}\")\n",
    "    \n",
    "    # Check data time bounds\n",
    "    print(f\"\\n📊 Combined Data Time Bounds:\")\n",
    "    print(f\"  📅 Data Start: {data_start}\")\n",
    "    print(f\"  📅 Data End: {data_end}\")\n",
    "    print(f\"  ⏱️ Data Duration: {data_end - data_start}\")\n",
    "    \n",
    "    # Check if sync times fall within data bounds\n",
    "    sync_start_in_data = data_start <= sync_start_time <= data_end\n",
    "    sync_end_in_data = data_start <= sync_end_time <= data_end\n",
    "    \n",
    "    print(f\"\\n🔍 Sync Event Coverage:\")\n",
    "    print(f\"  🎯 Sync Start in data range: {'✅' if sync_start_in_data else '❌'} {sync_start_in_data}\")\n",
    "    print(f\"  🎯 Sync End in data range: {'✅' if sync_end_in_data else '❌'} {sync_end_in_data}\")\n",
    "    \n",
    "    if not sync_start_in_data:\n",
    "        if sync_start_time < data_start:\n",
    "            time_diff = data_start - sync_start_time\n",
    "            print(f\"    ⚠️ Sync start is {time_diff} before data start\")\n",
    "        else:\n",
    "            time_diff = sync_start_time - data_end\n",
    "            print(f\"    ⚠️ Sync start is {time_diff} after data end\")\n",
    "    \n",
    "    if not sync_end_in_data:\n",
    "        if sync_end_time < data_start:\n",
    "            time_diff = data_start - sync_end_time\n",
    "            print(f\"    ⚠️ Sync end is {time_diff} before data start\")\n",
    "        else:\n",
    "            time_diff = sync_end_time - data_end\n",
    "            print(f\"    ⚠️ Sync end is {time_diff} after data end\")\n",
    "    \n",
    "    # Check data availability around sync events with different window sizes\n",
    "    window_sizes = [1, 5, 10, 30]  # minutes\n",
    "    \n",
    "    for event_name, event_time in [(\"Sync Start\", sync_start_time), (\"Sync End\", sync_end_time)]:\n",
    "        print(f\"\\n📊 Data availability around {event_name} ({event_time}):\")\n",
    "        \n",
    "        for window_min in window_sizes:\n",
    "            half_window = pd.Timedelta(minutes=window_min/2)\n",
    "            window_start = event_time - half_window\n",
    "            window_end = event_time + half_window\n",
    "            \n",
    "            # Count data points in window\n",
    "            mask = (combined_data.index >= window_start) & (combined_data.index <= window_end)\n",
    "            data_in_window = combined_data[mask]\n",
    "            \n",
    "            # Check sensor data availability\n",
    "            sensor_data_in_window = data_in_window[sensor_columns]\n",
    "            non_null_count = sensor_data_in_window.notna().sum().sum()\n",
    "            total_possible = len(data_in_window) * len(sensor_columns)\n",
    "            coverage = (non_null_count / total_possible * 100) if total_possible > 0 else 0\n",
    "            \n",
    "            # Check label data availability\n",
    "            label_data_in_window = data_in_window[label_column] if label_column in data_in_window.columns else pd.Series()\n",
    "            labeled_count = (label_data_in_window != '').sum() if len(label_data_in_window) > 0 else 0\n",
    "            label_coverage = (labeled_count / len(data_in_window) * 100) if len(data_in_window) > 0 else 0\n",
    "            \n",
    "            status = \"✅\" if len(data_in_window) > 0 else \"❌\"\n",
    "            print(f\"    {status} ±{window_min}min window: {len(data_in_window)} samples, {coverage:.1f}% sensor data, {label_coverage:.1f}% labeled\")\n",
    "            \n",
    "            # Show exact time bounds that would be plotted\n",
    "            actual_start = max(window_start, data_start)\n",
    "            actual_end = min(window_end, data_end)\n",
    "            print(f\"        Time range: {actual_start} to {actual_end}\")\n",
    "            \n",
    "            if len(data_in_window) > 0:\n",
    "                # Show first and last timestamps with data\n",
    "                print(f\"        First data: {data_in_window.index.min()}\")\n",
    "                print(f\"        Last data: {data_in_window.index.max()}\")\n",
    "                \n",
    "                # Check for gaps in data around event\n",
    "                time_diffs = data_in_window.index.to_series().diff()\n",
    "                max_gap = time_diffs.max()\n",
    "                avg_gap = time_diffs.mean()\n",
    "                print(f\"        Sampling: avg gap {avg_gap}, max gap {max_gap}\")\n",
    "                \n",
    "                # Show sample of available sensor channels\n",
    "                available_sensors = sensor_data_in_window.dropna(axis=1, how='all').columns\n",
    "                print(f\"        Available sensors: {len(available_sensors)}/{len(sensor_columns)} ({list(available_sensors[:5])}{'...' if len(available_sensors) > 5 else ''})\")\n",
    "    \n",
    "    # Additional checks for timezone issues\n",
    "    print(f\"\\n🌍 Timezone and Format Analysis:\")\n",
    "    print(f\"  📅 Sync Start timezone: {sync_start_time.tz}\")\n",
    "    print(f\"  📅 Data Start timezone: {data_start.tz}\")\n",
    "    print(f\"  📅 Data timezone info: {combined_data.index.tz}\")\n",
    "    \n",
    "    # Try different time formats or offsets\n",
    "    print(f\"\\n🔧 Testing Alternative Time Interpretations:\")\n",
    "    \n",
    "    # Test if there's a timezone offset issue\n",
    "    for offset_hours in [-12, -6, -3, -1, 0, 1, 3, 6, 12]:\n",
    "        adjusted_sync_start = sync_start_time + pd.Timedelta(hours=offset_hours)\n",
    "        adjusted_sync_end = sync_end_time + pd.Timedelta(hours=offset_hours)\n",
    "        \n",
    "        start_in_range = data_start <= adjusted_sync_start <= data_end\n",
    "        end_in_range = data_start <= adjusted_sync_end <= data_end\n",
    "        \n",
    "        if start_in_range or end_in_range:\n",
    "            print(f\"    🎯 With {offset_hours:+}h offset: Start in range: {start_in_range}, End in range: {end_in_range}\")\n",
    "            \n",
    "            # Check data around adjusted times\n",
    "            for event_name, adjusted_time in [(\"Adjusted Start\", adjusted_sync_start), (\"Adjusted End\", adjusted_sync_end)]:\n",
    "                if data_start <= adjusted_time <= data_end:\n",
    "                    window = pd.Timedelta(minutes=5)\n",
    "                    mask = (combined_data.index >= adjusted_time - window) & (combined_data.index <= adjusted_time + window)\n",
    "                    data_count = combined_data[mask].shape[0]\n",
    "                    print(f\"      📊 {event_name} ({adjusted_time}): {data_count} samples in ±5min window\")\n",
    "    \n",
    "    # Check for exact timestamp matches\n",
    "    print(f\"\\n🎯 Exact Timestamp Matching:\")\n",
    "    exact_sync_start_match = sync_start_time in combined_data.index\n",
    "    exact_sync_end_match = sync_end_time in combined_data.index\n",
    "    \n",
    "    print(f\"  📍 Exact sync start match: {'✅' if exact_sync_start_match else '❌'} {exact_sync_start_match}\")\n",
    "    print(f\"  📍 Exact sync end match: {'✅' if exact_sync_end_match else '❌'} {exact_sync_end_match}\")\n",
    "    \n",
    "    # Find nearest timestamps\n",
    "    if len(combined_data) > 0:\n",
    "        # Find closest timestamps to sync events\n",
    "        time_diffs_start = abs(combined_data.index - sync_start_time)\n",
    "        time_diffs_end = abs(combined_data.index - sync_end_time)\n",
    "        \n",
    "        closest_start_idx = time_diffs_start.argmin()\n",
    "        closest_end_idx = time_diffs_end.argmin()\n",
    "        \n",
    "        closest_start_time = combined_data.index[closest_start_idx]\n",
    "        closest_end_time = combined_data.index[closest_end_idx]\n",
    "        \n",
    "        start_diff = abs(closest_start_time - sync_start_time)\n",
    "        end_diff = abs(closest_end_time - sync_end_time)\n",
    "        \n",
    "        print(f\"  🎯 Closest to sync start: {closest_start_time} (diff: {start_diff})\")\n",
    "        print(f\"  🎯 Closest to sync end: {closest_end_time} (diff: {end_diff})\")\n",
    "        \n",
    "        # Show data around closest timestamps\n",
    "        if start_diff <= pd.Timedelta(hours=1):\n",
    "            print(f\"    📊 Data at closest start timestamp: Available\")\n",
    "            sample_data = combined_data.loc[closest_start_time, sensor_columns[:5]]\n",
    "            print(f\"    📊 Sample values: {sample_data.values}\")\n",
    "        \n",
    "        if end_diff <= pd.Timedelta(hours=1):\n",
    "            print(f\"    📊 Data at closest end timestamp: Available\")\n",
    "            sample_data = combined_data.loc[closest_end_time, sensor_columns[:5]]\n",
    "            print(f\"    📊 Sample values: {sample_data.values}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No sync events available for analysis\")\n",
    "    print(\"This could mean:\")\n",
    "    print(\"  1. Sync events file doesn't exist\")\n",
    "    print(\"  2. No sync events found for this subject\")\n",
    "    print(\"  3. Sync events are not properly formatted\")\n",
    "\n",
    "# Recommend actions based on findings\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "if sync_start_time is not None and sync_end_time is not None:\n",
    "    if not (data_start <= sync_start_time <= data_end and data_start <= sync_end_time <= data_end):\n",
    "        print(\"  🔧 Sync events are outside data time range:\")\n",
    "        print(\"     - Check if sync events file has correct timestamps\")\n",
    "        print(\"     - Verify timezone consistency between sync events and sensor data\")\n",
    "        print(\"     - Consider if data preprocessing removed sync event periods\")\n",
    "        print(\"     - Check if subject ID matches between sync events and data files\")\n",
    "    else:\n",
    "        print(\"  ✅ Sync events are within data time range\")\n",
    "        print(\"     - Data should be available for plotting around sync events\")\n",
    "        print(\"     - Use the interactive visualizer to navigate to sync times\")\n",
    "\n",
    "print(\"  📊 To investigate further:\")\n",
    "print(\"     - Use the sync jump buttons in the interactive visualizer\")\n",
    "print(\"     - Check the original data files before preprocessing\")\n",
    "print(\"     - Verify the Data_Preprocessing script alignment logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c66dc",
   "metadata": {},
   "source": [
    "## 8. Quick Sync Event Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc6e9e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QUICK SYNC EVENT DATA TEST ===\n",
      "\n",
      "🎯 Testing Sync Start: 2024-07-26 10:55:00\n",
      "   In data range: ✅ True\n",
      "   ✅ ±1min window: 1500 samples available\n",
      "   ✅ ±5min window: 7500 samples available\n",
      "   ✅ ±10min window: 15000 samples available\n",
      "\n",
      "🎯 Testing Sync End: 2024-07-28 10:13:00\n",
      "   In data range: ✅ True\n",
      "   ✅ ±1min window: 1500 samples available\n",
      "   ✅ ±5min window: 7500 samples available\n",
      "   ✅ ±10min window: 15000 samples available\n",
      "\n",
      "✅ SYNC EVENT DATA TEST PASSED\n",
      "   Data is available around sync events\n",
      "   You should be able to plot sync event data in the interactive visualizer\n",
      "\n",
      "📊 TESTING PLOT AROUND SYNC START\n",
      "   Selected channels for test: ['corsano_wrist_wrist_acc_x', 'corsano_wrist_wrist_acc_y', 'corsano_wrist_wrist_acc_z']\n",
      "   Test plot window: 2024-07-26 10:50:00 to 2024-07-26 11:00:00\n",
      "   Samples in plot window: 15000\n",
      "   ✅ Plot data available - ready for interactive visualization!\n",
      "   💡 Click 'Sync Start' button in the interactive visualizer to view\n"
     ]
    }
   ],
   "source": [
    "# Quick test function to check sync event data\n",
    "def quick_sync_check():\n",
    "    \"\"\"Quick function to test data availability around sync events\"\"\"\n",
    "    print(\"=== QUICK SYNC EVENT DATA TEST ===\")\n",
    "    \n",
    "    if sync_start_time is None or sync_end_time is None:\n",
    "        print(\"❌ No sync events available for testing\")\n",
    "        return False\n",
    "    \n",
    "    success = True\n",
    "    \n",
    "    for event_name, event_time in [(\"Sync Start\", sync_start_time), (\"Sync End\", sync_end_time)]:\n",
    "        print(f\"\\n🎯 Testing {event_name}: {event_time}\")\n",
    "        \n",
    "        # Check if event is in data range\n",
    "        in_range = data_start <= event_time <= data_end\n",
    "        print(f\"   In data range: {'✅' if in_range else '❌'} {in_range}\")\n",
    "        \n",
    "        if not in_range:\n",
    "            success = False\n",
    "            if event_time < data_start:\n",
    "                print(f\"   Event is {data_start - event_time} before data starts\")\n",
    "            else:\n",
    "                print(f\"   Event is {event_time - data_end} after data ends\")\n",
    "            continue\n",
    "        \n",
    "        # Test different window sizes\n",
    "        for window_min in [1, 5, 10]:\n",
    "            half_window = pd.Timedelta(minutes=window_min/2)\n",
    "            window_start = max(event_time - half_window, data_start)\n",
    "            window_end = min(event_time + half_window, data_end)\n",
    "            \n",
    "            mask = (combined_data.index >= window_start) & (combined_data.index <= window_end)\n",
    "            data_in_window = combined_data[mask]\n",
    "            \n",
    "            if len(data_in_window) > 0:\n",
    "                print(f\"   ✅ ±{window_min}min window: {len(data_in_window)} samples available\")\n",
    "            else:\n",
    "                print(f\"   ❌ ±{window_min}min window: No data available\")\n",
    "                success = False\n",
    "    \n",
    "    if success:\n",
    "        print(f\"\\n✅ SYNC EVENT DATA TEST PASSED\")\n",
    "        print(\"   Data is available around sync events\")\n",
    "        print(\"   You should be able to plot sync event data in the interactive visualizer\")\n",
    "    else:\n",
    "        print(f\"\\n❌ SYNC EVENT DATA TEST FAILED\")\n",
    "        print(\"   Data is NOT available around sync events\")\n",
    "        print(\"   Check the detailed analysis above for troubleshooting\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "# Run the quick test\n",
    "test_result = quick_sync_check()\n",
    "\n",
    "# If test fails, provide additional debugging\n",
    "if not test_result and sync_start_time is not None:\n",
    "    print(f\"\\n🔍 ADDITIONAL DEBUGGING INFO:\")\n",
    "    \n",
    "    # Show sample of data timestamps around sync events\n",
    "    print(f\"\\nData timestamps near sync start time:\")\n",
    "    sync_area_mask = (combined_data.index >= sync_start_time - pd.Timedelta(hours=2)) & (combined_data.index <= sync_start_time + pd.Timedelta(hours=2))\n",
    "    sync_area_data = combined_data[sync_area_mask]\n",
    "    \n",
    "    if len(sync_area_data) > 0:\n",
    "        print(f\"   Found {len(sync_area_data)} samples within ±2 hours of sync start\")\n",
    "        print(f\"   First 5 timestamps: {sync_area_data.index[:5].tolist()}\")\n",
    "        print(f\"   Last 5 timestamps: {sync_area_data.index[-5:].tolist()}\")\n",
    "    else:\n",
    "        print(f\"   No data found within ±2 hours of sync start\")\n",
    "        \n",
    "        # Show actual data time range vs sync time\n",
    "        print(f\"\\nTime comparison:\")\n",
    "        print(f\"   Sync start: {sync_start_time}\")\n",
    "        print(f\"   Data start: {data_start}\")\n",
    "        print(f\"   Data end: {data_end}\")\n",
    "        print(f\"   Difference to data start: {data_start - sync_start_time}\")\n",
    "        print(f\"   Difference to data end: {sync_start_time - data_end}\")\n",
    "\n",
    "# Test plotting around sync events if data is available\n",
    "if test_result and sync_start_time is not None:\n",
    "    print(f\"\\n📊 TESTING PLOT AROUND SYNC START\")\n",
    "    \n",
    "    # Set up for plotting around sync start\n",
    "    center_time_text.value = sync_start_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    window_minutes.value = 10  # 10 minute window\n",
    "    \n",
    "    # Select some default channels for testing\n",
    "    if len(sensor_columns) > 0:\n",
    "        test_channels = sensor_columns[:min(3, len(sensor_columns))]\n",
    "        channel_selection.value = test_channels\n",
    "        print(f\"   Selected channels for test: {test_channels}\")\n",
    "        \n",
    "        # Test plot data availability\n",
    "        half_window = pd.Timedelta(minutes=5)\n",
    "        plot_start = sync_start_time - half_window\n",
    "        plot_end = sync_start_time + half_window\n",
    "        \n",
    "        mask = (combined_data.index >= plot_start) & (combined_data.index <= plot_end)\n",
    "        test_plot_data = combined_data[mask]\n",
    "        \n",
    "        print(f\"   Test plot window: {plot_start} to {plot_end}\")\n",
    "        print(f\"   Samples in plot window: {len(test_plot_data)}\")\n",
    "        \n",
    "        if len(test_plot_data) > 0:\n",
    "            print(f\"   ✅ Plot data available - ready for interactive visualization!\")\n",
    "            print(f\"   💡 Click 'Sync Start' button in the interactive visualizer to view\")\n",
    "        else:\n",
    "            print(f\"   ❌ No plot data available in test window\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ No sensor columns available for testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
