{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9df2f1c",
   "metadata": {},
   "source": [
    "# Raw Data Processing Stage Analysis Notebook\n",
    "\n",
    "Purpose: Load data at different intermediate processing points from `raw_data_processor.py`\n",
    "for a specific subject and visualize selected sensors/modalities to verify the steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7269106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding project root to path: /scai_data3/scratch/stirnimann_r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 16:30:55,058 - INFO - Configuration loaded successfully from /scai_data3/scratch/stirnimann_r/config.yaml\n",
      "2025-06-24 16:30:55,130 - INFO - Loading global labels from: /scai_data3/scratch/stirnimann_r/All_Videos_with_Labels_Real_Time_Corrected_Labels.csv\n",
      "/tmp/ipykernel_2343871/857197066.py:100: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  global_labels_df = pd.read_csv(\n",
      "2025-06-24 16:30:55,161 - WARNING - Dropped 9 rows from labels file due to parsing errors in time columns.\n",
      "2025-06-24 16:30:55,163 - INFO - Loaded 7205 valid global labels.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b12ca9ecfe14f018c473fe7d63ae355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(Dropdown(description='Subject:', options=('OutSense-036', 'OutSenâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% Imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime, time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from scipy.signal import butter, sosfiltfilt\n",
    "from ipywidgets import interact, Dropdown, IntText, FloatText, Button, VBox, HBox, Output, DatePicker, Label\n",
    "import logging\n",
    "import re\n",
    "\n",
    "# --- Add project root to path to import project modules ---\n",
    "# Adjust this path if your script is located elsewhere relative to 'src'\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__)) # Get script directory\n",
    "except NameError:\n",
    "    script_dir = os.getcwd() # Fallback for interactive environments like notebooks\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\")) # Assumes script is in src\n",
    "\n",
    "# Temporarily add project root to sys.path if not already there\n",
    "if project_root not in sys.path:\n",
    "    print(f\"Adding project root to path: {project_root}\")\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# --- Import project modules ---\n",
    "try:\n",
    "    # Assuming the script is run from within the 'src' directory or 'src' is in PYTHONPATH\n",
    "    import utils\n",
    "    import config_loader\n",
    "    # Import specific functions needed from raw_data_processor\n",
    "    from raw_data_processor import (\n",
    "        correct_timestamp_drift,\n",
    "        process_file_numeric_time, # Needed by loaders\n",
    "        data_loader_no_dir,\n",
    "        data_loader_with_dir,\n",
    "        select_data_loader,\n",
    "        process_modality_duplicates,\n",
    "        handle_missing_data_interpolation, # Kept for potential other uses, but removed from main flow\n",
    "        modify_modality_names, # Added import\n",
    "        butter_lowpass_sos,\n",
    "        apply_filter_combined\n",
    "    )\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(f\"1. You are running this script/notebook from the '{os.path.basename(project_root)}/src' directory, OR\")\n",
    "    print(f\"2. The project root directory '{project_root}' is in your PYTHONPATH.\")\n",
    "    # You might need to exit or handle this error appropriately\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during imports: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Basic Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "SYNC_Y = 2024\n",
    "SYNC_Month = 2\n",
    "SYNC_D = 7\n",
    "\n",
    "SYNC_H = 11\n",
    "SYNC_M = 37\n",
    "SYNC_S = 0\n",
    "# --- Configuration Loading ---\n",
    "try:\n",
    "    # Construct config paths relative to the project root determined earlier\n",
    "    config_file_path = os.path.join(project_root, 'config.yaml')\n",
    "    sync_params_path = os.path.join(project_root, 'Sync_Parameters.yaml')\n",
    "\n",
    "    if not os.path.exists(config_file_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_file_path}\")\n",
    "    if not os.path.exists(sync_params_path):\n",
    "        raise FileNotFoundError(f\"Sync parameters file not found at {sync_params_path}\")\n",
    "\n",
    "    config = config_loader.load_config(config_file_path)\n",
    "    with open(sync_params_path, 'r') as f:\n",
    "        sync_params = yaml.safe_load(f)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(e)\n",
    "    # Handle error appropriately, e.g., exit or use defaults\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading configuration: {e}\", exc_info=True)\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Load Global Labels ---\n",
    "global_labels_df = pd.DataFrame() # Initialize empty DataFrame\n",
    "try:\n",
    "    # Use the path from config.yaml\n",
    "    global_labels_path = os.path.join(project_root, config.get('global_labels_file'))\n",
    "    if not os.path.exists(global_labels_path):\n",
    "        logging.warning(f\"Global labels file not found at {global_labels_path}. Cannot display labels.\")\n",
    "    else:\n",
    "        logging.info(f\"Loading global labels from: {global_labels_path}\")\n",
    "        global_labels_df = pd.read_csv(\n",
    "            global_labels_path,\n",
    "            # Assuming format might be DD.MM.YYYY HH:MM:SS based on previous example\n",
    "            # Use infer_datetime_format for flexibility, add dayfirst=True if needed\n",
    "            parse_dates=['Real_Start_Time', 'Real_End_Time'],\n",
    "            infer_datetime_format=True,\n",
    "            dayfirst=False, # Set to True if dates are DD/MM, False if MM/DD\n",
    "        )\n",
    "        # Drop rows where time parsing failed\n",
    "        rows_before = len(global_labels_df)\n",
    "        global_labels_df.dropna(subset=['Real_Start_Time', 'Real_End_Time'], inplace=True)\n",
    "        rows_after = len(global_labels_df)\n",
    "        if rows_before > rows_after:\n",
    "            logging.warning(f\"Dropped {rows_before - rows_after} rows from labels file due to parsing errors in time columns.\")\n",
    "\n",
    "        # Ensure TZ is naive for comparison with plot axes\n",
    "        global_labels_df['Real_Start_Time'] = global_labels_df['Real_Start_Time'].dt.tz_localize(None)\n",
    "        global_labels_df['Real_End_Time'] = global_labels_df['Real_End_Time'].dt.tz_localize(None)\n",
    "        logging.info(f\"Loaded {len(global_labels_df)} valid global labels.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logging.error(e)\n",
    "    logging.warning(\"Proceeding without global labels for plotting.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading or parsing global labels: {e}\", exc_info=True)\n",
    "    global_labels_df = pd.DataFrame() # Ensure it's an empty df on error\n",
    "    logging.warning(\"Proceeding without global labels for plotting.\")\n",
    "\n",
    "# --- Helper Function to Get Data at Stages ---\n",
    "\n",
    "def get_sensor_data_at_stages(subject_id, sensor_name, start_unix_ts, duration_secs, config, sync_params):\n",
    "    \"\"\"\n",
    "    Loads raw data and processes it step-by-step for a given time window,\n",
    "    mirroring the preprocessing order of raw_data_processor.py.\n",
    "\n",
    "    Args:\n",
    "        subject_id (str): The subject ID (e.g., 'OutSense-608').\n",
    "        sensor_name (str): The name of the sensor modality (e.g., 'mbient_imu_wc_accelerometer').\n",
    "        start_unix_ts (float): The starting timestamp (UNIX seconds) for the analysis window.\n",
    "        duration_secs (float): The duration of the analysis window in seconds.\n",
    "        config (dict): Loaded configuration from config.yaml.\n",
    "        sync_params (dict): Loaded synchronization parameters from Sync_Parameters.yaml.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing DataFrames for each stage:\n",
    "              'raw_initial', 'indexed_sorted', 'duplicates_handled', \n",
    "              'columns_renamed', 'resampled', 'imputed', 'filtered'.\n",
    "              Returns None if any step fails critically.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Processing {subject_id} - {sensor_name} from {start_unix_ts} for {duration_secs}s\")\n",
    "    data_stages_buffered = {}\n",
    "\n",
    "    # --- Configuration Values ---\n",
    "    # (Configuration loading remains the same as original)\n",
    "    raw_data_base_dir = os.path.join(project_root, config.get('raw_data_input_dir')) # Ensure project_root is defined\n",
    "    subject_dir = os.path.join(raw_data_base_dir, subject_id)\n",
    "    raw_data_parsing_config = config.get('raw_data_parsing_config', {})\n",
    "    sensor_settings = raw_data_parsing_config.get(sensor_name)\n",
    "    subject_correction_params = sync_params.get(subject_id, {})\n",
    "    sensor_corr_params = subject_correction_params.get(sensor_name, {})\n",
    "    downsample_freq = config.get('downsample_freq', 25) # Default from notebook, main script uses 20\n",
    "    filter_params = config.get('filter_parameters', {})\n",
    "    highcut = filter_params.get('highcut_kinematic', 9.9)\n",
    "    filter_order = filter_params.get('filter_order', 4)\n",
    "    target_freq_interval = pd.Timedelta(seconds=1.0 / downsample_freq) if downsample_freq > 0 else None\n",
    "\n",
    "    if not sensor_settings:\n",
    "        logging.error(f\"No parsing config found for sensor '{sensor_name}' in config.yaml.\")\n",
    "        return None\n",
    "    if target_freq_interval is None:\n",
    "         logging.error(f\"Invalid downsample_freq: {downsample_freq}\")\n",
    "         return None\n",
    "\n",
    "    buffer_secs = max(duration_secs * 0.5, 10.0)\n",
    "\n",
    "    # --- 1. Load Raw Data & Calculate Corrected Time ---\n",
    "    logging.debug(f\"Loading raw data for {sensor_name}...\")\n",
    "    loader = select_data_loader(sensor_name)\n",
    "    df_raw_full_sensor = loader(subject_dir, sensor_name, sensor_settings) # Renamed to avoid clash\n",
    "\n",
    "    if df_raw_full_sensor.empty or 'time' not in df_raw_full_sensor.columns:\n",
    "        logging.warning(f\"No raw data loaded or 'time' column missing for {sensor_name}.\")\n",
    "        return None\n",
    "\n",
    "    logging.debug(\"Calculating corrected timestamps for all raw data...\")\n",
    "    try:\n",
    "        time_unit = sensor_corr_params.get('unit', 's')\n",
    "        time_col_num = df_raw_full_sensor['time'].astype(float)\n",
    "        time_col_num_shifted = time_col_num / 1000.0 if time_unit == 'ms' else time_col_num\n",
    "        shift_val = sensor_corr_params.get('shift', 0)\n",
    "        time_col_num_shifted += shift_val\n",
    "        drift_params = sensor_corr_params.get('drift')\n",
    "        time_col_final_num = time_col_num_shifted.copy()\n",
    "\n",
    "        if drift_params and all(k in drift_params for k in ['t0', 't1', 'drift_secs']):\n",
    "            t0_dt = pd.to_datetime(drift_params['t0'], errors='coerce')\n",
    "            t1_dt = pd.to_datetime(drift_params['t1'], errors='coerce')\n",
    "            drift_secs = drift_params.get('drift_secs')\n",
    "            if pd.notna(t0_dt) and pd.notna(t1_dt):\n",
    "                t0_num = t0_dt.timestamp()\n",
    "                t1_num = t1_dt.timestamp()\n",
    "                time_col_final_num = time_col_num_shifted.apply(correct_timestamp_drift, args=(t0_num, t1_num, drift_secs))\n",
    "            else:\n",
    "                logging.warning(f\"Invalid drift timestamps. Skipping drift correction.\")\n",
    "        else:\n",
    "            logging.debug(\"No valid drift parameters. Skipping drift correction.\")\n",
    "\n",
    "        corrected_timestamps_dt = pd.to_datetime(time_col_final_num, unit='s', errors='coerce')\n",
    "        df_raw_full_sensor['corrected_time'] = corrected_timestamps_dt\n",
    "        df_raw_full_sensor.dropna(subset=['corrected_time'], inplace=True)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during initial time correction: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "    # Filter by user window + buffer (on corrected_time)\n",
    "    corrected_start_dt_buffered = pd.to_datetime(start_unix_ts - buffer_secs, unit='s')\n",
    "    corrected_end_dt_buffered = pd.to_datetime(start_unix_ts + duration_secs + buffer_secs, unit='s')\n",
    "    df_raw_buffered_stage_data = df_raw_full_sensor[\n",
    "        (df_raw_full_sensor['corrected_time'] >= corrected_start_dt_buffered) &\n",
    "        (df_raw_full_sensor['corrected_time'] <= corrected_end_dt_buffered)\n",
    "    ].copy()\n",
    "    \n",
    "    del df_raw_full_sensor # Free memory\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    if df_raw_buffered_stage_data.empty:\n",
    "        logging.warning(f\"No raw data in buffered corrected time range for {sensor_name}.\")\n",
    "        return None\n",
    "    data_stages_buffered['raw_initial'] = df_raw_buffered_stage_data\n",
    "\n",
    "    # --- 2. Create Indexed & Sorted Data ---\n",
    "    logging.debug(\"Creating indexed & sorted data stage...\")\n",
    "    try:\n",
    "        # Use the already windowed raw data which has 'corrected_time'\n",
    "        df_indexed_sorted_buffered = data_stages_buffered['raw_initial'].set_index('corrected_time').sort_index()\n",
    "        # Original 'time' column might still be here if it wasn't dropped by set_index, ensure it's not a data col\n",
    "        if 'time' in df_indexed_sorted_buffered.columns:\n",
    "             df_indexed_sorted_buffered = df_indexed_sorted_buffered.drop(columns=['time'])\n",
    "        data_stages_buffered['indexed_sorted'] = df_indexed_sorted_buffered\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating indexed_sorted data: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "    # --- 3. Handle Duplicates ---\n",
    "    logging.debug(\"Handling duplicates...\")\n",
    "    df_input_for_duplicates = data_stages_buffered.get('indexed_sorted')\n",
    "    if df_input_for_duplicates is not None and not df_input_for_duplicates.empty:\n",
    "        try:\n",
    "            sensor_orig_sr = sensor_settings.get('sample_rate', downsample_freq)\n",
    "            df_duplicates_handled = process_modality_duplicates(df_input_for_duplicates.copy(), sensor_orig_sr)\n",
    "            data_stages_buffered['duplicates_handled'] = df_duplicates_handled\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error handling duplicates: {e}\", exc_info=True)\n",
    "            data_stages_buffered['duplicates_handled'] = df_input_for_duplicates.copy() # Pass previous on error\n",
    "    else:\n",
    "        logging.warning(\"Skipping duplicate handling, input is empty.\")\n",
    "        data_stages_buffered['duplicates_handled'] = pd.DataFrame(index=df_input_for_duplicates.index if df_input_for_duplicates is not None else None)\n",
    "\n",
    "    # --- 4. Modify Modality Names ---\n",
    "    logging.debug(\"Modifying modality names...\")\n",
    "    df_input_for_rename = data_stages_buffered.get('duplicates_handled')\n",
    "    if df_input_for_rename is not None and not df_input_for_rename.empty:\n",
    "        try:\n",
    "            # modify_modality_names expects the original sensor_name from config, not a new_prefix\n",
    "            df_renamed_or_tuple = modify_modality_names(df_input_for_rename.copy(), sensor_name)\n",
    "            # Check if the function returned a tuple (sensor_name, df) as observed in logs\n",
    "            if isinstance(df_renamed_or_tuple, tuple) and len(df_renamed_or_tuple) == 2 and isinstance(df_renamed_or_tuple[1], pd.DataFrame):\n",
    "                logging.info(f\"modify_modality_names returned a tuple for {sensor_name}. Extracting DataFrame.\")\n",
    "                df_renamed = df_renamed_or_tuple[1]\n",
    "            elif isinstance(df_renamed_or_tuple, pd.DataFrame):\n",
    "                df_renamed = df_renamed_or_tuple\n",
    "            else:\n",
    "                logging.error(f\"modify_modality_names returned an unexpected type: {type(df_renamed_or_tuple)}. Expected DataFrame or (str, DataFrame). Using input for rename as fallback.\")\n",
    "                df_renamed = df_input_for_rename.copy() # Fallback\n",
    "            data_stages_buffered['columns_renamed'] = df_renamed\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error renaming columns: {e}\", exc_info=True)\n",
    "            data_stages_buffered['columns_renamed'] = df_input_for_rename.copy()\n",
    "    else:\n",
    "        logging.warning(\"Skipping column renaming, input is empty.\")\n",
    "        data_stages_buffered['columns_renamed'] = pd.DataFrame(index=df_input_for_rename.index if df_input_for_rename is not None else None)\n",
    "\n",
    "    # --- 5. Resample Data ---\n",
    "    logging.debug(\"Resampling data...\")\n",
    "    df_input_for_resample = data_stages_buffered.get('columns_renamed')\n",
    "\n",
    "    # Initialize 'resampled' stage with an empty DataFrame.\n",
    "    # Try to determine columns from df_input_for_resample if it's a DataFrame,\n",
    "    # otherwise fallback to 'duplicates_handled' or None.\n",
    "    expected_resampled_columns = None\n",
    "    if isinstance(df_input_for_resample, pd.DataFrame):\n",
    "        expected_resampled_columns = df_input_for_resample.columns\n",
    "    else:\n",
    "        # If df_input_for_resample is not a DataFrame (e.g. due to an issue in 'columns_renamed' stage),\n",
    "        # try to get columns from a previous valid stage.\n",
    "        df_prev_stage_for_cols = data_stages_buffered.get('duplicates_handled') # or 'indexed_sorted'\n",
    "        if isinstance(df_prev_stage_for_cols, pd.DataFrame):\n",
    "            expected_resampled_columns = df_prev_stage_for_cols.columns\n",
    "        else:\n",
    "            # As a last resort, if no column info can be derived, log it.\n",
    "            # The 'resampled' df will be initialized with no columns.\n",
    "            logging.warning(\"Could not determine columns for initializing 'resampled' stage from previous stages.\")\n",
    "\n",
    "    # Ensure data_stages_buffered['resampled'] is initialized as a DataFrame\n",
    "    # even if expected_resampled_columns is None (it will be an empty DF with no columns then)\n",
    "    data_stages_buffered['resampled'] = pd.DataFrame(index=pd.to_datetime([]), columns=expected_resampled_columns)\n",
    "\n",
    "    if isinstance(df_input_for_resample, pd.DataFrame): # Proceed only if it's a DataFrame\n",
    "        if not df_input_for_resample.empty:\n",
    "            try:\n",
    "                df_resampled_mean = df_input_for_resample.resample(target_freq_interval).mean()\n",
    "                \n",
    "                resample_start_time = df_resampled_mean.index.min() if not df_resampled_mean.empty else df_input_for_resample.index.min()\n",
    "                resample_end_time = df_resampled_mean.index.max() if not df_resampled_mean.empty else df_input_for_resample.index.max()\n",
    "\n",
    "                if pd.notna(resample_start_time) and pd.notna(resample_end_time) and resample_start_time <= resample_end_time:\n",
    "                    target_index_for_reindex = pd.date_range(\n",
    "                        start=resample_start_time.floor(target_freq_interval),\n",
    "                        end=resample_end_time.ceil(target_freq_interval),\n",
    "                        freq=target_freq_interval\n",
    "                    )\n",
    "                    df_resampled_reindexed = df_resampled_mean.reindex(target_index_for_reindex)\n",
    "                elif not df_resampled_mean.empty: # If mean is not empty but range is bad, use its index\n",
    "                    df_resampled_reindexed = df_resampled_mean\n",
    "                else: # Fallback to empty DF with correct index type and columns from input\n",
    "                    df_resampled_reindexed = pd.DataFrame(index=pd.to_datetime([]), columns=df_input_for_resample.columns)\n",
    "                \n",
    "                data_stages_buffered['resampled'] = df_resampled_reindexed # Assign successful result\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during resampling: {e}\", exc_info=True)\n",
    "                # 'resampled' remains the initialized empty DataFrame with expected_resampled_columns\n",
    "        else: # df_input_for_resample is an empty DataFrame\n",
    "            logging.warning(\"Skipping resampling, input DataFrame (columns_renamed) is empty.\")\n",
    "            # Ensure 'resampled' is an empty DF with the same columns as the input\n",
    "            data_stages_buffered['resampled'] = pd.DataFrame(index=pd.to_datetime([]), columns=df_input_for_resample.columns)\n",
    "    elif df_input_for_resample is None:\n",
    "        logging.warning(\"Skipping resampling, input (columns_renamed) is None.\")\n",
    "        # 'resampled' remains the initialized empty DataFrame\n",
    "    else: # df_input_for_resample is not a DataFrame and not None (e.g., it's a tuple)\n",
    "        logging.error(f\"Skipping resampling. Expected 'columns_renamed' to be a pandas DataFrame, but found {type(df_input_for_resample)}. Value: {df_input_for_resample}\")\n",
    "        # 'resampled' remains the initialized empty DataFrame\n",
    "\n",
    "    # --- 6. Impute Data ---\n",
    "    logging.debug(\"Imputing data...\")\n",
    "    df_input_for_imputation = data_stages_buffered.get('resampled')\n",
    "    if df_input_for_imputation is not None and not df_input_for_imputation.empty:\n",
    "        try:\n",
    "            df_imputed = df_input_for_imputation.copy()\n",
    "            # Imputation logic from raw_data_processor.py\n",
    "            df_imputed.ffill(limit=int(downsample_freq * 2), inplace=True)\n",
    "            df_imputed.bfill(limit=int(downsample_freq * 2), inplace=True)\n",
    "            df_imputed.fillna(0, inplace=True)\n",
    "            data_stages_buffered['imputed'] = df_imputed\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during imputation: {e}\", exc_info=True)\n",
    "            data_stages_buffered['imputed'] = df_input_for_imputation.copy()\n",
    "    else:\n",
    "        logging.warning(\"Skipping imputation, input is empty.\")\n",
    "        data_stages_buffered['imputed'] = pd.DataFrame(index=df_input_for_imputation.index if df_input_for_imputation is not None else None)\n",
    "        if data_stages_buffered['imputed'] is not None and df_input_for_imputation is not None: # Ensure columns if index exists\n",
    "             data_stages_buffered['imputed'] = data_stages_buffered['imputed'].reindex(columns=df_input_for_imputation.columns)\n",
    "\n",
    "\n",
    "    # --- 7. Apply Filter ---\n",
    "    logging.debug(\"Applying low-pass filter...\")\n",
    "    df_input_for_filter = data_stages_buffered.get('imputed')\n",
    "    if df_input_for_filter is not None and not df_input_for_filter.empty:\n",
    "        try:\n",
    "            sos = butter_lowpass_sos(highcut, downsample_freq, filter_order)\n",
    "            columns_to_filter = df_input_for_filter.select_dtypes(include=np.number).columns.tolist()\n",
    "            if columns_to_filter and sos is not None:\n",
    "                df_filtered = apply_filter_combined(df_input_for_filter.copy(), sos, columns_to_filter)\n",
    "                data_stages_buffered['filtered'] = df_filtered\n",
    "            elif sos is None:\n",
    "                logging.warning(\"SOS filter coefficients are None. Storing unfiltered.\")\n",
    "                data_stages_buffered['filtered'] = df_input_for_filter.copy()\n",
    "            else:\n",
    "                logging.warning(\"No numeric columns to filter or SOS is None. Storing unfiltered.\")\n",
    "                data_stages_buffered['filtered'] = df_input_for_filter.copy()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during filtering: {e}\", exc_info=True)\n",
    "            data_stages_buffered['filtered'] = df_input_for_filter.copy()\n",
    "    else:\n",
    "        logging.warning(\"Skipping filtering, input is empty.\")\n",
    "        data_stages_buffered['filtered'] = pd.DataFrame(index=df_input_for_filter.index if df_input_for_filter is not None else None)\n",
    "        if data_stages_buffered['filtered'] is not None and df_input_for_filter is not None: # Ensure columns\n",
    "            data_stages_buffered['filtered'] = data_stages_buffered['filtered'].reindex(columns=df_input_for_filter.columns)\n",
    "\n",
    "\n",
    "    # --- 8. Slice all stages to Final Requested Window (without buffer) ---\n",
    "    logging.debug(\"Slicing buffered data to final exact window using UNIX timestamp...\")\n",
    "    final_data_stages = {}\n",
    "    final_start_dt = pd.to_datetime(start_unix_ts, unit='s')\n",
    "    final_end_dt = pd.to_datetime(start_unix_ts + duration_secs, unit='s')\n",
    "    \n",
    "    # Define the order of stages for final slicing and output\n",
    "    # These are the keys used in data_stages_buffered\n",
    "    processing_stage_keys = ['raw_initial', 'indexed_sorted', 'duplicates_handled', \n",
    "                             'columns_renamed', 'resampled', 'imputed', 'filtered']\n",
    "\n",
    "    for stage_key in processing_stage_keys:\n",
    "        df_buffered_stage = data_stages_buffered.get(stage_key)\n",
    "        if df_buffered_stage is None:\n",
    "            final_data_stages[stage_key] = pd.DataFrame()\n",
    "            logging.debug(f\"Stage '{stage_key}' is empty or None, skipping final slice.\")\n",
    "            continue\n",
    "\n",
    "        logging.debug(f\"Slicing stage '{stage_key}'...\")\n",
    "        try:\n",
    "            if stage_key == 'raw_initial':\n",
    "                # Slice raw_initial stage based on its 'corrected_time' column\n",
    "                df_final_slice = df_buffered_stage[\n",
    "                    (df_buffered_stage['corrected_time'] >= final_start_dt) &\n",
    "                    (df_buffered_stage['corrected_time'] < final_end_dt) # Use < for end for consistency with date_range\n",
    "                ].copy()\n",
    "                # Do not drop 'corrected_time' here, it might be useful for inspection\n",
    "                # if 'corrected_time' in df_final_slice.columns:\n",
    "                # df_final_slice = df_final_slice.drop(columns=['corrected_time'])\n",
    "            elif isinstance(df_buffered_stage.index, pd.DatetimeIndex):\n",
    "                # Slice other stages based on their DatetimeIndex\n",
    "                df_final_slice = df_buffered_stage[\n",
    "                    (df_buffered_stage.index >= final_start_dt) &\n",
    "                    (df_buffered_stage.index < final_end_dt)\n",
    "                ].copy()\n",
    "            else:\n",
    "                logging.warning(f\"DataFrame for stage '{stage_key}' has unexpected index: {type(df_buffered_stage.index)}. Skipping.\")\n",
    "                df_final_slice = pd.DataFrame()\n",
    "            \n",
    "            final_data_stages[stage_key] = df_final_slice\n",
    "            logging.debug(f\" Stage '{stage_key}' final shape: {df_final_slice.shape}\")\n",
    "        except Exception as slice_err:\n",
    "             logging.error(f\"Error slicing stage '{stage_key}': {slice_err}\", exc_info=True)\n",
    "             final_data_stages[stage_key] = pd.DataFrame()\n",
    "\n",
    "    logging.info(f\"Successfully processed stages for {subject_id} - {sensor_name}\")\n",
    "    return final_data_stages\n",
    "\n",
    "\n",
    "# --- Plotting Function ---\n",
    "def plot_data_stages(data_stages_dict, subject_id, sensor_name, start_unix_ts, start_dt, duration_secs, config, subject_labels):\n",
    "    \"\"\"Plots the data from different processing stages with labels.\"\"\"\n",
    "\n",
    "    if not data_stages_dict:\n",
    "        logging.warning(\"No data provided to plot_data_stages.\")\n",
    "        return\n",
    "\n",
    "    # Updated stages to plot, in order\n",
    "    stages_to_plot = ['raw_initial', 'indexed_sorted', 'duplicates_handled', \n",
    "                      'columns_renamed', 'resampled', 'imputed', 'filtered']\n",
    "    num_stages = len(stages_to_plot)\n",
    "    # sensor_settings = config.get('raw_data_parsing_config', {}).get(sensor_name, {}) # Already available if needed\n",
    "\n",
    "    # --- Label preparation (remains largely the same) ---\n",
    "    unique_labels_in_plot_range = []\n",
    "    relevant_labels = pd.DataFrame()\n",
    "    if not subject_labels.empty:\n",
    "         plot_window_start_utc_naive = pd.to_datetime(start_unix_ts, unit='s')\n",
    "         plot_window_end_utc_naive = pd.to_datetime(start_unix_ts + duration_secs, unit='s')\n",
    "         subject_labels['Real_Start_Time'] = pd.to_datetime(subject_labels['Real_Start_Time'])\n",
    "         subject_labels['Real_End_Time'] = pd.to_datetime(subject_labels['Real_End_Time'])\n",
    "         relevant_labels = subject_labels[\n",
    "             (subject_labels['Real_Start_Time'] < plot_window_end_utc_naive) &\n",
    "             (subject_labels['Real_End_Time'] > plot_window_start_utc_naive)\n",
    "         ].copy()\n",
    "         if not relevant_labels.empty and 'Label' in relevant_labels.columns:\n",
    "              unique_labels_in_plot_range = sorted(relevant_labels['Label'].unique())\n",
    "    # --- End Label preparation ---\n",
    "    \n",
    "    plot_window_start_dt = pd.to_datetime(start_unix_ts, unit='s')\n",
    "    plot_window_end_dt = pd.to_datetime(start_unix_ts + duration_secs, unit='s')\n",
    "    xlim_for_corrected_axes = (plot_window_start_dt, plot_window_end_dt)\n",
    "\n",
    "    prop_cycle = plt.get_cmap('tab10')\n",
    "    num_colors_to_map = min(len(unique_labels_in_plot_range), 10)\n",
    "    label_colors_list = [prop_cycle(i / num_colors_to_map) for i in range(num_colors_to_map)] # Renamed\n",
    "    label_color_map = {label: label_colors_list[i % num_colors_to_map] for i, label in enumerate(unique_labels_in_plot_range)}\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(num_stages, 1, figsize=(15, 5 * num_stages), sharex=False)\n",
    "    fig.suptitle(f\"Data Processing Stages for {subject_id} - {sensor_name}\\\\nWindow: {datetime.fromtimestamp(start_unix_ts)} (Duration: {duration_secs}s)\", fontsize=16)\n",
    "\n",
    "    for i, stage_key in enumerate(stages_to_plot): # Use stage_key consistently\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"Stage: {stage_key.replace('_', ' ').capitalize()}\") # Prettify title\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_ylabel(\"Value\")\n",
    "\n",
    "        df = data_stages_dict.get(stage_key)\n",
    "        \n",
    "        if df is None or df.empty:\n",
    "            ax.text(0.5, 0.5, 'No data available for this stage/window', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "            # Set xlim even for empty plots for consistency, except for raw_initial if it has no time data\n",
    "            if stage_key != 'raw_initial':\n",
    "                 ax.set_xlim(xlim_for_corrected_axes)\n",
    "            continue\n",
    "\n",
    "        time_data_for_plot = None\n",
    "        df_to_plot_values = df # df whose columns will be plotted\n",
    "        \n",
    "        if stage_key == 'raw_initial' and 'time' in df.columns:\n",
    "            # For 'raw_initial', x-axis is original 'time', converted to datetime\n",
    "            time_data_for_plot = pd.to_datetime(df['time'], unit='s', errors='coerce')\n",
    "            valid_time_mask = time_data_for_plot.notna()\n",
    "            time_data_for_plot = time_data_for_plot[valid_time_mask]\n",
    "            df_to_plot_values = df[valid_time_mask]\n",
    "            ax.set_xlabel(\"Original Timestamp (from 'time' column, as Datetime)\")\n",
    "            if not time_data_for_plot.empty:\n",
    "                 ax.set_xlim(time_data_for_plot.min(), time_data_for_plot.max())\n",
    "        elif isinstance(df.index, pd.DatetimeIndex):\n",
    "            # For all other stages, x-axis is the DatetimeIndex (corrected time)\n",
    "            time_data_for_plot = df.index\n",
    "            df_to_plot_values = df\n",
    "            ax.set_xlabel(\"Corrected Timestamp (DatetimeIndex)\")\n",
    "            ax.set_xlim(xlim_for_corrected_axes)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Cannot determine time axis\", transform=ax.transAxes)\n",
    "            logging.warning(f\"Cannot determine time axis for stage '{stage_key}'. Index: {type(df.index)}\")\n",
    "            continue\n",
    "\n",
    "        columns_to_plot_now = df_to_plot_values.select_dtypes(include=np.number).columns.tolist()\n",
    "        # For raw_initial, ensure 'time' and 'corrected_time' (if numeric) are not plotted as y-values\n",
    "        if stage_key == 'raw_initial':\n",
    "            if 'time' in columns_to_plot_now: columns_to_plot_now.remove('time')\n",
    "            if 'corrected_time' in columns_to_plot_now: columns_to_plot_now.remove('corrected_time')\n",
    "        \n",
    "        line_handles, line_labels_legend = [], [] # Renamed for clarity\n",
    "        plotted_something = False\n",
    "        if not df_to_plot_values.empty and time_data_for_plot is not None and not time_data_for_plot.empty:\n",
    "            for col in columns_to_plot_now:\n",
    "                if col in df_to_plot_values.columns:\n",
    "                    line, = ax.plot(time_data_for_plot, df_to_plot_values[col], label=col, alpha=0.8)\n",
    "                    line_handles.append(line)\n",
    "                    line_labels_legend.append(col)\n",
    "                    plotted_something = True\n",
    "        \n",
    "        # Label Spans (axvspan) - only for stages with corrected time index\n",
    "        unique_labels_plotted_on_axis = set()\n",
    "        if stage_key != 'raw_initial' and not relevant_labels.empty:\n",
    "            for _, label_row in relevant_labels.iterrows():\n",
    "                label_start = label_row['Real_Start_Time']\n",
    "                label_end = label_row['Real_End_Time']\n",
    "                label_name = label_row['Label']\n",
    "                color = label_color_map.get(label_name, 'silver')\n",
    "                ax.axvspan(label_start, label_end, color=color, alpha=0.3, zorder=0)\n",
    "                unique_labels_plotted_on_axis.add(label_name)\n",
    "        \n",
    "        # Create Legend\n",
    "        combined_handles = list(line_handles)\n",
    "        combined_labels_legend = list(line_labels_legend) # Renamed\n",
    "        label_patches = []\n",
    "        if stage_key != 'raw_initial': # Add label patches for relevant stages\n",
    "             for label_name_patch in sorted(list(unique_labels_plotted_on_axis)): # Renamed\n",
    "                  if label_name_patch in label_color_map:\n",
    "                       patch = Patch(color=label_color_map[label_name_patch], alpha=0.3, label=f\"{label_name_patch}\")\n",
    "                       label_patches.append(patch)\n",
    "        combined_handles.extend(label_patches)\n",
    "        combined_labels_legend.extend([p.get_label() for p in label_patches])\n",
    "\n",
    "        if combined_handles:\n",
    "            ax.legend(combined_handles, combined_labels_legend, loc='upper left', bbox_to_anchor=(1.01, 1), fontsize='small', title=\"Legend\")\n",
    "        elif not plotted_something:\n",
    "             ax.text(0.5, 0.5, 'No numeric data or labels to plot', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 0.9, 0.95]) # Adjust for suptitle and legend\n",
    "    plt.show() # Ensure plot is displayed\n",
    "\n",
    "# --- Widgets Setup and Interaction ---\n",
    "\n",
    "# Get available subjects and sensors (as before)\n",
    "available_subjects = sorted(list(sync_params.keys()))\n",
    "available_sensors = sorted(list(config.get('raw_data_parsing_config', {}).keys()))\n",
    "\n",
    "# Create Widgets\n",
    "subject_dropdown = Dropdown(options=available_subjects, description='Subject:')\n",
    "sensor_dropdown = Dropdown(options=available_sensors, description='Sensor:')\n",
    "\n",
    "# --- Date/Time Widgets ---\n",
    "# Default to a sensible date/time (e.g., now, or a fixed date relevant to data)\n",
    "# Let's default to now, rounded down to the nearest second for simplicity\n",
    "now = datetime.now().replace(microsecond=0)\n",
    "start_date_picker = DatePicker(description='Start Date:', value=datetime(SYNC_Y, SYNC_Month, SYNC_D), tooltip=\"Select the starting date\")\n",
    "start_hour_text = IntText(value=SYNC_H, description='Hour (0-23):', min=0, max=23, step=1, style={'description_width': 'initial'}, layout={'width': '150px'})\n",
    "start_minute_text = IntText(value=SYNC_M, description='Min (0-59):', min=0, max=59, step=1, style={'description_width': 'initial'}, layout={'width': '150px'})\n",
    "start_second_text = IntText(value=SYNC_S, description='Sec (0-59):', min=0, max=59, step=1, style={'description_width': 'initial'}, layout={'width': '150px'})\n",
    "# --- End Date/Time Widgets ---\n",
    "\n",
    "# --- Label Shift Widget ---\n",
    "label_hour_shift_text = FloatText(value=0.0, description='Label Shift (h):', step=0.5, style={'description_width': 'initial'}, tooltip=\"Enter hours to shift labels (e.g., -2 or 1.5)\")\n",
    "label_minute_shift_text = IntText(value=0, description='(min):', step=1, style={'description_width': 'initial'}, layout={'width': '120px'}, tooltip=\"Enter minutes to shift labels\")\n",
    "label_second_shift_text = IntText(value=0, description='(sec):', step=1, style={'description_width': 'initial'}, layout={'width': '120px'}, tooltip=\"Enter seconds to shift labels\")\n",
    "\n",
    "duration_text = FloatText(value=120.0, description='Duration (s):', step=1.0, style={'description_width': 'initial'})\n",
    "run_button = Button(description='Analyze Window')\n",
    "plot_output = Output() # To display plots within the widget area\n",
    "\n",
    "# Layout\n",
    "# Group date/time widgets together for better organization\n",
    "time_input_box = HBox([start_hour_text, start_minute_text, start_second_text], layout={'justify_content': 'space-around'})\n",
    "datetime_controls = VBox([start_date_picker, time_input_box], layout={'border': '1px solid lightgray', 'padding': '5px', 'margin_bottom': '5px'})\n",
    "\n",
    "# Combine all controls\n",
    "label_shift_controls = HBox([label_hour_shift_text, label_minute_shift_text, label_second_shift_text], layout={'justify_content': 'flex-start'})\n",
    "controls = VBox([\n",
    "    HBox([subject_dropdown, sensor_dropdown]),\n",
    "    datetime_controls, # Add the new datetime controls VBox\n",
    "    duration_text,\n",
    "    label_shift_controls,\n",
    "    run_button\n",
    "])\n",
    "\n",
    "# Button Click Handler\n",
    "def on_run_button_clicked(b):\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True) # Clear previous plot/output\n",
    "        subject = subject_dropdown.value\n",
    "        sensor = sensor_dropdown.value\n",
    "        duration = duration_text.value\n",
    "        label_shift_hours = label_hour_shift_text.value\n",
    "        label_shift_minutes = label_minute_shift_text.value\n",
    "        label_shift_seconds = label_second_shift_text.value\n",
    "\n",
    "        # --- Read Date/Time Widgets and Construct Timestamp ---\n",
    "        start_date = start_date_picker.value\n",
    "        start_hour = start_hour_text.value\n",
    "        start_minute = start_minute_text.value\n",
    "        start_second = start_second_text.value\n",
    "\n",
    "        # --- Input Validation ---\n",
    "        error_messages = []\n",
    "        if start_date is None:\n",
    "            error_messages.append(\"Please select a valid start date.\")\n",
    "        # Combine checks for time components\n",
    "        if not (0 <= start_hour <= 23 and 0 <= start_minute <= 59 and 0 <= start_second <= 59):\n",
    "            error_messages.append(\"Invalid hour (0-23), minute (0-59), or second (0-59) value.\")\n",
    "        if duration <= 0:\n",
    "            error_messages.append(\"Duration must be positive.\")\n",
    "        if not subject:\n",
    "            error_messages.append(\"Please select a subject.\")\n",
    "        if not sensor:\n",
    "            error_messages.append(\"Please select a sensor.\")\n",
    "\n",
    "        if error_messages:\n",
    "            print(\"Input Errors Found:\")\n",
    "            for msg in error_messages:\n",
    "                print(f\"- {msg}\")\n",
    "            return # Stop processing if errors exist\n",
    "\n",
    "        # --- Construct datetime and timestamp ---\n",
    "        try:\n",
    "            # Combine date and time components into a datetime object\n",
    "            start_dt = datetime.combine(start_date, time(start_hour, start_minute, start_second))\n",
    "            # Convert the datetime object to a UNIX timestamp (float)\n",
    "            start_ts = start_dt.timestamp()\n",
    "            logging.info(f\"Selected Start DateTime: {start_dt}, Corresponding UNIX Timestamp: {start_ts}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: Could not construct valid datetime from inputs: {e}\")\n",
    "            logging.error(f\"Error constructing datetime from DatePicker/IntText: {e}\", exc_info=True)\n",
    "            return\n",
    "        # --- End Timestamp Construction ---\n",
    "\n",
    "        # --- >>> Apply Label Shift and Filter Labels <<< ---\n",
    "        subject_labels_shifted_filtered = pd.DataFrame() # Default to empty\n",
    "        if not global_labels_df.empty and 'Video_File' in global_labels_df.columns and 'Label' in global_labels_df.columns:\n",
    "             try:\n",
    "                  # 1. Create a copy to avoid modifying the original global_labels_df\n",
    "                  labels_to_shift = global_labels_df.copy()\n",
    "\n",
    "                  # 2. Apply the shift if it's non-zero\n",
    "                  total_shift_seconds_manual = (label_shift_hours * 3600) + (label_shift_minutes * 60) + label_shift_seconds\n",
    "                  if total_shift_seconds_manual != 0.0:\n",
    "                      logging.info(f\"Applying manual label time shift of {label_shift_hours} hours, {label_shift_minutes} minutes, {label_shift_seconds} seconds.\")\n",
    "                      time_delta_shift_manual = pd.Timedelta(seconds=total_shift_seconds_manual)\n",
    "                      # Ensure columns are datetime before shifting\n",
    "                      labels_to_shift['Real_Start_Time'] = pd.to_datetime(labels_to_shift['Real_Start_Time'])\n",
    "                      labels_to_shift['Real_End_Time'] = pd.to_datetime(labels_to_shift['Real_End_Time'])\n",
    "                      labels_to_shift['Real_Start_Time'] = labels_to_shift['Real_Start_Time'] + time_delta_shift_manual\n",
    "                      labels_to_shift['Real_End_Time'] = labels_to_shift['Real_End_Time'] + time_delta_shift_manual\n",
    "                  else:\n",
    "                       logging.info(\"Manual label time shift is 0. Using original label times for this step.\")\n",
    "\n",
    "                  # 3. Filter the (potentially shifted) labels for the subject\n",
    "                  subject_for_filter = subject\n",
    "                  if subject == 'OutSense-425_48h':\n",
    "                        subject_for_filter = 'OutSense-425' # Use base name for filtering Video_File\n",
    "\n",
    "                  subject_labels_intermediate = labels_to_shift[\n",
    "                      labels_to_shift['Video_File'].astype(str).str.contains(subject_for_filter, na=False, regex=False)\n",
    "                  ].copy()\n",
    "\n",
    "                  # 4. Apply Label_Time_Shift from Sync_Parameters.yaml to this subject-specific subset\n",
    "                  label_time_shift_str = sync_params.get(subject, {}).get('Label_Time_Shift', '0h 0min 0s')\n",
    "                  logging.info(f\"Applying Label_Time_Shift from Sync_Parameters for subject {subject}: {label_time_shift_str}\")\n",
    "                  shift_match = re.match(r'(?:(-?\\d+)h)?\\s*(?:(-?\\d+)min)?\\s*(?:(-?\\d+)s)?', label_time_shift_str)\n",
    "                  if shift_match:\n",
    "                      sync_shift_hours = int(shift_match.group(1) or 0)\n",
    "                      sync_shift_minutes = int(shift_match.group(2) or 0)\n",
    "                      sync_shift_seconds = int(shift_match.group(3) or 0)\n",
    "                      total_sync_shift_seconds = (sync_shift_hours * 3600) + (sync_shift_minutes * 60) + sync_shift_seconds\n",
    "\n",
    "                      if total_sync_shift_seconds != 0:\n",
    "                          logging.info(f\"Shifting labels by {total_sync_shift_seconds} seconds based on Sync_Parameters.\")\n",
    "                          time_delta_sync_shift = pd.Timedelta(seconds=total_sync_shift_seconds)\n",
    "                          subject_labels_intermediate['Real_Start_Time'] = pd.to_datetime(subject_labels_intermediate['Real_Start_Time']) + time_delta_sync_shift\n",
    "                          subject_labels_intermediate['Real_End_Time'] = pd.to_datetime(subject_labels_intermediate['Real_End_Time']) + time_delta_sync_shift\n",
    "                      else:\n",
    "                          logging.info(\"No additional label time shift from Sync_Parameters (shift is 0 seconds).\")\n",
    "                  else:\n",
    "                      logging.warning(f\"Could not parse Label_Time_Shift for subject {subject}: {label_time_shift_str}\")\n",
    "\n",
    "                  # 5. Keep only necessary columns and drop NaNs from time columns *after* all shifts\n",
    "                  subject_labels_shifted_filtered = subject_labels_intermediate[['Real_Start_Time', 'Real_End_Time', 'Label']].dropna(subset=['Real_Start_Time', 'Real_End_Time'])\n",
    "                  logging.info(f\"Filtered {len(subject_labels_shifted_filtered)} labels for subject {subject} (after all shifts).\")\n",
    "\n",
    "             except Exception as e:\n",
    "                  logging.error(f\"Error processing labels for subject {subject}: {e}\", exc_info=True)\n",
    "                  subject_labels_shifted_filtered = pd.DataFrame() # Ensure it's empty on error\n",
    "        else:\n",
    "             logging.warning(\"Global labels DataFrame is empty or essential columns ('Video_File', 'Label') missing. Cannot filter labels.\")\n",
    "        # --- >>> End Label Shift and Filtering <<< ---\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"DEBUG: Passing {len(subject_labels_shifted_filtered)} labels to plot function.\")\n",
    "        if not subject_labels_shifted_filtered.empty:\n",
    "            print(\"DEBUG: First 5 filtered labels passed:\")\n",
    "            print(subject_labels_shifted_filtered.head().to_string())\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        print(f\"\\nStarting analysis for Subject: {subject}, Sensor: {sensor}...\")\n",
    "\n",
    "        data_dict = get_sensor_data_at_stages(subject, sensor, start_ts, duration, config, sync_params)\n",
    "\n",
    "        if data_dict:\n",
    "            print(\"--- DataFrame Shapes After Processing ---\")\n",
    "            for stage, df_stage_data in data_dict.items(): # Renamed df to df_stage_data\n",
    "                print(f\"DEBUG: Stage '{stage}' shape: {df_stage_data.shape if df_stage_data is not None else 'None'}\")\n",
    "                if df_stage_data is not None and not df_stage_data.empty and isinstance(df_stage_data.index, pd.DatetimeIndex):\n",
    "                    print(f\"DEBUG: Stage '{stage}' index range: Min={df_stage_data.index.min()}, Max={df_stage_data.index.max()}\")\n",
    "                elif df_stage_data is not None and not df_stage_data.empty and stage == 'raw_initial' and 'time' in df_stage_data.columns: # Changed from 'raw'\n",
    "                    try:\n",
    "                        raw_time_dt = pd.to_datetime(df_stage_data['time'], unit='s', errors='coerce')\n",
    "                        print(f\"DEBUG: Stage '{stage}' original 'time' range: Min={raw_time_dt.min()}, Max={raw_time_dt.max()}\")\n",
    "                    except Exception:\n",
    "                        print(f\"DEBUG: Stage '{stage}' original 'time' could not be fully converted.\")\n",
    "\n",
    "            print(\"--- End DataFrame Shapes ---\")\n",
    "            print(\"Plotting data stages...\")\n",
    "            plot_data_stages(data_dict, subject, sensor, start_ts, start_dt, duration, config, subject_labels_shifted_filtered)\n",
    "            print(\"\\nAnalysis complete.\")\n",
    "        else:\n",
    "            print(\"\\nFailed to process data for the selected window. Check logs for details.\")\n",
    "\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "\n",
    "# --- Display Widgets ---\n",
    "# This should be the last part of your script if running interactively\n",
    "display(VBox([controls, plot_output]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
